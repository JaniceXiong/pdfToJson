{
    "dealing with infinite loops, underestimation, and overestimation of depth-first proof-number search": {
        "Title": "dealing with infinite loops, underestimation, and overestimation of depth-first proof-number search",
        "Authors": [
            "Akihiro Kishimoto"
        ],
        "Abstract": "Depth-first proof-number search (df-pn) is powerful AND/OR tree search to solve positions in games. However, df-pn has a notorious problem of infinite loops when applied to domains with repetitions. Df-pn(r) cures it by ignoring proof and disproof numbers that may lead to infinite loops. This paper points out that df-pn(r) has a serious issue of underestimating proof and disproof numbers, while it also suffers from the overestimation problem occurring in directed acyclic graph. It then presents two practical solutions to these problems. While bypassing infinite loops, the threshold controlling algorithm (TCA) solves the underestimation problem by increasing the thresholds of df-pn. The source node detection algorithm (SNDA) detects the cause of overestimation and modifies the computation of proof and disproof numbers. Both TCA and SNDA are implemented on top of df-pn to solve tsume-shogi (checkmating problem in Japanese chess). Results show that df-pn with TCA and SNDA is far superior to df-pn(r). Our tsume-shogi solver is able to solve several difficult positions previously unsolved by any other solvers.",
        "Keywords": [],
        "BookMarks": [
            "Introduction",
            "Tsume-Shogi as a Testbed of AI Research",
            "Related Work and Problem Descriptions",
            "Depth-First Proof-Number Search",
            "The Infinite Loop Problem and Df-pn(r)",
            "The Underestimation and Overestimation Problems",
            "Previous Work on the Overestimation Problem",
            "Our Solutions",
            "The Threshold Controlling Algorithm",
            "The Source Node Detection Algorithm",
            "Implementation Details",
            "Experimental Results",
            "Setup of Experiments",
            "Results",
            "Instance",
            "Conclusions and Future Work"
        ],
        "Papertext": [
            [
                "Developing efficient AND/OR tree search has been a fundamental topic in AI, because solving complex AND/OR trees is required for problem-solving procedures. Such an example is to find a winning way from a given position in games.",
                "Research in solving tsume-shogi (checkmating problem in Japanese chess) has produced a variety of powerful domain-independent AND/OR tree search techniques  (Seo, Iida, and Uiterwijk 2001; <ref id=#b19> Nagai 2002) <ref id=#b13> . In particular, successful ideas behind depth-first proof-number search (df-pn)  (Nagai 2002) <ref id=#b13>  have been adapted to other games, including checkers  (Schaeffer et al. 2007) <ref id=#b15> , and the life and death problem in the game of Go  (Kishimoto and Müller 2005) <ref id=#b9> .",
                "Df-pn is an enhanced reformulation of best-first proofnumber search (PNS)  (Allis, van der Meulen, and van den Herik 1994) <ref id=#b0>  in a depth-first manner. A leaf node to expand next is selected by Allis' proof and disproof numbers, an estimated difficulty to find a win or loss. The df-pn search is controlled by the thresholds of proof and disproof numbers.",
                "If the search space is a tree, df-pn is equivalent to PNS with preserving good properties of reducing mem-Copyright c 2010, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. ory requirement and interior node re-expansions. However, the search space of many games is directed acyclic graph (DAG), or even directed cyclic graph (DCG). While df-pn is still a pragmatic choice, several issues must be addressed.",
                "The infinite loop problem is an essential problem causing df-pn never to solve a position even in infinite time. Although df-pn(r)  (Kishimoto 2005) <ref id=#b11>  cures this issue, we address that df-pn(r) suffers from underestimating proof and disproof numbers. The overestimation problem is caused by counting proof and disproof numbers of the same node many times, and has been addressed previously in  (Schijf 1993; <ref id=#b18> Müller 2003; <ref id=#b12> Seo, Iida, and Uiterwijk 2001) <ref id=#b19> . Both overestimation and underestimation delay searching promising nodes and increase the search effort by a large margin.",
                "This paper presents techniques to handle the three aforementioned problems. Its contributions are synthesis of df-pn with two novel methods and a demonstration of the promise of our approach in solving difficult tsume-shogi instances.",
                "The rest of this paper is organized as: Starting with a description of tsume-shogi, we review related work. Our new algorithms are then described, followed by experimental results, and conclusions with outlines of further work."
            ],
            [
                "Shogi (Japanese chess) is a game with 15 million players and the history of over 400 years. Not only there are a few hundred human professional players but also an annual world computer shogi championship has been held for 20 years to determine the best of over 40 participating programs 1 .",
                "As in chess the goal of shogi is to capture the king. However, unlike in chess, captured pieces can be later reused by dropping one of them on the board  (Hosking 1996) <ref id=#b2> . Because of this rule, shogi has a larger average branching factor than chess (80 -100 versus 35). Developing strong shogi programs has been an important subject of game-playing  (Schaeffer 2001; <ref id=#b16> Iida, Sakuta, and Rollason 2002) <ref id=#b3> .",
                "Tsume-shogi is the checkmating problem where the attacker checks the king and the defender escapes. The attacker is the first player. To solve tsume-shogi, the attacker must find at least one winning move, while all the moves responded by the defender must be proven to be check mate.",
                "Tsume-shogi is an ideal domain to investigate ideas on AND/OR tree search, because there are many difficult instances composed by tsume-shogi creators for hundreds of years. Searching over a thousand of depth (ply) is often required to find solutions. Besides, specialized tsume-shogi search engines are incorporated into most shogi programs, because checkmating attacks plays a crucial role in shogi."
            ],
            [],
            [
                "Let a proof be a win for the attacker (OR node) and a disproof be a win for the defender (AND node).  Df-pn (Nagai 2002) <ref>  leverages proof and disproof numbers pn(n) and dn(n)  (Allis, van der Meulen, and van den Herik 1994) <ref id=#b0>  to estimate a difficulty of finding a proof or disproof of node n.",
                "pn(n) is the minimum number of leaf nodes that must be proven to find a proof for n in a currently generated tree. dn(n) is the minimum number of leaf nodes to be disproven to find a disproof for n. pn(n) = 0 and dn(n) = ∞ are set for proven terminal node n, while pn(n) = ∞ and dn(n) = 0 for disproven terminal node. pn(n) = dn(n) = 1 is set for unproven leaf. Let n 1 , n 2 , • • • , n k be children of interior node n. Since only one proven child suffices to prove an OR node, while all children must be proven to prove an AND node (and vice versa for disproof). pn(n) and dn(n) are:",
                "<formula_0>",
                "As in PNS, df-pn expands a leaf node (called a mostpromising node) reached from the root by selecting a child with the smallest proof number at each OR node, and a child with the smallest disproof number at each AND node. Unlike PNS, df-pn does so by using two thresholds: th pn (n) for proof numbers and th dn (n) for disproof numbers.",
                "At the root r, th pn (r) and th dn (r) are initialized to ∞. Whenever df-pn reaches n from its parent p, or backtracks to n after expanding n's descendants, it computes pn(n) and dn(n) by using proof and disproof numbers of n's children. pn(n) and dn(n) are cached in the transposition table for reuse. If pn(n) ≥ th pn (n) or dn(n) ≥ th dn (n) holds, df-pn backtracks to p. Otherwise, it selects a child n 1 with the smallest (dis)proof number at OR (AND) node n. Let n 2 be a child with the second smallest (dis)proof number at OR (AND) node n. Df-pn searches n 1 with the following thresholds indicating the condition in which either n 1 or n is no longer on the path to reach a most-promising node:",
                "• For OR node n, th pn (n 1 ) = min(th pn (n), pn(n 2 ) + 1) and th dn (n 1 ) = th dn (n) − dn(n) + dn(n 1 ). • For AND node n, th pn (n 1 ) = th pn (n) − pn(n) + pn(n 1 ) and th dn (n 1 ) = min(th dn (n), dn(n 2 ) + 1). Df-pn continues these procedures until finding a solution."
            ],
            [
                "When standard df-pn is applied to DCG, it has a fundamental problem of causing infinite loops. Although  (Nagai 2002) <ref id=#b13>  Figure 1: An example showing that df-pn loops forever, adapted from  (Kishimoto and Müller 2008) <ref id=#b10>  does not mention the existence of this problem 2 , (Kishimoto 2005) confirms it occurs in many domains including Go, checkers, and shogi. Besides,  (Kishimoto and Müller 2008) <ref id=#b10>  prove that df-pn loops forever when searching a DCG in  Figure 1 <figure> . Df-pn must expand P to prove A. However, this never occurs, because df-pn keeps selecting two paths",
                "<formula_1>",
                "Df-pn(r) is a practical method to handle infinite loops  (Kishimoto and Müller 2003; <ref id=#b7> Kishimoto 2005) <ref id=#b11> . pn(n) for OR node and dn(n) for AND node are computed in a standard way. Because adding (dis)proof numbers from an ancestor to a node is intuitively bad, df-pn(r) detects this case by computing minimal distance md(n), the length of the shortest path from the root to node n (see md in  Figure 1 <figure> ). Df-pn(r) classifies n's children c into two types: c is normal if md(n) < md(c). Otherwise c is old. If at least one unproven normal child exists, all old children are ignored to compute dn(n) for OR node n. Otherwise, dn(n) is set to the maximum of disproof numbers of all old children. pn(n) for AND node n is analogously treated.",
                "In  Figure 1 <figure> , df-pn(r) computes dn(O) = dn(P ) for unproven P and H, because H is old and P is normal at O."
            ],
            [
                "Although  (Kishimoto 2005) <ref id=#b11>  shows that df-pn(r) is effective in Go and checkers, we point out that it often underestimates (dis)proof numbers, as shown in the left example in  Figure  2 <figure id=#fig_0> . Df-pn(r) computes dn(D) = dn(E) for unproven E and F , because F is D's old child. However, more reasonable is dn(D) = dn(E) + dn(F ), because both E and F must be disproven to disprove D.",
                "Moreover, both df-pn and df-pn(r) overestimate proof and disproof numbers. In the right example in  Figure 2 <figure id=#fig_0> , the"
            ],
            [
                "The overestimation problem occurs only when computing pn for AND node and dn for OR node. In the other cases, pn(n) and dn(n) are computed in a standard way.",
                "Since overestimation also occurs in PNS, a few solutions are presented but are specific to PNS  (Allis 1994; <ref id=#b1> Schijf, Allis, and Uiterwijk 1994; <ref id=#b17> Müller 2003 <ref id=#b12> ). An exact method in  (Schijf 1993 <ref id=#b18> ) is unfortunately impractical even for tic-tactoe, due to high computational overhead.",
                "In  Figure 2 <figure id=#fig_0> , let us call J a destination node, which has more than one parent, and G a source node of destination node n d , which is n d 's first ancestor merged into by tracing back different paths starting at n d .  (Nagai 2002 <ref id=#b13> ) presents a method to detect a source node n s and takes the maximum of (dis)proof numbers of n s 's children to compute pn(n s ) (or dn(n s )), instead of summing up them. In his method, each transposition table (TT) entry of node n has one pointer to one parent p. When df-pn reaches n via p, p is saved in n's TT entry. Then, if df-pn reaches n via another parent q, it indicates that n is a destination node and there may exist a source node n s that counts pn(n) or dn(n) more than once. n s is detected by checking if one of the ancestors obtained by recursively traversing pointers from n in the TT is merged into n s , which is also on the path of the current df-pn search.",
                "In  Figure 2 <figure id=#fig_0> , assume that J points to H and H points to G in the TT, because df-pn previously reaches J via G → H → J. Then, if df-pn reaches J via G → I → J, Nagai's method detects that J has a different parent I, which is not stored in J's TT entry (i.e., H). G is detected as a source node by traversing J → H and then H → G in the TT, and G is on path G → I → J. Nagai's method then sets pn(G) = max(dn(H), dn(I)) = dn(J) accurately. However, this approach underestimates dn(A), because it considers A to be a source and computes dn(A) = max(dn(B), dn(C), dn(F )) = max(dn(B), dn(E) + dn(F ), dn(F )) = max(dn(B), dn(E) + dn(F )). The true value of dn(A) should be dn(B) + dn(E) + dn(F ).",
                "Let b be the number of unproven children and n 1 , • • • n k be children of node n. Weak proof-number search (WPNS)  (Ueda et al. 2008) <ref id=#b21> , which is a refinement to  (Okabe 2005) <ref id=#b14> , modifies pn(n) and dn(n) in the following way:",
                "<formula_2>",
                "WPNS avoids counting dn(J) twice to compute dn(G) in  Figure 2 <figure id=#fig_0> , at the price of slightly overestimating dn(G) to dn(J)+1 and reusing the proof and disproof numbers in the TT less frequently than df-pn. Also, WPNS underestimates proof and disproof numbers, because most of the (dis)proof numbers at interior nodes are assumed to be 1."
            ],
            [
                "This section describes two practical algorithms to overcome the issues of previous approaches."
            ],
            [
                "Df-pn(r) tries to break the termination condition at node n that has an unproven old child by decreasing pn(n) or dn(n). This way, it reaches a leaf, avoiding infinite loops. In contrast, the Threshold Controlling Algorithm (TCA), breaks the termination condition by increasing the thresholds at n. This way, TCA tries to reach a leaf to expand.",
                "The underestimation problem of df-pn(r) is caused by ignoring old children determined by md(n), although md(n) does not always indicate the existence of infinite loops. TCA computes pn(n) and dn(n) in a standard way. TCA therefore cures underestimation in  Figure 2 <figure id=#fig_0> .",
                "<formula_3>",
                "To overcome infinite loops, TCA leverages the minimal distance as a criterion of increasing thresholds. When TCA enters n and computes pn(n) and dn(n) by retrieving the TT entries of n's children, it checks if any unproven child of n is old. If n has no unproven old child, the standard df-pn procedure is performed (this will be modified later). Otherwise, TCA resets the thresholds to max(th pn (n), pn(n) + 1) and max(th dn (n), dn(n) + 1). With the increased thresholds, pn(n) < th pn (n) and dn(n) < th dn (n) hold. TCA therefore expands n and selects the best child n 1 -i.e., one with the smallest proof number at OR node and one with smallest disproof number at AND node. TCA adjusts th pn (n 1 ) and th dn (n 1 ) in a standard way except that they are based on the increased thresholds.",
                "Assume that TCA increases th pn (n) and th dn (n) at node n that has an unproven old child, selects the best child c at n, and enters c. th pn (c) and th dn (c) are also increased even if c has only unproven normal children. This is to avoid infinite loops caused by the case where TCA immediately backtracks to n from c by satisfying the termination condition at c. TCA continues the procedure of increasing thresholds until either expanding a leaf, or detecting a new cyclic path. Expanding a leaf reduces the unexplored search space. If a new cycle is detected,  (Kishimoto and Müller 2004) <ref id=#b8>  correctly saves a (dis)proof in the TT, and the (dis)proof is propagated back. This also reduces the unexplored space. When TCA backtracks to n, it makes progress in finding a solution. Therefore, after TCA recomputes pn(n) and dn(n), the previously increased thresholds are reused to determine if TCA still needs to re-search n.  Figure 3 <figure>  presents the pseudo-code of df-pn with TCA. In particular, see lines marked by (*) to clarify the difference between standard df-pn and TCA. Note that inc f lag is a flag to determine if TCA increases thpn and thdn.",
                "DFPNwithTCA(n, thpn, thdn, inc flag) { if (n is a terminal node) { handle n and return; } first time = true; while (1) { (*) /* determine whether thpn and thdn are increased. */ (*) if (n is a leaf) inc flag = false; (*) if (n has an unproven old child) inc flag = true; (*) expand and compute pn(n) and dn(n); (*) if (first time && inc flag) { (*) /* increase thresholds */ (*) thpn = max(thpn, pn(n) + 1); (*) thdn = max(thdn, dn(n) + 1); (*) } if (pn(n) ≥ thpn || dn(n) ≥ thdn) break; // termination condition is satisfied (*) first time= false;",
                "find the best child n1 and second best child n2; if (n is an OR node) { /* set new thresholds */ thpn child = min(thpn, pn(n2) + 1); thdn child = thdn -dn(n) + dn(n1); else { thpn child = thpn -pn(n) + pn(n1); thdn child = min(thdn, dn(n2) + 1); } DFPNwithTCA(n1, thpn child, thdn child, inc flag); } save pn(n) and dn(n) in TT. }  Figure 3 <figure> : Pseudo code of df-pn with TCA One might argue that increasing thresholds may result in exploring the larger search space that may not efficiently help prove or disprove the root. However, the threshold control scheme of TCA is based on the observation that most nodes have only normal children. As we will see in the next section, TCA empirically performs better than df-pn(r)."
            ],
            [
                "Because TCA does not ignore old children, the overestimation problem tends to occur more frequently than in df-pn(r). The Source Node Detection Algorithm (SNDA) overcomes this problem by generalizing Nagai's method, while avoiding underestimation that appears in his method, and computing (dis)proof numbers more accurately than WPNS.",
                "As in  (Nagai 2002) <ref id=#b13> , SNDA contains one pointer to one parent in each TT entry. SNDA detects a source node n s in the same way. However, unlike in Nagai's method, each TT entry contains the fixed number N of moves.",
                "Assume that c 1 and c 2 are n s 's children created by respectively making moves m 1 and m 2 at n s . SNDA detects that n s is a source node leading to a destination node n d , if n s and c 1 are on the path currently taken by df-pn search, c 2 is reached by a recursive procedure of traversing parent pointers starting from n d , and the pointer in c 2 's TT entry points to n s . m 1 and m 2 are then saved in the TT entry of n s , because they eventually lead to n d . If another move m 3 is detected at n s , m 3 is additionally saved in n s 's TT entry, as long as less than N moves are retained.",
                "Let n 1 , • • • , n l be n's children obtained by making moves in the TT entry of n, and n l+1 , • • • n k be other children of n. SNDA computes pn(n) and dn(n) as follows:",
                "<formula_4>",
                "pn(n j ) (AND node)",
                "In  Figure 2 <figure id=#fig_0> , SNDA saves moves A → C and A → F in A's TT entry when detecting that A is a source node of F . dn(A) is now accurately computed because:",
                "<formula_5>",
                "Also, dn(G) = dn(J) is derived by SNDA."
            ],
            [
                "As in  (Kishimoto and Müller 2003) <ref id=#b7> , TCA updates the minimal distance of node n if n has only unproven old children. There are several choices when implementing SNDA. We choose the best possible one, based on the results obtained by our preliminary implementations.",
                "In the worst case, the complexity of operations to traverse pointers is equal to the maximum search depth of all previous search. Moreover, this checking process is performed whenever SNDA detects a destination node n d reached via a parent that is not in n d 's TT entry. This is because many paths to n d exist as well as whether a source node n s exists or not is determined only by comparing a node on the current path of depth-first search with a set of nodes obtained by traversing the pointers in the TT. Since detecting n s incurs intensive computations, we incorporate a strategy in  (Nagai 2002) <ref id=#b13> . This strategy checks if pn(n d ) or dn(n d ) is involved in computing pn(n s ) or dn(n s ). If n d is not involved, the procedure of detecting n s is immediately terminated.",
                "Assume that move m is saved in n s 's TT entry. While m was a cause of overestimating pn(n s ) or dn(n s ) in previous iterations, it may be unrelated to overestimating pn(n s ) or dn(n s ) in the next iteration. One choice is to remove m from the TT entry in this case. However, m is preserved once m is saved in the TT entry in our implementation, because removing m returned larger (dis)proof numbers, and degraded the performance. Similarly, the implementation retains the parent pointer once it is saved in the TT entry.",
                "Each TT entry contains at most 10 moves within a total of 64 bit integer, which is possible due to a small average branching factor (about 5) in tsume-shogi. The move indexes obtained by sorting moves are saved in the TT entry to have consistent results for the identical node via different paths. If more than 10 moves exist causing overestimation, 10 best moves are selected by shogi-specific knowledge."
            ],
            [],
            [
                "Many state-of-the-art techniques are integrated with all of our implementations including  (Kawano 1996; <ref id=#b6> Seo 1999; <ref id=#b20> Nagai 2002; <ref id=#b13> Kaneko et al. 2005; <ref id=#b4> Kishimoto and Müller 2004; <ref id=#b8> . Overestimation is sometimes easily detected, such as the case where both players keep capturing pieces at a certain square on the board. These are handled by shogispecific techniques. The remaining overestimation problem is hard to detect by shogi-specific knowledge.",
                "Unlike  (Ueda et al. 2008) <ref id=#b21> , our WPNS implementation includes an evaluation function h(n) based on the material balance to heuristically initialize proof and disproof numbers at leaf node. Therefore, we slightly change the computation of WPNS to add (b − 1) × h(n), instead of adding (b − 1) to pn(n) or dn(n). This modification improves WPNS by a large margin. Additionally, h(n) is incorporated into all the versions.",
                "78 notoriously difficult tsume-shogi instances composed by tsume-shogi creators are selected from  (Kato 2009) <ref id=#b5> , including ones that turned out to have no mating sequence or shorter unexpected solutions, because many of them are still non-trivial. The solution lengths range between 300 and 1525 ply. The instances involve complicated DAG and cycles that occur after tsume-shogi solvers search over tens of ply. Moreover, the test suite includes several instances that have not been solved by any other tsume-shogi solver 3 .",
                "Our experiments were run on a 2.66GHz Xeon L5410 with 6MB L2 cache with the time limit of 50,000 seconds (about 13.9 hours) per instance with a 2GB transposition table. We prepare several versions to compare performance.  Table 1 <table id=#tab_0>  summarizes the number of unsolved instances. The table confirms that df-pn with TCA solves more instances than df-pn(r). TCA solves 40 instances more quickly than df-pn(r) of 63 instances solved by both versions."
            ],
            [
                "A typical observed example showing that df-pn(r) fails in solving instances due to underestimation is explained with the help of  Figure 4 <figure> . Assume that B must be proven to prove the root, C has two unproven children D and E, which are typically created by moving kings in two directions, and E is old because of a shorter path via F , which is typically created by moving a promoted bishop or rook. Because dfpn(r) underestimates pn(C) = pn(D) by ignoring E, it selects C at A and delays expanding B. In contrast, TCA selects B because pn(C) = pn(D) + pn(E) > pn(D).",
                "Due to the overestimation problem caused by DAG, df-pn with TCA occasionally suffers from a phenomenon in which 3  (Nagai 2002) <ref id=#b13>  claims that his solver solved all instances with solution lengths of over 300 ply, which were available at the year of 2002. Not only quite a few difficult instances are later composed by tsume-shogi creators, but also Nagai's remarkable results are unfortunately known to be unreproducible, partly due to a lack of descriptions on the infinite loop problem. For example, although  (Okabe 2005 <ref id=#b14> ) develops one of the best solvers, his solver cannot still solve at least 6 instances in our test suite. Moreover, neither the source nor executable code of Nagai's solver is publicly available.   Figure 4 <figure> : A typical case of underestimation on df-pn(r) the (dis)proof number at the root exceeds a large integer value(=200,000,000), which is defined as ∞ in our implementation to represent proof and disproof numbers within 32 bit integer. Of 8 unsolved instances, TCA is unable to solve 5 instances because of the phenomenon. This demonstrates that it is necessary to handle overestimation. Interestingly, while including a method to detect overestimation (i.e., NAGAI, WPNS, or SNDA) improves the solving ability of df-pn with TCA, df-pn(r) with SNDA solves much fewer instances than standard df-pn(r). We observed that df-pn(r) with SNDA occasionally suffers from severer underestimation and results in poor performance. For example, df-pn(r) happens to accurately compute dn(A) = dn(B) + dn(E) + dn(F ) for unproven B, E, and F in  Figure 2 <figure id=#fig_0> , because dn(D) = dn(E). With SNDA, df-pn(r) underestimates dn(A) = dn(B)+max(dn(C), dn(F )) = dn(B) + max(dn(E), dn(F )), because SNDA detects the existence of source node A from destination F .",
                "In  Table 1 <table id=#tab_0> , it is not surprising to confirm that SNDA is better than NAGAI to be integrated with df-pn + TCA, because SNDA extends NAGAI. However, WPNS with df-pn + TCA solves the same set of instances solved by SNDA. In most cases, WPNS achieves comparable performance to SNDA. Of 77 problems solved by both, SNDA solves 38 problems more quickly, while WPNS solves 39 problems more quickly. On average, the node expansion rate with WPNS is about 37% larger than that with SNDA. However, when instances become extremely hard to solve, SNDA is more important.  Tables 2 and 3 show the  Table 3 <table id=#tab_1> : Node expansions on the hardest problems for df-pn + TCA + WPNS to solve."
            ],
            [
                "TCA + WPNS TCA + SNDA  775, <ref> 332, <ref> 911 1, <ref> 045, <ref> 623, <ref> 765 Sekitoba 3, <ref> 260, <ref> 072, <ref> 126 1, <ref> 048, <ref> 715, <ref> 194 Megalopolis 7, <ref> 479, <ref> 994, <ref> 586 853, <ref> 059, <ref> 521 <ref>  hardest instances for df-pn + TCA + WPNS. SNDA solves these instances 2.5-6.2 times more quickly than WPNS, because WPNS expands 3.1-8.8 times more nodes than SNDA. In solving Meta-Shinsekai, while WPNS is expected to underestimate proof numbers, SNDA returns smaller proof numbers than WPNS on several positions located on the solution sequence in our analysis. One hypothesis is that slight overestimation of proof numbers in WPNS is accumulated when proof numbers are backed up to the root with very long sequence, and eventually becomes non-negligible 4 . One important note is that both SNDA and WPNS must be integrated with TCA. While SNDA and WPNS often avoid the infinite loop problem by making proof and disproof numbers smaller, they do not always handle infinite loops. If we switch off TCA, the number of unsolved instances is increased to 7 for both df-pn + SNDA and df-pn + WPNS. Of 7 unsolved instances, each version seems to be unable to solve 5 instances because of the infinite loop problem.",
                "To the best of our knowledge, three instances (\"Megalopolis\" (modified version, solution length of 515 ply), \"Journey to Jupiter\" (411 ply), and \"Atlantis\" (951 ply)) are newly solved only by our solver (13,590 seconds, and 5,481 seconds, and 210 seconds by df-pn + TCA + SNDA). Also, our solver solves all the problems that are solved by Nagai's solver but that remain unsolved by any other solvers. Such examples include Meta-Shinsekai and Sekitoba in  Table 2 <table id=#tab_1> ."
            ],
            [
                "This paper presents practical solutions to handle infinite loops, underestimation, and overestimation occurring in depth-first proof-number search. Promising results are obtained in solving notoriously difficult tsume-shogi problems.",
                "One obvious research direction is to apply TCA and SNDA to other domains. Tsume-Go is an ideal domain, because the best solver is based on df-pn(r)  (Kishimoto and Müller 2005) <ref id=#b9> . Also, because our TCA also improves WPNS, which has an advantage of the faster node expansion rate than SNDA, a hybrid approach of SNDA and WPNS is another research possibility. Finally, as an open question,  (Kishimoto and Müller 2008) <ref id=#b10>  describes that df-pn(r) is a strong candidate for guaranteeing the completeness on solving any finite DCG. Our remedy for infinite loops is also a candidate to investigate theoretical completeness properties."
            ]
        ],
        "Tables": [
            {
                "id": "tab_0",
                "Head": "Table 1 :",
                "Label": "1",
                "Description": "The number of problems unsolved by each method"
            },
            {
                "id": "tab_1",
                "Head": "Table 2 :",
                "Label": "2",
                "Description": "Execution time (seconds) on the hardest problems for df-pn + TCA + WPNS to solve"
            }
        ],
        "Figures": [
            {
                "id": "fig_0",
                "Head": "Figure 2 :",
                "Label": "2",
                "Description": "Figure 2: An example of the problems of underestimation (left) and overestimation (right)"
            }
        ],
        "References": [
            {
                "id": "b0",
                "Title": "Proof-number search",
                "Authors": [
                    "L Allis",
                    "M Van Der Meulen",
                    "H Van Den Herik"
                ],
                "Journal": "Artificial Intelligence"
            },
            {
                "id": "b3",
                "Title": "Computer shogi",
                "Authors": [
                    "H Iida",
                    "M Sakuta",
                    "J Rollason"
                ],
                "Journal": "Artificial Intelligence"
            },
            {
                "id": "b4",
                "Title": "Dfpn with fixed-depth search at frontier nodes",
                "Authors": [
                    "T Kaneko",
                    "T Tanaka",
                    "K Yamaguchi",
                    "S Kawai"
                ]
            },
            {
                "id": "b6",
                "Title": "Using similar positions to search game trees",
                "Authors": [
                    "Y Kawano"
                ]
            },
            {
                "id": "b7",
                "Title": "Df-pn in Go: Application to the one-eye problem",
                "Authors": [
                    "A Kishimoto",
                    "M Müller"
                ]
            },
            {
                "id": "b8",
                "Title": "A general solution to the graph history interaction problem",
                "Authors": [
                    "A Kishimoto",
                    "M Müller"
                ]
            },
            {
                "id": "b9",
                "Title": "Search versus knowledge for solving life and death problems in Go",
                "Authors": [
                    "A Kishimoto",
                    "M Müller"
                ]
            },
            {
                "id": "b10",
                "Title": "About the completeness of depth-first proof-number search",
                "Authors": [
                    "A Kishimoto",
                    "M Müller"
                ]
            },
            {
                "id": "b12",
                "Title": "Proof-set search",
                "Authors": [
                    "M Müller"
                ]
            },
            {
                "id": "b14",
                "Title": "Application of the route branch number for solving tsume shogi problems",
                "Authors": [
                    "F Okabe"
                ]
            },
            {
                "id": "b15",
                "Title": "Checkers is solved",
                "Authors": [
                    "J Schaeffer",
                    "N Burch",
                    "Y Björnsson",
                    "A Kishimoto",
                    "M Müller",
                    "R Lake",
                    "P Lu",
                    "S Sutphen"
                ],
                "Journal": "Science"
            },
            {
                "id": "b16",
                "Title": "A gamut of games",
                "Authors": [
                    "J Schaeffer"
                ],
                "Journal": "AI Magazine"
            },
            {
                "id": "b17",
                "Title": "Proofnumber search and transpositions",
                "Authors": [
                    "M Schijf",
                    "L Allis",
                    "J Uiterwijk"
                ],
                "Journal": "ICCA Journal"
            },
            {
                "id": "b19",
                "Title": "The PN * -search algorithm: Application to tsume-shogi",
                "Authors": [
                    "M Seo",
                    "H Iida",
                    "J Uiterwijk"
                ],
                "Journal": "Artificial Intelligence"
            },
            {
                "id": "b20",
                "Title": "On effective utilization of dominance relations in tsume-shogi solving algorithms",
                "Authors": [
                    "M Seo"
                ]
            },
            {
                "id": "b21",
                "Title": "Weak proof-number search",
                "Authors": [
                    "T Ueda",
                    "T Hashimoto",
                    "J Hashimoto",
                    "H Iida"
                ]
            }
        ]
    },
    "a low false negative filter for detecting rare bird species from short video segments using a probable observation data set-based ekf method *": {
        "Title": "a low false negative filter for detecting rare bird species from short video segments using a probable observation data set-based ekf method *",
        "Authors": [
            "Dezhen Song",
            "Yiliang Xu"
        ],
        "Abstract": "We report a new filter for assisting the search for rare bird species. Since a rare bird only appears in front of the camera with very low occurrence (e.g. less than ten times per year) for very short duration (e.g. less than a fraction of a second), our algorithm must have very low false negative rate. We verify the bird body axis information with the known bird flying dynamics from the short video segment. Since a regular extended Kalman filter (EKF) cannot converge due to high measurement error and limited data, we develop a novel Probable Observation Data Set (PODS)-based EKF method. The new PODS-EKF searches the measurement error range for all probable observation data that ensures the convergence of the corresponding EKF in short time frame.The algorithm has been extensively tested in experiments. The results show that the algorithm achieves 95.0% area under ROC curve in physical experiment with close to zero false negative rate.",
        "Keywords": [],
        "BookMarks": [
            "Introduction",
            "Related Work",
            "Problem Description",
            "Assumptions",
            "Inputs and Output",
            "Modeling a Flying Bird",
            "Probable Observation Data Set-based EKF Extended Kalman Filter",
            "EKF Convergence",
            "Probable Observation Data Set-based EKF Method",
            "Approximate Computation for PODS-EKF",
            "Experiments",
            "Simulation",
            "Physical Experiments",
            "Conclusion"
        ],
        "Papertext": [
            [
                "Our group focuses on developing autonomous observatories to assist nature scientists to search rare birds. In our recent project, a camera was installed in the middle of a forest in Bayou DeView in eastern Arkansas, running 24 hours a day, to assist the ornithologists to search for the thought-to-beextinct ivory-billed woodpecker (IBWO) (see  Fig. 1 <figure> ).",
                "Three critical conditions must be met for the searching task. First, a rare bird only appears in front of the camera with very low occurrence (e.g. less than ten times per year) for very short duration (e.g. less than a fraction of a second), our algorithm must have very low false negative rate. Second, since the final verification has to be performed by human experts, it is necessary to reduce the huge data volume to a manageable size, which also means that the filter can tolerate a less ideal false positive rate. Third, the system must be easy to setup in the forest. Due to power and communication constraints, a single camera is preferred because  Figure 1 <figure> : An example of a short video sequence of a flying bird that is captured in Bayou DeView in eastern Arkansas. It superimposes the segmented bird images from consecutive video frames on the top of the background frame.",
                "it does not require the precise calibration and synchronization as dislocated stereo rigs would for distant birds.  Fig. 1 <figure>  shows the input of the problem is a short segmented motion sequence of an object. The output of the problem is to determine whether the motion sequence is caused by a targeted bird species. We verify the bird body axis information with the known bird flying dynamics. Since a regular extended Kalman filter (EKF) cannot converge due to the high measurement error and the limited observation data due to the high flying speed of the bird (e.g. the sample bird sequence in  Fig. 1 <figure>  only contains seven data points), we develop a probable observation data set (PODS)-based EKF and an approximate computation scheme. The new PODS-EKF searches the measurement error range for all probable observation data that ensure the convergence of the corresponding EKF. The filtering is based on whether the PODS is non-empty and the corresponding velocity is within the known bird flying velocity profile. We show that the PODS-EKF theoretically ensures a zero false negative rate. We have evaluated the filtering algorithm using both the simulated data and field test data."
            ],
            [
                "Animal detection and recognition using video images has been a active research direction. Most of the existing approaches build appearance models of animals by silhou-  Figure 2 <figure> : An illustration of bird filtering. The motion sequence of the bird can be used to extract a set of moving line segments that correspond to the body axis of the bird. The segmentation error of the end of body axis is uniformed distributed in the u-v image plane and can be represented as an inverse pyramid when the error range is back-projected from the camera center to the 3D space. ette/contour  (Sebastian, Klein, and Kimia 2004) <ref id=#b7> , 2D kinematic chains of rectangular segments  (Ramanan, Forsyth, and Barnard 2006) <ref id=#b3>  etc. A known set of animal images are used to train and test the model using techniques such as clustering  (Ramanan, Forsyth, and Barnard 2006) <ref id=#b3> , template matching  (Sebastian, Klein, and Kimia 2004) <ref id=#b7>  etc. Different from this class of techniques, a large learning set is unavailable for our applications such as detecting rare birds.",
                "Periodic motion detection  (Ran et al. 2007; <ref id=#b4> Briassouli and Ahuja 2007) <ref id=#b1>  assumes objects, such as animals, with periodic motion pattern and applies time-frequency analysis  (Briassouli and Ahuja 2007) <ref id=#b1>  or image sequence alignment  (Laptev et al. 2005) <ref id=#b2>  to capture the periodicity. Applications of periodic motion detection have been found to vehicles, humans, and even canines. However, recognizing birds is different because a bird flying pattern combines both gliding and wing-flapping and the periodic motion assumption does not hold.",
                "Recently, the 3D structure inference using monocular vision has drawn increasing research attention.  Ribnick et al. (Ribnick, Atev, and Papanikolopoulos 2009) <ref id=#b5>  propose an algorithm for estimating the 3D parabolic trajectories of projectiles in monocular views. Saxena et al.  (Saxena, Sun, and Ng 2008) <ref id=#b6>  propose a learning algorithm that estimates 3D structures of a static scene based on a single still image. The work models the scene with sets of planes using Markov Random Field (MRF) and trains the model based on depth cues such as texture variations & gradients, color, haze, and defocus etc. Different from these works, our approach deals with a highly dynamic object (i.e., the bird) and its trajectory is not necessarily parabolic.",
                "Visual tracking has been a active research area. Various techniques have been proposed and a recent survey can be found in  (Yilmaz, Javed, and Shah 2006) <ref id=#b12> . The fundamental technique we used is the extended Kalman filer. (Extended) Kalman filter  (Spinello, Triebel, and Siegwart 2008) <ref id=#b11>  and their variations can be viewed as model-based filtering methods that verify the prior known dynamic models of the objects. It has seen a wide range of applications in object tracking and recognition such as vehicles, pedestrians etc. However, there is no existing work regarding how to detect a flying bird. Most existing works assume rigid objects and do not worry about the convergence of Kalman filter because an ample amount of observation data is available. Unfortunately, those conditions do not hold for the filtering of a highly-dynamic and high-speed flying bird.",
                "Our group has developed systems and algorithms  (Song 2009; <ref id=#b10> Song, van der Stappen, and Goldberg 2006; <ref id=#b9> Song and Goldberg 2007) <ref id=#b8>  for networked robotic cameras for a variety of applications such as construction monitoring, distance learning, panorama construction, and nature observation. Our previous work  (Song et al. 2008 <ref id=#b9> ) details how to build an autonomous nature observation system using motion detection. We learn that mere motion detection cannot save the biologists from the herculean task of image sorting, which inspires this work."
            ],
            [
                "Our system is a monocular vision system with a narrow field of view (FOV). The position of objects with respect to the camera is based on a 3D Cartesian camera coordinate system (CCS) with its origin at the camera center as shown in  Fig. 2 <figure> . The x-axis and y-axis of the CCS are parallel to the u-axis and the v-axis of the image coordinate system, respectively.",
                "From the knowledge provided by ornithologists, we know that a flying bird is usually an adult bird. A bird does not change its size once reaching its adulthood. Birds of the same species share a similar size and flying speed range. This biological information allows us to distinguish the targeted species from other moving objects."
            ],
            [
                "To establish the bird detection problem, we also have the following assumptions, • A fixed and pre-calibrated camera is used. With a calibrated camera and without loss of generality, we can always transform camera intrinsic parameter matrix K c to diag(f, f, 1), where f is the focal length of the camera in units of pixel side length. ICS must have its origin located on the principal axis (z axis) of CCS. Hence we have perspective project matrix",
                "<formula_0>",
                "• There is only one bird in the image sequence. If there are multiple birds in the scene, we assume each individual bird sequence has been isolated out using multiple object tracking techniques  (Yilmaz, Javed, and Shah 2006) <ref id=#b12> . • The bird is flying along a straight line with a constant speed when captured by the camera. This assumption usually holds considering a fast flying bird can only stay in the FOV for less than a second."
            ],
            [
                "The input of the problem is a sequence of n images which contain a moving object of any type. Each frame is timestamped. Based on the information from ornithologists, we know the body length l b and the flying speed",
                "<formula_1>",
                "of the targeted bird species. The output is to determine if the motion sequence is caused by the targeted bird species or not."
            ],
            [
                "To develop a bird filter, the key is to extract the bird flying dynamic information from the segmented bird motion sequence and associate the information with the known flying models and the prior information regarding the targeted species. As detailed in  (Song et al. 2008) <ref id=#b9> , we segment the moving object from its background and obtain a motion sequence. The most important bird flying information is the body axis. Based on the information provided by ornithologists and our data, we know that a flying bird maintains a fixed body length during flight and birds from the same species share a similar size. Let us define the bird body line segment in the image frame as",
                "<formula_2>",
                "is the head position and (u t , v t ) is the tail position. From z, we can compute the body axis orientation",
                "<formula_3>",
                ", and the body axis length",
                "<formula_4>",
                "Note that l is different from l b . l is the projection of l b on the image plane and is in unit of pixel. Since the bird body axis is almost parallel to the bird flying trajectory as in  Fig. 2 <figure> , we can extract the bird body axis using approaches such as Hough transform, detailed in  (Song et al. 2008) <ref id=#b9> .",
                "To determine whether the motion information is caused by the targeted species, we need to establish a bird flying model in the image frame. Let p = [x, y, z] T denote the head position of the bird body axis and v = [ẋ,ẏ,ż] T denote its velocity in the CCS. Since the bird flies along a straight line with a constant velocity, we havė",
                "<formula_5>",
                "where the state variable x = [p, v] T ∈ R 6 describes the position and velocity of the bird head. Define p tail = [x t , y t , z t ] T as the position of the bird tail and we have",
                "<formula_6>",
                "T . As illustrated in  Fig. 2 <figure> , the relationship between the measurement data z defined in (1) and the corresponding state x can be described using the pin-hole camera model,",
                "<formula_7>",
                "(3) where h(•) is usually called measurement function and w represents the measurement noise."
            ],
            [
                "The nonlinear dynamic system described by (3) captures the prior known information regarding the targeted species. If the motion is caused by the targeted species, then the bird body axis information should follow the nonlinear dynamic system described by (3), which can be validated using an EKF to track the states of the moving object. Eqs.  (2) and <formula id=#formula_5> (3) can be re-written in a discrete-time form,",
                "x(k + 1) = A(k + 1)",
                "<formula_8>",
                "where q(k) ∈ R 6 and w(k) ∈ R 4 represent the white Gaussian transition and measurement noises at time k with covariance matrix Q(k) ∈",
                "<formula_9>",
                ", and A(k + 1) is the state transition matrix at time k + 1,",
                "<formula_10>",
                "where ∆T (k + 1|k) is the time interval between time k and time k + 1. We define P ∈ R 6×6 as the covariance matrix for the state variable x. The EKF for the system in (4) can be implemented as a state prediction step x(k|k − 1),P (k|k − 1) and a measurement correction step x(k|k),P (k|k) recursively as follows,",
                "<formula_11>",
                "<formula_12>",
                "<formula_13>",
                "where K(k) ∈ R 6×4 is the \"Kalman gain\" and H(k) ∈ R 4×6 is the Jacobian matrix of h(•) in (3) with respect to x.",
                "<formula_14>",
                "T . For the n-image motion sequence, the predictedx(n|n) contains the bird velocityv(n|n). The decision of accepting (indicated as \"1\") or rejecting (indicated as \"0\") the moving object as a member of the targeted species is defined as the following indicator function,",
                "<formula_15>",
                "where Z 1:n = {z(1), z(2), ..., z(n)} is the set of body axes across n frames. Z 1:n is also referred to as the observed data. Eq. (6) basically states that the moving object is a member of the targeted species if the EKF converges to the desired absolute velocity range V."
            ],
            [
                "As indicated in (6), automatically determining whether the EKF converges or not is necessary. Define the estimated state set as X 1:n = {x(1|1),x(2|2), ...,x(n|n)}. Since velocity convergence implies position convergence andv(k|k) converges means v(k|k) −v(k − 1|k − 1) → 0, we determine the convergence of the EKF by inspecting",
                "<formula_16>",
                "where ω(k) > 0 is the weighting factor at time k. ω(k) is a monotonically-increasing function of k, which gives more weight to later states. ω(k) is usually pre-generated using simulated random inputs across the entire possible parameter range without measurement error (i.e. W (k) = 0 4×4 ). Setting W (k) = 0 4×4 is to ensure EKF convergency, which will be explained later in the paper. Denote v as the speed of the bird known in each trial of simulation. We repeat the EKF with randomized inputs for over 10 6 times to observe the quantity of",
                "<formula_17>",
                ", which is the inverse of the relative speed change at time k. We choose the weighting factor as ω",
                "<formula_18>",
                "where function E(•) computes the expected value over all simulation trials for the targeted species. When the EKF converges, v(k|k) −v(k − 1|k − 1) appears as a decreasing function of k after a few initial steps. Correspondingly, ω(k) is an increasing function of k. If v(k|k) −v(k − 1|k − 1) → 0, then ε(X 1:n ) is smaller than that of the case v(k|k) −v(k − 1|k − 1) 0. Therefore, to determine the EKF convergence, we employ a threshold δ on ε(X 1:n ) and introduce a new indicator variable,",
                "<formula_19>",
                "Then the decision-making in (6) is re-written as,",
                "<formula_20>",
                "The underlying condition for (8) to be an effective bird filtering mechanism is that if observation Z 1:n is caused by the targeted bird species then the convergence of the EKF has to be guaranteed. Unfortunately, the condition usually does not hold due to two reasons: n is small and the measurement noise w(k) is too big. Due to the fact that the bird flies very fast, the bird usually stays in the camera FOV for less than 1 second and thus n < 11 for most cases in our experiments. The measurement noise covariance matrix W (k) is directly determined by the image segmentation error. Even at 1 pixel level, its relative range is 4% for a bird body length of 25 pixels. For the nonlinear deterministic discrete time system in (4), the large W (k) means the EKF either fails to converge or converges very slowly according to  (Boutayeb, Rafaralahy, and Darouach 1997) <ref id=#b0> . The performance of the bird filter would be close to that of a random guess if the simple EKF implementation is used, which makes it useless."
            ],
            [
                "Since simply applying EKF cannot address the bird filtering problem, a new approach is required. Let us first assume there is no measurement noise (i.e. W (k) = 0 4×4 ) and no state transition noise Q(k) = 0 6×6 . At each time k, the EKF in (5) is a system of equations with four inputs, which is the dimensionality of z, and six outputs, which is the dimensionality of x. We also know that matrix A introduces two constraints: the constant speed and the linear trajectory. Therefore, the equation system can be solved within one step. The convergence of the EKF is not a problem when there is no noise and the bird does not fly in a degenerated trajectory (i.e. flying along the camera optical axis). Although usually Q(k) = 0 6×6 , the state transition noise q(k) is often very small, which leads to the following lemma, Lemma 1. The EKF described in (5) converges when W (k) = 0 4×4 . Proof. We skip the proof because our system in (4) is a linear time-invariant discrete time system with a nonlinear observer. The convergence of its EKF can be proved by the results in  (Boutayeb, Rafaralahy, and Darouach 1997) <ref id=#b0> .",
                "At first glance, this result is useless because we cannot get rid of the measurement noise. However, this result opens the door to a new approach. Define the observation data without measurement error as Z 1:",
                "<formula_21>",
                "Although we do not have Z 1:n * , we know it is within the segmentation error range of Z 1:n . For the k-th image, the measurement data is",
                "<formula_22>",
                "Define the error-free measurement data at time k as z",
                "<formula_23>",
                "Let us define the segmentation error is within τ pixels. The value of τ is usually small since the camera is set with short shuttle time and small iris.",
                "<formula_24>",
                ", and the segmentation error range at time k as S(k). Hence,",
                "<formula_25>",
                "Definition 1. Define the probable observation data set (PODS) Z 1:n as the set of observation data Z 1:n that satisfies the condition for the EKF convergence as in (7), Z 1:n = {Z 1:n |z(k) ∈ S(k), k = 1, ..., n, and ε(X 1:n ) < δ}.",
                "Obviously Z 1:n * ∈ Z 1:n . Each Z 1:n ∈ Z 1:n is likely to be Z 1:n * and hence it is named as the probable observation data. For a given PODS Z 1:n , there is a corresponding estimated state set X 1:n , which contains a set of all possible estimated velocities at time n, which is defined as V = { v(n|n) such that X 1:n ∈ X 1:n }. Then the decision making in (8) can be written as, I(Z 1:n ) = 1 if V ∩ V = ∅ and Z 1:n = ∅, 0 otherwise. Hence we have the following lemma, Lemma 2. If the non-degenerated observation data Z 1:n is triggered by a bird of the targeted species, then I(Z 1:n ) = 1.",
                "Proof. Since Z 1:n is triggered by the targeted species, its corresponding Z 1:n * ensures the convergence of the EKF according to Lemma 1. Define X 1:n * as the corresponding estimated states for Z 1:n * . Hence ε(X 1:n * ) < δ → Z 1:n = ∅, because Z 1:n * ∈ Z 1:n . Following our naming convention, v * (n|n) is the velocity component of X * (n|n) ∈ X 1:n * . Since the observation data is not degenerated, v * (n|n) ∈ V. We also know v * (n|n) ∈ V by definition, V ∩ V = ∅ holds. Since both conditions are satisfied, I(Z 1:n ) = 1.",
                "Lemma 2 ensures that the PODS-EKF method theoretically has a zero false negative rate, which is a very desirable property for the purpose of searching for rare bird species."
            ],
            [
                "Computing the PODS Z 1:n is nontrivial. Note that we actually do not need Z 1:n because all we need to know is whether the conditions V ∩ V = ∅ and Z 1:n = ∅ hold or not. This allows an approximation method. For a given observation Z 1:n , we define the following optimization problem,",
                "<formula_26>",
                "where Z 1:n is the optimal solution to the minimization problem above. Actually, (9) is a typical nonlinear optimization problem with the error range z(k) ∈ S(k); k = 1, ..., n and the EKF in (5) as constraints. Define X 1:n = { x(1|1), x(2|2), ..., x(n|n)} as the estimated states corresponding to Z 1:n . We have the following lemma, Lemma 3. ε( X 1:n ) > δ ⇐⇒ Z 1:n = ∅.",
                "Proof. Since (9) is a minimization problem, X 1:n yields the minimal ε(X 1:n ), namely, ε( X 1:n ) > δ ⇐⇒ ε(X 1:n ) > δ, ∀X 1:n ∈ X 1:n ⇐⇒ Z 1:n = ∅.",
                "It is worth mentioning that this method is an approximation in computation because the nonlinear programming solver often falls in a local minimum. Now we want to determine whether V ∩ V = ∅. If we view the EKF output v(n|n) as a function of Z 1:n , it is continuous and differentiable with respect to each entry in Z 1:n . Since Z 1:n is actually very small, the variance of the speed in the set V is very small. Instead of comparing V to V, we select a value in V to check if it is in V. Define v(n|n) as the velocity component of x(n|n) ∈ X 1:n . The chosen value is the v(n|n) because it is readily available. Therefore, the approximation is v(n|n) ∈ V ⇐⇒ V ∩ V = ∅. Due to the approximation, the zero false negative rate cannot be guaranteed. However, the false negative rate is still very low under the approximation as shown in the experiments."
            ],
            [
                "We have implemented the PODS-EKF algorithm and tested the algorithm on both the simulated data and the real data from field experiments. The computer used in the test is a desktop PC with a Pentium(R) D 3.20GHz CPU and 2GB RAM. The PC runs Windows XP. The PODS-EKF has been implemented using Matlab 7. We choose Arecont Vision 3100 high resolution networked video cameras as the imaging devices. The camera runs at 11 fps with a resolution of 3 Mega-pixel per frame. The lens for the camera is a Tamron auto iris vari-focus lens with a focal length range of 10-40 mm. We adjust the lens to ensure a 20 • horizontal FOV."
            ],
            [
                "We first test our PODS-EKF using the simulated inputs.",
                "An intermediate step is to analyze the convergence of the EKF. We generate 10 6 random 3D bird trajectories with constant velocity, which intersect with the conic camera FOV. The trajectory segment intersecting with the camera FOV projects back to the image plane as the visual measurements. We simulate three types of birds: house sparrows, rock pigeons, and IBWOs  (Table 1) <table id=#tab_0> . The three species are small, medium, and large in size, respectively. Their speeds range from 24 to 64 km/h and cover the range of the most of existing bird species. The initial statex(0|0) is estimated by solving the linear equation system in (3) with the first at least 3 observations and an additional constraint/guess v(0|0) = E(V). Details are omitted for space constraint.  Fig. 3(a) <figure id=#fig_0>  shows the EKF convergence for rock pigeon in different configurations by tracking the signal v(k|k)−v , wherev is the true bird velocity known in simulation. It is shown that without image noise, the EKF nicely converges as Lemma 1 points out. With the image noise (τ = 1 pixel), the EKF diverges with both increasing signal mean and variance. The PODS-EKF on the other hand, ensures the EKF to converge very close to the noise-free case. This validates the foundation of our PODS-EKF method. Now we are ready to analyze the performance of PODS-EKF. We generate a set of random inputs to mimic three birds as in  Table 1 <table id=#tab_0> . We set a speed range from 15 to 85 km/h with an incremental step of 5 km/h and a bird size range from 10 to 60 cm with an incremental step of 2 cm. We set the segmentation error range τ = 1 pixel. For each setting of the input data, 20 trials are carried out. The average computation time for each trial is 5.6 seconds.  Fig. 3(b) <figure id=#fig_0>  demonstrates how the rates of false positive (FP) and false negative (FN) change according to δ. After δ > 1.0 × 10 6 , the FN rates can be reasonably controlled to be less than 10%, 4% and 1%, for house sparrow, rock pigeon and IBWO, respectively. This confirms that the approximation computation is reasonable. The reason PODS-EKF works worst for house sparrow is that with the same FOV in the simulation, the smallest house sparrow leads to the highest noise-signal ratio, indicated as E(τ /l) in  Fig. 3(b) <figure id=#fig_0> . Our PODS-EKF is not biased for particular bird. To cope with small birds, we can increase the focal length to reduce E(τ /l). The FP rates of the PODS-EKF converge to 23%, 45% and 38%, respectively, which are a little high. However, considering that we are comparing the targeted bird with birds similar in size and speed, this result is not surprising. In fact, the algorithm should behave better in real tests where the noise from the moving objects has much larger range in both size and speed. Furthermore, the monocular system has its problem in detecting objects with their trajectories close to the camera optical axis, which also contributes to the high FP rate."
            ],
            [
                "We have conducted a field experiment of detecting flying rock pigeons. With a camera setup in the forest in Bayou DeView in eastern Arkansas, we have captured 119 events with n ≥ 8 for each motion sequence. 29 of the sequences are rock pigeons while the other 90 are not pigeons, which are image sequences of typical environment noises such as falling leaves, flying insects, and other bird species.  Fig. 4(a) <figure id=#fig_1>  shows how the FN and FP rates change according to difference δ. The convergence threshold is set as δ = 1.35 × 10 6 . It is shown that our algorithm can achieve extremely low FN rate (0/29 = 0%). This is very important for the purpose of finding rare bird species. The FP rate is 9/90 = 10%, which is better than that of the simulation results. This is due to the fact that it is much easier for the algorithm to distinguish the targeted species from noises such as flying insects and falling leaves in real experiment rather than from similar birds as in simulation above.  Fig. 4(b) <figure id=#fig_1>  illustrates the ROC curves from both the simulation and physical experiment for the rock pigeon. The convergence threshold range is [4.6×10 3 , 1.5×10 6 ] for simulation and [1.8 × 10 4 , 3.3 × 10 6 ] for experiment. The areas under the ROC curve are 91.5% and 95.0% for simulation and physical experiment, respectively, which again shows the algorithm performs better in physical experiments.",
                "We apply the algorithm to our IBWO search field data  (Oct. 2006 <ref> -Oct. 2007 <ref> . After initial motion detection filtering as in  (Song et al. 2008) <ref id=#b9> , we reduce the total video data of 29.41 TB to 27.42 GB, which is still prohibitively huge for human experts. After applying the PODS-EKF, we eventually reduce the data volume to 146.7 MB (about 960 images) with close to zero false negative. The overall reduction rate is 99.9995%."
            ],
            [
                "We reported our development of a bird filtering algorithm. We developed a novel Probable Observation Data Set (PODS)-based EKF method to ensure the convergence of the EKF under insufficient and noisy measurement data. The algorithm has been extensively tested using both simulated inputs and physical experiments. The results showed that the PODS-EKF bird filter has reduced the video data by 99.9995% with close to zero false negative and 95.0% area under the ROC curve in physical experiments."
            ]
        ],
        "Tables": [
            {
                "id": "tab_0",
                "Head": "Table 1 :",
                "Label": "1",
                "Description": "Species used in the experiments"
            }
        ],
        "Figures": [
            {
                "id": "fig_0",
                "Head": "Figure 3 :",
                "Label": "3",
                "Description": "Figure 3: (a) Convergence for different EKF configurations based on simulated rock pigeon data. (b) False positive (FP) and false negative (FN) rates with different δ in simulation."
            },
            {
                "id": "fig_1",
                "Head": "Figure 4 :",
                "Label": "4",
                "Description": "Figure 4: (a) Physical experiment for rock pigeons. (b) The ROC curves for both the simulation and the physical experiment. The corresponding areas under the ROC curves are 91.5% and 95.0%, respectively."
            }
        ],
        "References": [
            {
                "id": "b0",
                "Title": "Convergence analysis of the extended kalman filter used as an observer for nonlinear deterministic discrete-time systems",
                "Authors": [
                    "M Boutayeb",
                    "H Rafaralahy",
                    "M Darouach"
                ],
                "Journal": "IEEE Transactions on Automatic Control"
            },
            {
                "id": "b1",
                "Title": "Extraction and analysis of multiple periodic motions in video sequences",
                "Authors": [
                    "A Briassouli",
                    "N Ahuja"
                ],
                "Journal": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            {
                "id": "b2",
                "Title": "Periodic motion detection and segmentation via approximate sequence alignment",
                "Authors": [
                    "I Laptev",
                    "S Belongie",
                    "P Pérez",
                    "J Wills"
                ]
            },
            {
                "id": "b3",
                "Title": "Building models of animals from video",
                "Authors": [
                    "D Ramanan",
                    "D Forsyth",
                    "K Barnard"
                ],
                "Journal": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            {
                "id": "b4",
                "Title": "Pedestrian detection via periodic motion analysis",
                "Authors": [
                    "Y Ran",
                    "I Weiss",
                    "Q Zheng",
                    "L Davis"
                ],
                "Journal": "International Journal of Computer Vision (IJCV)"
            },
            {
                "id": "b5",
                "Title": "Estimating 3d positions and velocities of projectiles from monocular views",
                "Authors": [
                    "E Ribnick",
                    "S Atev",
                    "N Papanikolopoulos"
                ],
                "Journal": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            {
                "id": "b6",
                "Title": "Make3d: Depth perception from a single still image",
                "Authors": [
                    "A Saxena",
                    "M Sun",
                    "A Ng"
                ]
            },
            {
                "id": "b7",
                "Title": "Recognition of shapes by editing their shock graphs",
                "Authors": [
                    "T Sebastian",
                    "P Klein",
                    "B Kimia"
                ],
                "Journal": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            {
                "id": "b8",
                "Title": "Approximate algorithms for a collaboratively controlled robotic camera",
                "Authors": [
                    "D Song",
                    "K Goldberg"
                ],
                "Journal": "IEEE Transactions on Robotics"
            },
            {
                "id": "b9",
                "Title": "System and algorithms for an autonomous observatory assisting the search for the ivory-billed woodpecker",
                "Authors": [
                    "D Song",
                    "N Qin",
                    "Y Xu",
                    "C Kim",
                    "D Luneau",
                    "K Goldberg",
                    "D Song",
                    "F Van Der Stappen",
                    "K Goldberg"
                ]
            },
            {
                "id": "b11",
                "Title": "Multimodal people detection and tracking in crowded scenes",
                "Authors": [
                    "L Spinello",
                    "R Triebel",
                    "R Siegwart"
                ]
            },
            {
                "id": "b12",
                "Title": "Object tracking: A survey",
                "Authors": [
                    "A Yilmaz",
                    "O Javed",
                    "M Shah"
                ],
                "Journal": "ACM Computing Surveys"
            }
        ]
    },
    "interactive categorization of containers and non-containers by unifying categorizations derived from multiple exploratory behaviors": {
        "Title": "interactive categorization of containers and non-containers by unifying categorizations derived from multiple exploratory behaviors",
        "Authors": [
            "Shane Griffith",
            "Alexander Stoytchev"
        ],
        "Abstract": "",
        "Keywords": [],
        "BookMarks": [
            "Introduction",
            "Experimental Setup",
            "Methodology",
            "Results",
            "Conclusion and Future Work"
        ],
        "Papertext": [
            [
                "The ability to form object categories is an important milestone in human infant development  (Cohen 2003) <ref id=#b0> . We propose a framework that allows a robot to form a unified object categorization from several interactions with objects. This framework is consistent with the principle that robot learning should be ultimately grounded in the robot's perceptual and behavioral repertoire  (Stoytchev 2009) <ref id=#b3> . This paper builds upon our previous work  (Griffith et al. 2009 <ref id=#b1> ) by adding more exploratory behaviors (now 6 instead of 1) and by employing consensus clustering for finding a single, unified object categorization. The framework was tested on a container/non-container categorization task with 20 objects.",
                "Initial attempts at robotic object categorization have produced limited results as they assume that robots will explore objects using only a single behavior  (Griffith et al. 2009) <ref id=#b1> . Research with animals, however, has shown that some birds use almost their entire behavioral repertoire to explore a novel object  (Lorenz 1996) <ref id=#b2> . This suggests that robots should do the same when categorizing objects. Indeed, an object categorization derived from multiple exploratory behaviors may contain more information compared to one derived from a single behavior. Further work is necessary, however, to determine how a robot can combine its observations from multiple behaviors to come up with one unified categorization for a set of objects, instead of having a separate categorization for each behavior.",
                "This paper tests the hypothesis that a robot can use consensus clustering to form a single categorization for a set of objects after it interacts with them using multiple exploratory behaviors. Our robot performed a sequence of 6 exploratory behaviors during multiple interaction trials with 20 objects (10 containers and 10 non-containers). The robot extracted features from its interaction history with each object and then employed unsupervised clustering to form 6 different categorizations. Consensus clustering was used to combine the 6 different categorizations into a unified object categorization. This resulted in a meaningful separation of containers from non-containers, even in the presence of noisy clusterings. Before each trial the block and one of the twenty objects were placed at marked locations on the table. The robot performed each behavior in the order shown after it grasped the block and positioned its arm in the area above the object."
            ],
            [
                "The upper-torso humanoid robot shown in  Fig. 1 <figure>  was used for this study. Two 7-dof Barrett Whole Arm Manipulators (WAMs) were used for the robot's arms, each with the 3finger Barrett Hand as its end effector. A 3-D camera (ZCam by 3DV Systems) was mounted on the robot and used to capture both color and depth images of the environment.",
                "The robot interacted with one block and 20 objects placed on a table in front of it (see  Fig. 2 <figure> ). Half of the objects were containers (household containers or children's bucket toys); the other half were the same objects, only flipped over. So while there were only ten real objects the robot was exposed to 20 \"different\" objects from an interaction point of view. Each trial consisted of six behaviors with the block and one of the objects (see  Fig. 1 <figure> ). A total of 12,000 behavioral interactions were performed (6 exploratory behaviors per trial and 100 trials for each of the 20 objects)."
            ],
            [
                "The robot used the visual co-movement patterns of the block and the object in order to form object categories. A movement was detected when the position of the block or the position of the object changed by more than a threshold, δ, over a short temporal window. A box filter was used to remove noise from the movement detection data.",
                "The movement data for the block and the object was converted into a state sequence. The states of this sequence were four visual co-movement events: 1) neither object moved; 2) the block moved; 3) the object moved; or 4) they both moved. The robot acquired a set of 2,000 state sequences from the 2,000 trials for a given behavior. The robot recursively bi-partitioned the set of state sequences using the spectral clustering algorithm in order to learn visual outcome classes. The visual outcome classes, C = {c 1 , ..., c k }, are the leaf nodes of the tree created by the recursive algorithm. The robot repeated the same process for each of the six different exploratory behaviors in order to learn six different sets of visual outcome classes.",
                "Next, the robot categorized the objects using the frequency with which different visual outcomes occurred with each object. More formally, given a set of visual outcome classes, C = {c 1 , ..., c k }, each object, i, is described with a feature vector",
                "<formula_0>",
                "such that h i j is the number of outcomes from outcome class c j that were observed when interacting with object i. Objects are grouped into categories by unsupervised clustering of their H vectors using X-means. This process is repeated for each of the six different exploratory behaviors to form six different sets of object categories.",
                "Unifying the six object categorizations is a necessary step toward identifying a single behavior-grounded categorization of the objects. The best unified categorization is defined as the clustering that has the highest possible total normalized mutual information with the six input clusterings. Finding the best clustering, however, is intractable. Thus, it is necessary to search for a clustering that is approximately the best. For this task, we used the hard consensus clustering algorithm  (Strehl and Ghosh 2002) <ref id=#b4> . The algorithm takes as input the six clusterings formed for each of the six exploratory behaviors, searches for a good approximation, and outputs the best unified clustering that it finds."
            ],
            [
                "All six behaviors produced visual co-movement patterns that could be used for object categorization. Some behaviors captured the 'container' property better than others. The flip behavior, for example, led to a categorization that perfectly matched human labels. Next in order were move (3 incorrect classifications), shake (4 incorrect), drop object (5 incorrect), drop block (6 incorrect), and grasp (7 incorrect).",
                "The fact that some clusterings produced by the robot were noisy was expected. Some behaviors are simply better at capturing certain object properties than others. With 20 objects of various shapes, sizes, and materials there are many ways the robot could have categorized them. No behaviors, however, clearly separated objects by size or material. On the other hand, flip and move captured the 'container' functionality well.",
                "The result of unifying the six object categorizations is shown in  Fig. 2 <figure> . The consolidated clustering separated the objects into two groups, which closely correspond to what a human would call containers and non-containers. Only two  Figure 2 <figure> : Illustration of the unified object categorization formed by the robot. Only two objects were incorrectly classified (when compared with the ground-truth labels provided by a human and the majority class of the category).",
                "objects were misclassified, which shows that the consensus clustering algorithm was able to find a meaningful categorization even when only two behaviors produced a good clustering of the objects. The noisy clusterings produced by the other behaviors only marginally affected the consolidated clustering. Thus, the consensus clustering method successfully unified the clusterings from multiple behaviors in order to identify a single, behavior-grounded categorization."
            ],
            [
                "The experiments show that a robot can derive meaningful object categories by interacting with objects and observing their visual co-movement patterns. Although categorizations derived from some of the interactive behaviors were noisy, the consensus clustering algorithm identified a meaningful object categorization (relative to human labels). Thus, different categorizations derived from different exploratory behaviors can be combined into a single one using consensus clustering. The experiments and the findings of this paper are still preliminary. We plan to extend this study in the future by including auditory events in the framework and by analyzing the effects of different clustering algorithms. For additional details and results, see http://www.ece.iastate.edu/~shaneg/AAAI10/."
            ]
        ],
        "Tables": [],
        "Figures": [
            {
                "id": "fig_0",
                "Head": "",
                "Label": "",
                "Description": "Copyright © 2010, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. The six exploratory behaviors performed by the robot."
            }
        ],
        "References": [
            {
                "id": "b0",
                "Title": "Early category and concept development",
                "Authors": [
                    "L Cohen"
                ]
            },
            {
                "id": "b1",
                "Title": "Toward interactive learning of object categories by a robot: A case study with container and non-container objects",
                "Authors": [
                    "S Griffith",
                    "J Sinapov",
                    "M Miller",
                    "A Stoytchev"
                ]
            },
            {
                "id": "b2",
                "Title": "Learning as Self-Organization",
                "Authors": [
                    "K Lorenz"
                ]
            },
            {
                "id": "b3",
                "Title": "Some basic principles of developmental robotics",
                "Authors": [
                    "A Stoytchev"
                ],
                "Journal": "IEEE Transactions on Autonomous Mental Development"
            },
            {
                "id": "b4",
                "Title": "Cluster ensembles -a knowledge reuse framework for combining multiple partitions",
                "Authors": [
                    "A Strehl",
                    "J Ghosh"
                ],
                "Journal": "Journal of Machine Learning Research"
            }
        ]
    },
    "recognizing multi-agent activities from gps data": {
        "Title": "recognizing multi-agent activities from gps data",
        "Authors": [
            "Adam Sadilek",
            "Henry Kautz"
        ],
        "Abstract": "Recent research has shown that surprisingly rich models of human behavior can be learned from GPS (positional) data. However, most research to date has concentrated on modeling single individuals or aggregate statistical properties of groups of people. Given noisy real-world GPS data, we-in contrast-consider the problem of modeling and recognizing activities that involve multiple related individuals playing a variety of roles. Our test domain is the game of capture the flag-an outdoor game that involves many distinct cooperative and competitive joint activities. We model the domain using Markov logic, a statistical relational language, and learn a theory that jointly denoises the data and infers occurrences of high-level activities, such as capturing a player. Our model combines constraints imposed by the geometry of the game area, the motion model of the players, and by the rules and dynamics of the game in a probabilistically and logically sound fashion. We show that while it may be impossible to directly detect a multi-agent activity due to sensor noise or malfunction, the occurrence of the activity can still be inferred by considering both its impact on the future behaviors of the people involved as well as the events that could have preceded it. We compare our unified approach with three alternatives (both probabilistic and nonprobabilistic) where either the denoising of the GPS data and the detection of the high-level activities are strictly separated, or the states of the players are not considered, or both. We show that the unified approach with the time window spanning the entire game, although more computationally costly, is significantly more accurate.",
        "Keywords": [],
        "BookMarks": [
            "Introduction Motivation",
            "Our Approach",
            "Significance of Results",
            "H4",
            "H7",
            "Capture The Flag Domain",
            "Background Markov Logic",
            "Models",
            "Baseline Model",
            "Baseline Model with States",
            "Two-Step Model",
            "Unified Model",
            "Experiments and Results",
            "Conclusions and Future Work"
        ],
        "Papertext": [
            [
                "Imagine two teams-seven players each-playing capture the flag on a university campus, where each player carries a consumer-grade global positioning system (GPS) that logs its location every second (see  Fig. 1 <figure> ). Accuracy of the GPS data varies from 1 to more than 10 meters. In open areas, readings are typically off by 3 meters, but the discrepancy is much higher in locations with tall buildings or other obstructions. The error has a systematic component as well as a significant stochastic component. Errors between devices are poorly correlated, because subtle differences between players, such as the angle at which the device sits in Copyright c 2010, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.  Figure 1 <figure> : A snapshot of a game of capture the flag that shows the game area. Players are represented by pins with letters. In our version of CTF, the two \"flag areas\" are stationary and are shown as white circles near the top and the bottom of the figure. The horizontal road in the middle of the image is the territory boundary. The data is shown prior to any denoising or corrections for map errors. Videos of our recorded games are available from the first author's website. the player's pocket, can dramatically affect accuracy. Moreover, since we consider multi-agent scenarios, the errors in individual players' readings can add up, thereby creating a large discrepancy between the reality and the recorded dataset. Because players can move freely through open areas, we cannot reduce the data error by assuming that the players move along road or walkways, as is done in much work on GPS-based activity recognition (e.g.,  (Liao, Fox, and Kautz 2004) <ref id=#b6> ). Finally, traditional techniques for denoising GPS data, such as Kalman filtering, are of little help, due to the low data rate (1 sample per second) relative to the small amount of time required for a player to completely change her speed or direction.",
                "Given such raw and noisy data, can we automatically and reliably detect and recognize interesting events that happen within the game (such as one player capturing another player)? Moreover, can we jointly denoise the GPS data and infer instances of game events? In this paper, we present an approach that provides evidence for answering a resounding \"yes\" to both of the above questions.",
                "Our work is not primarily motivated by the problem of annotating strategy games, although there are obvious applications of our results to sports and combat situations. We are, more generally, exploring relational learning and inference methods for recognizing multi-agent activities from location data. We accept the fact that the GPS data at our disposal is inherently unreliable and ambiguous for any one individual. We therefore focus on methods that jointly and simultaneously localize and recognize the high-level activities of groups of individuals.",
                "GPS location data can be used to learn (perhaps surprisingly) much about human behavior. Most work to date has concentrated either on inferring activities performed by a single person at a sequence of locations  (Bui 2003; <ref id=#b2> Liao, Fox, and Kautz 2004; <ref id=#b6> 2005) <ref id=#b7> , or on how locations are typically used by anonymous groups of individuals  (Abowd et al. 1997) <ref id=#b0> . By contrast, we are interested in using GPS data to discover particular activities that involve multiple related individuals playing a variety of distinct roles. For example, consider modeling a situation that includes an elderly person interacting with a circle of caregivers. GPS data could be used to infer various kinds of caregiving activities, such as an adult son taking his mother clothes shopping-an event that would be virtually impossible to capture if we considered each person in isolation.",
                "Many different kinds of cooperative and competitive multi-agent activities occur in games. Because of the rich yet well defined nature of the domain, we had decided to begin our investigation of multi-agent activity recognition with GPS data we collected from people playing capture the flag (details of the data collection are below). The lowest-level joint activities are based on location and movement, and include \"approaching\" and \"being at the same location.\" Note, that noise in the GPS data often makes it difficult or impossible to directly detect these simple activities. At the next level come competitive multi-agent activities including capturing and attacking; cooperative activities include freeing; and there are activities, such as chasing and guarding, that may belong to either category or to both categories. There are also more abstract tactical activities, such as making a sacrifice, and overall strategies, such as playing defensively. In this paper, we concentrate on activities at the first two levels."
            ],
            [
                "We provide a unified framework for intelligent relational denoising of the raw GPS data while simultaneously labeling instances of a player being captured by an enemy. Both the denoising and the labeling are cast as a learning and inference problem in Markov logic. By denoising, we mean mod-ifying the raw GPS trajectories of the players such that the final trajectories satisfy constraints imposed by the geometry of the game area, the motion model of the players, as well as by the rules and the dynamics of the game. In this paper, we refer to this trajectory modification as \"snapping\" since we tile the game area with 3 by 3 meter cells and snap each raw GPS reading to an appropriate cell. By creating cells only in unobstructed space, we ensure the final trajectory is consistent with the map of the area.",
                "We express the constraints as weighted formulas in Markov logic (see section \"Models\" below). Some of the constraints are \"hard,\" in the sense that we are only interested in solutions that satisfy all of them. Hard constraints capture basic physical constraints (e.g., a player is only at one location at a time) and inviolable rules of the game (e.g., a captured player must stand still until freed or the game ends).  1 <ref>  The rest of the constraints are \"soft,\" meaning there is a finite weight associated with each one. Some of the soft constraints correspond to a traditional low-level data filter, expressing preferences for smooth trajectories that are close to the raw GPS readings. Other soft constraints capture high-level constraints concerning when individual and multi-agent activities are likely to occur. For example, a soft constraint states that if a player encounters an enemy on the enemy's territory, the player is likely to be captured. The exact weights on the soft constraints are learned from labeled data, as described below.  Fig. 2 <figure id=#fig_0>  gives an English description of our hard and soft constraints for the low-level movement and player capture rules of capture the flag.",
                "The most likely explanation of the data is one that satisfies all the hard constraints while maximizing the sum of the weights of the satisfied soft constraints. Inference is done simultaneously over an entire game (on average, about 10 minutes worth of data). Note that we do not restrict inference to a (small) sliding time window. As the experiments described below show, many events in this domain can only be definitely recognized long after they occur. For example, GPS noise may make it impossible to determine whether or not a player has been captured at the moment of the capture, but as the player thereafter remains in place for a long time, the possibility of his capture becomes certain."
            ],
            [
                "We show that while it may be impossible to directly detect a multi-agent activity due to sensor noise or malfunction, the occurrence of the activity can still be inferred by considering both its impact on the future behaviors of the people involved as well as the likelihood of the events that potentially preceded it. Our experiments demonstrate that a fairly short theory in Markov logic (which follows directly from the rules of the game) coupled with automatically learned weights, can reliably recognize instances of capture events even at times when other approaches largely fail. Nine out ten captures were correctly identified in tens of thousands of GPS readings and in the presence of hundreds of almostcaptures (situations that at first sight look like captures but Hard Rules:",
                "H1. The final trajectory is consistent with the map of the game area (i.e., players don't magically walk through walls).",
                "H2. Each raw GPS reading is snapped to exactly one cell.",
                "H3. No player is captured at the beginning of the game."
            ],
            [
                ". When player a1 captures player a2, then both involved players must be snapped to a common cell at that time.",
                "H5. A player can be captured only by an enemy who is on his or her home territory.",
                "H6. A player can be captured only when standing on enemy territory."
            ],
            [
                ". A player can be captured only if he or she is currently free.",
                "H8. A player transitions from a free state to a captured state via a capturing event.",
                "H9. A captured player remains captured until freed.",
                "H10. If a player is captured then he or she must remain in the same location.",
                "Soft Rules:",
                "S1. Minimize the distance between the raw GPS reading and the snapped-to cell.",
                "S2. Minimize projection variance, i.e., two consecutive \"snappings\" should be generally correlated.",
                "S3. Maximize smoothness (both in terms of space and time) of the final player trajectories.",
                "S4. If players a1 and a2 are enemies, a1 is on home territory, a2 is on a1's territory, a2 is not captured already, and they are close to each other, then a1 probably captures a2.",
                "S5. Capture events are generally rare, i.e., there are typically only a few captures within a game. after careful analysis turn out to be false alarms). Furthermore, we show that our approach is applicable even in sizeable domains and is easily decomposable and extensible. Even though the capture the flag domain doesn't capture all the complexities of life, most of the problems that we are addressing here clearly have direct analogues in more reallife tasks that artificial intelligence needs to address-such as improving smart environments, human-computer interaction, surveillance, assisted cognition, and battlefield control."
            ],
            [
                "We collected the CTF dataset by having subjects play three games of capture the flag on a university campus while carrying basic GPS loggers. Players were required to remain outdoors. We manually labeled instances of capture events in all three games based partly on our notes taken during the experiment and partly on our best judgement. We consider our labeling to be the ground truth for both training our models and for evaluation purposes.",
                "The visualization of a game is shown in  Fig. 1 <figure> . In our variant of CTF, we have two teams-each consisting of seven players. The south team's territory is the area south of the  Figure 3 <figure> : Three snapshots of a game situation that illustrate the need for an approach that exploits both the relational and the far reaching temporal structure of our domain. road in the middle of the map and analogously for the north team. The flags are actually stationary and are shown as white circles in  Fig. 1 <figure> . The goal is to enter the opponent's flag area within 15 minutes. Players can be captured only while on enemy territory by being touched by the enemy. Upon being captured, they must remain in place until freed or the game ends.",
                "If we are to reliably recognize interesting events that happen in these games, we need to consider not only each player individually but also the relationships among them over extended periods of time (possibly the whole length of the game). Consider a real game situation illustrated in  Fig. 3 <figure> . There we see three snapshots of a game projected over a map before any modification of the GPS data. The game time is shown on each snapshot. Players D, F, G are allies and are currently on their home territory near their flag, whereas players L and M are their enemies. In the first snapshot, players L and M head for the opponent's flag but then-in the second frame-they are intercepted by G. At this point it is unclear what is happening because of the substantial error in the GPS data-the three players appear to be very close to each other but in actuality they could have been 20 or more meters apart. However, once we see the third snapshot (note that tens of seconds have passed), in retrospect, we realize that player G actually captured only player M and didn't capture L since he is still chasing him. The fact that player M remains stationary coupled with the fact that neither D nor F attempts to capture him suggests that M has indeed been captured. Our unified model gives the correct labeling even for complex situations like these whereas limited approaches largely fail."
            ],
            [
                "Given the inherent uncertainty involved in reasoning about real-world activities as observed through noisy sensor readings, we looked for a methodology that would provide an elegant combination of probabilistic reasoning with expressive, relatively natural, and compact but unfortunately strictly true or false formulas of first order logic (FOL). And that is exactly what Markov logic provides thereby allowing us to elegantly model complex relational non-i.i.d. domains  (Richardson and Domingos 2006) <ref id=#b9> . A Markov logic network (MLN) consists of a set of constants C and of a set of pairs F i , w i such that each FOL formula F i has a weight w i ∈ R associated with it.",
                "Hard Formulas ∀a1, a2, t : capturing(a1, a2, t) ⇒ enemies(a1, a2)∧ onHomeTer(a1, t) ∧ onEnemyTer(a2, t)∧ (H4-H7) ¬isCaptured(a2, t) ∧ samePlace(a1, a2, t) ∀a1, a2, t : samePlace(a1, a2, t) ⇒ (H4)",
                "∃c : snap(a1, c, t) ∧ snap(a2, c, t) ∀a, t : ¬isCaptured(a, t) ∧ isCaptured(a, t + 1) ⇒ ∃=1a : capturing(a , a, t)",
                "Soft Formulas",
                "<formula_1>",
                "∀a,c1, c2, t : A MLN can be viewed as a template for a Markov network (MN) as follows: MN contains one node for each possible ground atom of MLN. Each weight w i in the MLN intuitively represents the relative \"importance\" of satisfying (or violating, if the weight is negative) the corresponding formula F i . Thus the problem of satisfiability is relaxed in MLNs. We no longer search for a satisfying truth assignment as in traditional FOL. Instead, we are looking for a truth assignment that maximizes the sum of the weights of all satisfied formulas.",
                "<formula_2>",
                "Maximum a posteriori inference in Markov logic given the state of the observed atoms reduces to finding a truth assignment to the hidden atoms such that the weighed sum of satisfied clauses is maximal. Even though this problem is in general #P-complete, we achieve reasonable run times by applying Cutting Plane MAP Inference (CPI)  (Riedel 2008) <ref id=#b10> . CPI can be thought of as a meta solver that incrementally and partially grounds a Markov logic network thereby creating a Markov network that is subsequently solved by any applicable method-such as MaxWalkSAT or via a reduction to an integer linear program. After obtaining this (possibly preliminary) solution, CPI searches for additional grounding that could contribute to the score."
            ],
            [
                "Since to date no results on attacking a comparable multiagent relational learning, denoising, and recognition problem have been published, we compare our unified approach with three alternative models. The first two models (baseline and baseline with states) are purely deterministic and they separate the denoising of the GPS data and the labeling of game events. We implemented both of them in Perl. They do not involve any training phase.",
                "On the other hand, the two-step model and the unified model are probabilistic and are both cast in Markov logic. The unified model handles the denoising and labeling in a joint fashion whereas the two-step approach first performs snapping given the geometric constraints and subsequently labels instances of capturing. The latter two models are evaluated using three-fold cross-validation where in order to test on a given game, we first trained our model on the two other games.",
                "Our models can access the following observed data: raw GPS position of each player at any time and indication whether they are on enemy or home territory, location of each 3 by 3 meter cell, cell adjacency, and list of pairs of players that are enemies. We tested all four models on the same raw GPS data. The following subsections describe each of the four approaches in more detail."
            ],
            [
                "This model has two separate stages. First we snap each reading to the nearest cell and afterward we label the instances of player a capturing player b. The labeling rule is simple: we loop over the whole discretized (via snapping) data set and output capturing(a,b,t) every time we encounter a pair of players a and b such that they were snapped (in the first step) to either the same cell or to two mutually adjacent cells at time t, they are enemies, and a is on its home territory while b is not."
            ],
            [
                "This second model builds on top of the previous one by introducing a notion that players have states. If player a captures player b at time t, b enters a captured state (in logic, isCaptured(b,t+1)). Then b remains in captured state until it moves (is snapped to a different cell at a later time) or the game ends. As per rules of CTF, a player who is in captured state cannot be captured again. Thus, this model works just like the previous one except whenever it is about to label a capturing event, it checks the states of the involved players and outputs capturing(a,b,t) only if both a and b are not in captured state."
            ],
            [
                "In the two-step approach, we have two separate theories in Markov logic. The first theory is used to perform a preliminary snapping of each of the player trajectories individually using constraints H1, H2, and S1-S3. The second theory then takes this preliminary denoising as a list of observed atoms in the form preliminarySnap(a,c,t) (meaning player a is snapped to cell c at time t) and uses the remaining constraints to label instances of capturing, while considering cell adjacency in the same manner as the baseline model. The two-step model constitutes a decomposition of the unified model (see below) and overall contains virtually the same formulas, thus we omit elaborating on it here."
            ],
            [
                "In the unified approach, we express all the hard constraints H1-H10 and soft constraints S1-S5 in Markov logic as a single theory that jointly denoises the data and labels game events. Selected interesting formulas are shown in  Fig. 4 <figure id=#fig_1> their labels correspond to the listing in  Fig. 2 <figure id=#fig_0> . Note that formulas S1 and S2 contain real-valued functions d 1 and d 2 respectively. (Markov logic networks that contain such functions are called hybrid MLNs.) d 1 returns the distance between agent a and cell c at time t. Similarly, d 2 returns the dissimilarity of the two consecutive \"snapping vectors\" 2 given agent a's position at time t and t+1 and the location of the centers of two cells c 1 and c 2 . Since w p and w s are both assigned negative values during training, formulas S1 and S2 effectively softly enforce the corresponding geometric constraints."
            ],
            [
                "The raw data set contains missing data at a rate of approximately 1 in 250, while there are typically contiguous, several seconds long segments of readings missing. Since the players can move quite erratically, in this work we haven't attempted to explicitly fill in the missing data. Instead we simply assume that a player whose data is missing remains at his or her last seen location. Adding extra formulas to our theory that weigh readings according to their corresponding logged signal quality, which would be lowest for missing data points, has been left for future work.",
                "We apply theBeast software package to do weight learning and MAP inference in our domain. theBeast implements the cutting plane inference meta solving scheme and we use the integer linear program solver as the base solver (as opposed to MaxWalkSAT) since the resulting run times are still relatively short (under an hour even for training and testing the most complex unified model) and we gain exactness of the inference.",
                "We specify the Markov logic formulas by hand and optimize the weights of the soft formulas via supervised on-line learning. We set theBeast to use Margin Infused Relaxed Algorithm (MIRA) for weight updates while the loss function is computed from the number of false positives and false negatives over the hidden atoms.  Table 1 <table>  lists for each game the number of raw GPS readings and the number of captures (ground truth), and summarizes our results in terms of precision, recall, and F1 score. We see that the unified approach yields the best results in each case. Overall, it labels 9 out of 10 captures correctlythere is only one false negative. In fact, this tenth capture data (e.g., conversation, text message, etc.), and GPS information has only an indirect impact on the particular joint activity, since the participants are necessarily not in the same location. A recent effort to collect data on face-to-face conversation along with GPS data  (Wyatt et al. 2007 <ref id=#b13> ) might well be useful for inferring location-based multi-agent activities, but results published to date from that study have not made use of location information.",
                "Recent work on relational spacial reasoning includes an attempt to locate-using spacial abduction-caches of weapons in Iraq based on information about attacks within a geographic area  (Shakarian, Subrahmanian, and Spaino 2009) <ref id=#b11> .",
                "Finally, a number of researchers in machine vision have worked on the problem of recognizing events in videos of sporting events, such as impressive recent work on learning models of baseball plays  (Gupta et al. 2009) <ref id=#b4> . Most work in that area has focused on recognizing individual actions (e.g., catching and throwing), and the state of the art is just beginning to consider relational actions (e.g., the ball is thrown from player a to player b). The computational challenges of dealing with video data make it necessary to limit the time windows of a few seconds; by contrast, we demonstrated above that many events in the capture the flag data can only be disambiguated by considering arbitrarily long temporal sequences. In general, however, both our work and that in machine vision rely upon similar probabilistic models, and there is already some evidence that Markov logic-type relational models can be used for activity recognition from video  (Tran and Davis 2008; <ref id=#b12> Biswas, Thrun, and Fujimura 2007) <ref id=#b1> ."
            ],
            [
                "We presented a novel methodology-cast in Markov logicfor effectively combining data denoising with higher-level relational reasoning about a complex multi-agent domain. Experiments on real GPS data validate our approach while leaving an open door for future (incremental) additions to the ML theory.",
                "We compared our unified model with three alternatives (both probabilistic and nonprobabilistic) where either the denoising of the GPS data and the detection of the high-level activities are strictly separated, or the states of the players are not considered, or both. We showed that the unified approach with the time window spanning the entire game, although more computationally costly, is significantly more accurate.",
                "We are currently extending our framework in three directions. The first focuses on recognizing a richer set of game events of various types outlined in the introduction (such as freeing, chasing, hiding, cooperation, failed attempts at an activity, . . . ). The events are often tied together and thus recognizing one of them improves the performance on the others (e.g., imagine adding freeing recognition to our theory). The second extension is built on top of the denoising and recognition model and performs reinforcement learning to learn game tactics while exploiting the higher-level information inferred by the base model. Finally, we explore casting our activity recognition and denoising problem as inference in logical hidden Markov models (LHMMs), a generalization of standard HMMs that compactly represents probability distributions over sequences of logical atoms  (Kersting, De Raedt, and Raiko 2006) <ref id=#b5> ."
            ]
        ],
        "Tables": [],
        "Figures": [
            {
                "id": "fig_0",
                "Head": "Figure 2 :",
                "Label": "2",
                "Description": "Figure 2: Description of the hard and soft rules for capture the flag."
            },
            {
                "id": "fig_1",
                "Head": "Figure 4 :",
                "Label": "4",
                "Description": "Figure 4: Selected formulas in Markov logic. See corresponding constraints in Fig. 2 for an English description. (∃=1 denotes unique existential quantification.)"
            }
        ],
        "References": [
            {
                "id": "b0",
                "Title": "Cyberguide: a mobile contextaware tour guide",
                "Authors": [
                    "G Abowd",
                    "C Atkeson",
                    "J Hong",
                    "S Long",
                    "R Kooper",
                    "M Pinkerton"
                ],
                "Journal": "Wirel. Netw"
            },
            {
                "id": "b1",
                "Title": "Recognizing activities with multiple cues",
                "Authors": [
                    "R Biswas",
                    "S Thrun",
                    "K Fujimura"
                ]
            },
            {
                "id": "b3",
                "Title": "Reality mining: sensing complex social systems",
                "Authors": [
                    "N Eagle",
                    "A Pentland"
                ],
                "Journal": "Personal and Ubiquitous Computing"
            },
            {
                "id": "b4",
                "Title": "Understanding videos, constructing plots learning a visually grounded storyline model from annotated videos",
                "Authors": [
                    "A Gupta",
                    "P Srinivasan",
                    "J Shi",
                    "L Davis"
                ]
            },
            {
                "id": "b5",
                "Title": "Logical hidden Markov models",
                "Authors": [
                    "K Kersting",
                    "L De Raedt",
                    "T Raiko"
                ],
                "Journal": "J. Artif. Int. Res"
            },
            {
                "id": "b6",
                "Title": "Learning and inferring transportation routines",
                "Authors": [
                    "L Liao",
                    "D Fox",
                    "H Kautz"
                ]
            },
            {
                "id": "b8",
                "Title": "CRF-filters: discriminative particle filters for sequential state estimation",
                "Authors": [
                    "B Limketkai",
                    "D Fox",
                    "L Liao"
                ]
            },
            {
                "id": "b9",
                "Title": "Markov logic networks",
                "Authors": [
                    "M Richardson",
                    "P Domingos"
                ],
                "Journal": "Mach. Learn"
            },
            {
                "id": "b10",
                "Title": "Improving the accuracy and efficiency of MAP inference for Markov logic",
                "Authors": [
                    "S Riedel"
                ]
            },
            {
                "id": "b12",
                "Title": "Visual event modeling and recognition using Markov logic networks",
                "Authors": [
                    "S Tran",
                    "Davis",
                    "L"
                ]
            }
        ]
    },
    "robust policy computation in reward-uncertain mdps using nondominated policies": {
        "Title": "robust policy computation in reward-uncertain mdps using nondominated policies",
        "Authors": [
            "Kevin Regan",
            "Craig Boutilier"
        ],
        "Abstract": "The precise specification of reward functions for Markov decision processes (MDPs) is often extremely difficult, motivating research into both reward elicitation and the robust solution of MDPs with imprecisely specified reward (IRMDPs). We develop new techniques for the robust optimization of IR-MDPs, using the minimax regret decision criterion, that exploit the set of nondominated policies, i.e., policies that are optimal for some instantiation of the imprecise reward function. Drawing parallels to POMDP value functions, we devise a Witness-style algorithm for identifying nondominated policies. We also examine several new algorithms for computing minimax regret using the nondominated set, and examine both practically and theoretically the impact of approximating this set. Our results suggest that a small subset of the nondominated set can greatly speed up computation, yet yield very tight approximations to minimax regret.",
        "Keywords": [],
        "BookMarks": [
            "Introduction",
            "Background",
            "Markov Decision Processes",
            "Imprecise Reward MDPs",
            "Using Nondominated Policies",
            "Generating Nondominated Policies",
            "The πWitness Algorithm",
            "Empirical Results",
            "Approximating the Nondominated Set",
            "πWitness Anytime Performance",
            "Conclusion"
        ],
        "Papertext": [
            [
                "Markov decision processes (MDPs) have proven their value as a formal model for decision-theoretic planning. However, the specification of MDP parameters, whether transition probabilities or rewards, remains a key bottleneck. Recent work has focused on the robust solution of MDPs with imprecisely specified parameters. For instance, if a transition model is learned from observational data, there will generally be some uncertainty associated with its parameters, and a robust solution will offer some guarantees on policy quality even in the face of such uncertainty  (Iyengar 2005; <ref id=#b7> Nilim and Ghaoui 2005) <ref id=#b10> .",
                "Much research has focused on solving imprecise MDPs using the maximin criterion, emphasizing transition model uncertainty. But recent work deals with the robust solution of MDPs whose rewards are incompletely specified  (Delage and Mannor 2007; <ref id=#b6> McMahan, Gordon, and Blum 2003; <ref id=#b9> Regan and Boutilier 2009; <ref id=#b12> Xu and Mannor 2009) <ref id=#b14> . This is the problem we consider. Since reward functions must often be tailored to the preferences of specific users, some form of preference elicitation is required  (Regan and Boutilier 2009) <ref id=#b12> ; and to reduce user burden we may wish to solve an MDP before the entire reward function is known. Rather than maximin, minimax regret has been proposed as a suitable criterion for such MDPs with imprecise rewards (IR-MDPs)  (Regan and Boutilier 2009; <ref id=#b12> Xu and Mannor 2009) <ref id=#b14> , providing robust solutions and serving as an effective means of generating elicitation queries  (Regan and Boutilier 2009) <ref id=#b12> . However, computing the regret-optimal policy in IRMDPs is theoretically complex  (Xu and Mannor 2009) <ref id=#b14>  and practically difficult  (Regan and Boutilier 2009) <ref id=#b12> .",
                "In this work, we develop techniques for solving IRMDPs that exploit the existence of nondominated policies. Informally, if R is a set of possible reward functions, we say a policy π is nondominated if there is an r ∈ R for which π is optimal. The set of nondominated policies can be exploited to render minimax regret optimization far more efficient. We offer three main contributions. First, we describe a new algorithm for the minimax solution of an IRMDP that uses the set Γ of nondominated policies to great computational effect. Second, we develop an exact algorithm for computing Γ by drawing parallels with partially observable MDPs (POMDPs), specifically, the piecewise linear and convex nature of optimal value over R. Indeed, we suggest several approaches based on this connection to POMDPs. We also show how to exploit the low-dimensionality of reward space in factored reward models to render the complexity of our algorithm largely independent of state and action space size. Third, we provide a method for generating approximately nondominated sets. While Γ can be extremely large, in practice, very close approximations of small size can be found. We also show how such an approximate set impacts minimax regret computation, bounding the error theoretically, and investigating it empirically."
            ],
            [
                "We begin with relevant background on IRMDPs."
            ],
            [
                "We restrict our focus to infinite horizon, finite state and action MDPs S, A, {P sa }, γ, β, r , with states S, actions A, transition model P sa (•), non-negative reward function r(•, •), discount factor γ < 1, and initial state distribution β(•). We use vector notation for convenience with: r an |S| × |A| matrix with entries r(s, a), and P an |S||A| × |S| transition matrix. Their restrictions to action a are denoted r a and P a , respectively; and matrix E is identical to P with 1 subtracted from each self-transition probability P sa (s).",
                "Our aim is to find an optimal policy π that maximizes the sum of expected discounted rewards. Ignoring the initial state distribution β, value function V π : S → R for deterministic policy π satisfies:",
                "<formula_0>",
                "where restrictions to π are defined in the usual way. Given initial distribution β, π has expected value βV π . It also induces occupancy frequencies f π , where f π (s, a) is the total discounted probability of being in state s and taking action a. π can be recovered from f π via π(s, a) = f π (s, a)/ a ′ f π (s, a ′ ) (for deterministic π, f π sa = 0 for all a = π(s)). Let F be the set of valid occupancy frequencies w.r.t. a fixed MDP, i.e., those satisfying  (Puterman 1994) <ref id=#b11> :",
                "<formula_1>",
                "We write f π [s] to denote the occupancy frequencies induced by π when starting in state s (i.e., ignoring β). In what follows, we use frequencies and policies interchangeably since each uniquely determines the other. An optimal policy π * satisfies V π * ≥ V π (pointwise) for all π. For any positive β > 0, maximizing expected value βV π requires that π be optimal in this strong sense."
            ],
            [
                "In many settings, a reward function can be hard to obtain, requiring difficult human judgements of preference and tradeoffs (e.g., in domains such as cognitive assistive technologies  (Boger et al. 2006) <ref id=#b1> ), or expensive computation (see, e.g., value computation as a function of resource availability in autonomic computing  (Boutilier et al. 2003; <ref id=#b2> Regan and Boutilier 2009) <ref id=#b12> ). We define an imprecise reward MDP (IRMDP) S, A, {P sa }, γ, β, R by replacing reward r by a set of feasible reward functions R. The set R naturally arises from observations of user behaviour, partial elicitation of preferences, or information from domain experts, which typically place linear constraints on reward. We assume that R is a bounded, convex polytope defined by linear constraint set {r | Ar ≤ b} and use |R| to denote the number of constraints. In the preference elicitation model that motivates this work, these constraints arise from user responses to queries about the reward function  (Regan and Boutilier 2009) <ref id=#b12> . Given an IRMDP, we desire a policy that is robust to the imprecision in reward. Most robust optimization for imprecise MDPs adopts the maximin criterion, producing policies with maximum security level or worst-case value  (Bagnell, Ng, and Schneider 2003; <ref id=#b0> Iyengar 2005; <ref id=#b7> McMahan, Gordon, and Blum 2003; <ref id=#b9> Nilim and Ghaoui 2005) <ref id=#b10> . With imprecise reward, maximin value is:",
                "<formula_2>",
                "Maximin policies can be computed given an uncertain transition function by dynamic programming and efficient suboptimization to find worst case transition functions  (Bagnell, Ng, and Schneider 2003; <ref id=#b0> Iyengar 2005; <ref id=#b7> Nilim and Ghaoui 2005) <ref id=#b10> . However, these models cannot be extended to imprecise rewards. Maximin policies for IRMDPs can be determined using linear programming with constraint generation  (McMahan, Gordon, and Blum 2003) <ref id=#b9> . The maximin criterion leads to conservative policies by optimizing against the worst instantiation of r. Instead we adopt the minimax regret criterion  (Savage 1954 <ref id=#b13> ) applied recently to IRMDPs  (Regan and Boutilier 2009; <ref id=#b12> Xu and Mannor 2009) <ref id=#b14> . Let f , g be policies (i.e., their occupancy frequencies), r a reward function, and define:",
                "<formula_3>",
                "R(f , r) is the regret or loss of policy f relative to r, i.e., the difference in value between f and the optimal policy under r. MR(f , R) is the maximum regret of f w.r.t. feasible reward set R. Should we chose a policy f , MR(f , R) represents the worst-case loss over possible realizations of reward; i.e., the regret incurred in the presence of an adversary who chooses r to maximize loss. Equivalently, it can be viewed as the adversary choosing a policy with greatest pairwise max regret PMR(f , g, R), defined as the maximal difference in value between policies f and g under possible reward realizations.",
                "In the presence of such an adversary, we wish to minimize this max regret: MMR(R) is the minimax regret of feasible reward set R. This can be seen as a game between a decision maker (DM) choosing f to minimize loss relative to the optimal policy, and an adversary selecting r to maximize this loss given the DM's choice. Any f * that minimizes max regret is a minimax optimal policy, while the r that maximizes regret of f * is the adversarial reward, and the optimal policy g for r is the adversarial policy. Minimax regret measures performance by assessing the policy ex post and makes comparisons only w.r.t. specific reward realizations. Thus, policy π is penalized on reward r only if there exists a π ′ that has higher value w.r.t. r itself. Apart from producing robust policies using an intuitively appealing criterion, minimax regret is also an effective driver of reward elicitation. Unlike maximin, regret provides guidance as to maximal possible improvement in value should we obtain further information about the reward.  Regan and Boutilier (2009) <ref id=#b12>  develop an elicitation strategy in which a user is queried about relevant reward data based on the current minimax regret solution. It is empirically shown to reduce regret very quickly and give rise to provably optimal policies for the underlying MDP with very little reward information"
            ],
            [
                "While minimax regret is a natural robustness criterion and effectively guides elicitation, it is computationally complex. Computing the regret optimal policy for an IRMDP is NPhard  (Xu and Mannor 2009) <ref id=#b14> , and empirical studies using a   (Regan and Boutilier 2009) <ref id=#b12>  show poor scaling (we discuss this further below). Hence further development of practical algorithms is needed.",
                "We focus on the use of nondominated policies to ease the burden of minimax regret computation in IRMDPs. In what follows, assume a fixed IRMDP with feasible reward set R.",
                "We say policy f is nondominated w.",
                "<formula_4>",
                "In other words, a nondominated policy is optimal for some feasible reward. Let Γ(R) denote the set of nondominated policies w.r.t. R; since R is fixed, we write Γ for simplicity. Observation 1. For any IRMDP and policy f ,",
                "<formula_5>",
                "Thus the adversarial policy used to maximize regret of f must lie in Γ, since an adversary can only maximize regret by choosing some r ∈ R and an optimal policy f * r for r. If the set of nondominated policies is relatively small, and can be identified easily, then we can exploit this fact.",
                "Define V (r) = max f ∈F f • r to be the optimal value obtainable when r ∈ R is the true reward. Since policy value is linear in r, V is piecewise linear and convex (PWLC), much like the belief-state value function in POMDPs  (Cheng 1988; <ref id=#b5> Kaelbling, Littman, and Cassandra 1998) <ref id=#b8> , a fact we exploit below.  Fig. 1 <figure id=#fig_0>  illustrates this for a simplified 1-D reward, with nondominated policy set Γ = {f 1 , f 2 , f 3 , f 5 } (f 4 is dominated, i.e., optimal for no reward).  Xu and Mannor (2009) <ref id=#b14>  propose a method that exploits nondominated policies, computing minimax regret using the following linear program (LP), which \"enumerates\" Γ and has O(|R||Γ|) variables:",
                "<formula_6>",
                "subject to:",
                "<formula_7>",
                "Here R is defined by inequalities Ar ≤ b, andΓ is a matrix whose columns are elements of Γ. The variables c encode a randomized policy with support set Γ, which they show must be minimax optimal. For each potential adversarial policy f i ∈ Γ, equations Eq. (10) encode the dual of",
                "<formula_8>",
                "We refer to this approach as LP-ND1.  (Xu and Mannor (2009) <ref id=#b14>  provide no computational results for this formulation.)",
                "We can modify LP-ND1 to obtain the following LP (encoding the DM's policy choice using Eq. (2) rather than a convex combination of nondominated policies):",
                "<formula_9>",
                "subject to:",
                "<formula_10>",
                "This LP, LP-ND2, reduces the representation of the DM's policy from O(|Γ|) to O(|S||A|) variables. Empirically, we find that usually |Γ| ≫ |S||A| (see below). Rather than solving a single, large LP, we can use the constraint generation approach of  Regan and Boutilier (2009) <ref id=#b12> , solving a series of LPs:",
                "<formula_11>",
                "Here GEN is a subset of generated constraints corresponding to a subset of possible adversarial choices of policies and rewards. If GEN contains all vertices r of polytope R and corresponding optimal policies g * r , this LP solves minimax regret exactly. However, most constraints will not be active so iterative generation is used: given a solution f to the relaxed problem with only a subset of constraints, we wish to find the most violated constraint, i.e., the pair r, g * r that maximizes regret of f . If no violated constraints exist, then solution f is optimal. In  (Regan and Boutilier 2009) <ref id=#b12> , violated constraints are computed by solving a MIP (the major computational bottleneck). However, we can instead exploit Obs. 1 and solve, for each g ∈ Γ, a small LP to determine which reward gives g maximal advantage over the current relaxed solution f :",
                "<formula_12>",
                "The g with largest objective value determines the maximally violated constraint. Thus we replace the MIP for violated constraints in  (Regan and Boutilier 2009 <ref id=#b12> ) with a set of smaller LPs, and denote this approach by ICG-ND.",
                "We compare these three approaches to minimax regret computation using nondominated policies, as well as the   2009 <formula> (ICG-MIP), on very small, randomly generated IRMDPs. We fix |A| = 5 and vary the number of states from 3 to 7. A sparse transition model is generated (each (s, a)-pair has min(2, log 2 (|S|)) random, non-zero transition probabilities). An imprecise reward model is generated by: i) random uniform selection of each r(s, a) from a predefined range; ii) random generation of an uncertain interval whose size is normally distributed; and iii) then uniform random placement of the interval around the \"true\" r(s, a). A random state is chosen as the start state (point distribution). We generate 20 MDPs of each size.  Fig. 2 <figure id=#fig_1>  shows the computation time of the different algorithms as a function of the number of nondominated policies in each sampled MDP. LP-ND1  (Xu and Mannor 2009) <ref id=#b14>  performs poorly, taking more than 100s. to compute minimax regret for MDPs with more than 1000 nondominated policies. Our modified LP, LP-ND2, performs only slightly better. The most effective approach is our LP-based constraint generation procedure, ICG-ND, in which nondominated policies are exploited to determine maximally violated constraints. While |Γ| LPs must be solved at each iteration, these are extremely small. ICG-ND is also more effective than the original MIP model ICG-MIP  (Regan and Boutilier 2009) <ref id=#b12> , which does not make use of nondominated policies. This is seen in  Fig. 3 <figure id=#fig_2> , which shows average computation time (lines) and number of nondominated vectors (scatterplot) for each MDP size. We see that, while ICG-MIP performs reasonably well as the number of states grows (eventually outperforming LP-ND1 and LP-ND2), the ICG-ND approach still takes roughly an order of magnitude less time than ICG-MIP. As a result, we focus on ICG-ND below when we investigate larger MDPs."
            ],
            [
                "While the effectiveness of ICG-ND in exploiting the nondominated set Γ seems evident, the question remains: how to identify Γ? The PWLC nature of the function V (r) is analogous to the situation in POMDPs, where policy value is linear in belief state. For this reason, we adapt a wellknown POMDP algorithm Witness  (Kaelbling, Littman, and Cassandra 1998) <ref id=#b8>  to iteratively construct the set of nondominated policies. As discussed below, other POMDP methods can be adapted to this problem as well."
            ],
            [
                "Let f be the occupancy frequencies for policy π. Suppose, when starting at state s we take action a rather than π(s) as prescribed by π, but follow π thereafter. The occupancy frequencies induced by this local adjustment to π are given by:",
                "<formula_13>",
                "where e s:a is an S ×A vector with a 1 in position s, a and zeroes elsewhere. It follows from standard policy improvement theorems  (Puterman 1994 <ref id=#b11> ) that if f is not optimal for reward r, then there must be a local adjustment s, a such that f s:a • r > f • r. 1 This gives rise to a key fact: Theorem 1. Let Γ ′ Γ be a (strictly) partial set of nondominated policies. Then there is an f ∈ Γ ′ , an (s, a), and an r ∈ R such that f s:",
                "<formula_14>",
                "This theorem is analogous to the witness theorem for POMDPs  (Kaelbling, Littman, and Cassandra 1998) <ref id=#b8>  and suggests a Witness-style algorithm for computing Γ. Our πWitness algorithm begins with a partial set Γ consisting of a single nondominated policy optimal for an arbitrary r ∈ R. At each iteration, for all f ∈ Γ, it checks whether there is a local adjustment (s, a) and a witness reward r s.t. f s,a • r > f ′ • r for all f ′ ∈ Γ (i.e., whether f s,a offers an improvement at r). If there is an improvement, we add the optimal policy f * r for that r to Γ. If no improvement exists for any f , then by Thm. 1, Γ is complete. The algorithm is sketched in Alg. 1. The agenda holds the policies for which we have not yet explored all local adjustments. find-WitnessReward tries to find an r for which f s:a has higher value than any f ′ ∈ Γ by solving the LP:",
                "<formula_15>",
                "There may be multiple witnesses for a single adjustment, thus findWitnessReward is called until no more witnesses are found. findBest finds the optimal policy given r. The order in which the agenda is processed can have an impact on anytime behavior, a fact we explore in the next section.",
                "We can see that the runtime of the πWitness algorithm is polynomial in inputs |S|, |A|, |R| (interpreted as the number of constraints defining the polytope), and output |Γ|, assuming bounded precision in the input representation. When a witness r w is found, it testifies to a nondominated f which is added to Γ and the agenda. Thus, the number of policies added to the agenda is exactly |Γ|. The subroutine findWit-nessReward is called at most |S||A| times for each f ∈ |Γ| to test local adjustments for witness points (total of |Γ||S||A| calls). findWitnessReward requires solution of an LP with |S||A| + 1 variables and no more than |Γ| + |R| constraints, thus the LP encoding has polynomial size (hence solvable in polytime). findBest is called only when a witness is found, i.e., exactly |Γ| times. It requires solving an MDP, which is polynomial in the size of its specification  (Puterman 1994) <ref id=#b11> . Thus πWitness is polynomial. This also means that for any class of MDPs with a polynomial number of nondominated policies, minimax regret computation is itself polynomial."
            ],
            [
                "The number of nondominated policies is influenced largely by the dimensionality of the reward function and less so by conventional measures of MDP size, |S| and |A|. Intuitively, this is so because a high dimensional r allows variability across the state-action space, admitting different optimal policies depending on the realization of reward. When reward is completely unrestricted (i.e., the r(s, a) are \"in-     (Boutilier, Dean, and Hanks 1999) <ref id=#b3>  have large state and action spaces defined over sets of state variables; and typically reward depends only on a small fraction of these, often in an additive way. 2 In our empirical investigation of πWitness, we exploit this fact, exploring how its performance varies with reward dimension. We first test πWitness on MDPs of varying sizes, but with reward of small fixed dimension. States are defined by 2-6 binary variables (yielding |S| = 4 . . . 64), and a factored additive reward function on two attributes: r(s) = r 1 (x 1 ) + r(x 2 ). The transition model and feasible reward set R is generated randomly as above, with random reward intervals generated for the parameters of each factor rather than for each (s, a)-pair. 3  Table 1 <table id=#tab_0>  shows the number of nondominated policies discovered (with mean (µ) and standard deviation (σ) over 20 runs), and demonstrates that Γ does not grow appreciably with |S|, as expected with 2-D reward. The running time of πWitness is similar, growing slightly greater than linearly in |S|. We also examine MDPs of fixed size (6 attributes, |S| = 64), varying the dimensionality of the reward function from 2-8 by varying the number of additive reward attributes from 1-4. Results (20 instances of each dimension) are shown  Table 2 <table id=#tab_3> . While Γ is very small for dimensions 2 and 4, it grows dramatically with reward dimensionality, as does the running time of πWitness. This demonstrates the strong impact of the size of the output set Γ on the running time of πWitness."
            ],
            [
                "The complexity of both πWitness and our procedure ICG-ND are influenced heavily by the size of Γ; and while the 2 Our approach extends directly to more expressive generalized additive (GAI) reward models, to which the minimax regret formulations can be applied in a straightforward manner  (Braziunas and Boutilier 2005) <ref id=#b4> .  3 <ref>  We can exploit factored reward computationally in πWitness and minimax regret computation (we defer details to a longer version of the paper). We continue to use an unstructured transition model to emphasize the dependence on reward dimensionality. number of nondominated policies scales reasonably well with MDP size, it grows quickly with reward dimensionality. This motivates investigation of methods that use only a subset of the nondominated policies that reasonably approximates Γ, or specifically, the PWLC function V Γ (•) induced by Γ. We first explore theoretical guarantees on minimax regret when ICG-ND (or any other method that exploits Γ) is run using a (hopefully, well-chosen) subset of Γ.",
                "LetΓ ⊆ Γ. The VΓ(r) induced byΓ is clearly a lower bound on V Γ (r). Define the error in VΓ to be maximum difference between the approximate and exact value functions:",
                "<formula_16>",
                "This error is illustrated in  Fig. 1 <figure id=#fig_0> , where the dashed line (marked with a *) shows the error introduced by using the subset of dominated policies {f 1 , f 3 , f 5 } (removing f 2 ). The error in VΓ can be used to derive a bound on error in computed minimax regret. Let MMR(Γ) denote true minimax regret when adversarial policy choice is unrestricted and MMR(Γ) denote the approximation when adversarial choice is restricted toΓ. 4 MMR(Γ) offers a lower bound on true MMR; and the difference ǫ MMR (Γ) can be bounded, as can the difference between the true max regret of the approximately optimal policy so constructed: Theorem 2. Thus, should we generate a set of nondominated policies Γ that ǫ-approximates Γ, any algorithm (including ICG-ND) that uses nondominated sets will produce a policy that is within a factor of 2ǫ(Γ) of minimizing max regret.",
                "This suggests that careful enumeration of nondominated policies can provide tremendous computational leverage. By adding policies to Γ that \"contribute\" the most to error reduction, we may be able to construct a partial setΓ of small size, but that closely approximates Γ. Indeed, as we discuss below, the agenda in πWitness can be managed to help accomplish just this. We note that a variety of algorithms for POMDPs attempt to build up partial sets of f -vectors to approximate a value function (e.g.,  (Cheng 1988 <ref id=#b5> )) and we are currently investigating the adaptation of such methods to nondominated policy enumeration as well."
            ],
            [
                "We can construct a small approximating setΓ using πWitness by exploiting its anytime properties and careful management of the agenda. Intuitively, we want to add policies toΓ that hold the greatest \"promise\" for reducing error ǫ(Γ). We measure this as follows. LetΓ n be the nth nondominated set produced by πWitness, constructed by adding optimal policy f * n for the nth witness point r n . When f * n is added to the agenda, it offers improvement to the current approximation:",
                "<formula_17>",
                ". We process the agenda in priority queue fashion, using ∆(f ) as the priority measure for any policy f remaining on the agenda. Thus, we examine adjustments to policies that provided greater increase in value when added toΓ before considering adjustments to policies that provided lesser value.",
                "Informal experiments show that using a priority queue reduced the error ǫ(Γ) much more quickly than using standard stack or queue approaches. Hence we investigate the anytime performance of πWitness with a priority queue on random MDPs with 128 and 256 states (30 runs of each). The reward dimension is fixed to 6 (3 additive factors) and the number of actions to 5. We first compute the exact minimax regret for the MDP, then run πWitness. When the nth nondominated policy is found, we compute an approximation of minimax regret using the algorithm ICG-ND with approximate nondominated set Γ n . We measure the relative error in minimax regret: ǫ MMR (Γ)/MMR.  Fig. 4 <figure id=#fig_3>  shows the relative error as nondominated policies are added using the priority queue implementation. The runtime of ICG-ND algorithm for computing minimax regret is also shown. With 256 (resp., 128) states, relative error drops below 0.02 after just 500 (resp., 300) policies have been added to Γ. Minimax regret computation using ICG-ND grows linearly with the number of nondominated policies added to |Γ|, but stays well below 1 second: at the 0.02 error point, solution of 256-state (resp., 128-state) MDPs averages under 0.4 seconds (resp., 0.2 seconds). Given our goal of using minimax regret to drive preference elicitation, these results suggest that using a small set of nondominated policies and the ICG-ND algorithm will admit real-time interaction with users. Critically, while πWitness is much more computationally intensive, it can be run offline, once, to precompute nondominated policies (or a small approximate set) before engaging in online elicitation with users.  Fig. 5 <figure id=#fig_5>  shows the cumulative runtime of πWitness as it adds policies to Γ. With 256 states, the first 500 policies (error level 0.02) are generated in under 2 hours on average (128 states, under 1 hour). In both cases, runtime πWitness is only slightly super-linear in the number of policies."
            ],
            [
                "We presented a new class of techniques for solving IR-MDPs that exploit nondominated policies. We described new algorithms for computing robust policies using minimax regret that leverage the set Γ of nondominated policies, and developed the πWitness algorithm, an exact method for computing Γ in polynomial time. We showed how lowdimensional factored reward allows πWitness to scale to large state spaces, and examined the impact of approximate nondominated sets, showing that small sets can yield good, quickly computable approximations to minimax regret.",
                "Some important directions remain. We are investigating methods to compute tight bounds on minimax regret error while generating nondominated policies, drawing on algorithms from the POMDP literature (e.g.,  Cheng's (1988) <ref id=#b5>  linear support algorithm). An algorithm for generating nondominated policies that yields a bound at each step, allows termination when a suitable degree of approximation is reached. We are exploring the integration with preference elicitation as well. A user provides reward information (e.g., by responding to queries), to reduce reward imprecision and improve policy quality. Since this constrains the feasible reward space, fewer nondominated policies result; thus as elicitation proceeds, the set of nondominated policies can be pruned allowing more effective computation. Finally, we are interested in the formal relationship between the number of nondominated policies and reward dimensionality."
            ]
        ],
        "Tables": [
            {
                "id": "tab_0",
                "Head": "Algorithm 1 :",
                "Label": "1",
                "Description": "The πWitness algorithm r ← some arbitrary r ∈ R f ← findBest(r) Γ ← { f } agenda ← { f } while agenda is not empty do f ← next item in agenda foreach s, a do r w ← findWitnessReward(f s:a , Γ) while witness found do f best ← findBest(r w ) add f best to Γ add f best to agenda r"
            },
            {
                "id": "tab_2",
                "Head": "Table 1 :",
                "Label": "1",
                "Description": "Varying Number of States"
            },
            {
                "id": "tab_3",
                "Head": "Table 2",
                "Label": "2",
                "Description": ""
            }
        ],
        "Figures": [
            {
                "id": "fig_0",
                "Head": "Figure 1 :",
                "Label": "1",
                "Description": "Figure 1: Illustration of value as a linear function of reward"
            },
            {
                "id": "fig_1",
                "Head": "Figure 2 :",
                "Label": "2",
                "Description": "Figure 2: Scaling of MMR computation w.r.t. nondominated policies"
            },
            {
                "id": "fig_2",
                "Head": "Figure 3 :",
                "Label": "3",
                "Description": "Figure 3: Scaling of MMR computation (lineplot on left yaxis) and nondominated policies (scatterplot on right y-axis) w.r.t. number of states"
            },
            {
                "id": "fig_3",
                "Head": "Figure 4 :",
                "Label": "4",
                "Description": "Figure 4: Relative minimax regret error and cumulative πWitness runtime vs. number of nondominated policies."
            },
            {
                "id": "fig_4",
                "Head": "ǫ",
                "Label": "",
                "Description": "MMR (Γ) = MMR(Γ) − MMR(Γ) ≤ ǫ(Γ); and MR(Γ, R) − MMR(Γ) ≤ 2ǫ(Γ)."
            },
            {
                "id": "fig_5",
                "Head": "Figure 5 :",
                "Label": "5",
                "Description": "Figure 5: πWitness computation time (hrs.) vs. number of nondominated policies."
            }
        ],
        "References": [
            {
                "id": "b1",
                "Title": "A planning system based on Markov decision processes to guide people with dementia through activities of daily living",
                "Authors": [
                    "J Boger",
                    "P Poupart",
                    "J Hoey",
                    "C Boutilier",
                    "G Fernie",
                    "A Mihailidis"
                ],
                "Journal": "IEEE Transactions on Information Technology in Biomedicine"
            },
            {
                "id": "b2",
                "Title": "Cooperative negotiation in autonomic systems using incremental utility elicitation",
                "Authors": [
                    "C Boutilier",
                    "R Das",
                    "J Kephart",
                    "G Tesauro",
                    "W Walsh"
                ]
            },
            {
                "id": "b3",
                "Title": "Decision theoretic planning: Structural assumptions and computational leverage",
                "Authors": [
                    "C Boutilier",
                    "T Dean",
                    "S Hanks"
                ],
                "Journal": "Journal of Artificial Intelligence Research"
            },
            {
                "id": "b4",
                "Title": "Local utility elicitation in GAI models",
                "Authors": [
                    "D Braziunas",
                    "C Boutilier"
                ]
            },
            {
                "id": "b6",
                "Title": "Percentile optimization in uncertain Markov decision processes with application to efficient exploration",
                "Authors": [
                    "E Delage",
                    "S Mannor"
                ]
            },
            {
                "id": "b7",
                "Title": "Robust dynamic programming",
                "Authors": [
                    "G Iyengar"
                ],
                "Journal": "Operations Research"
            },
            {
                "id": "b8",
                "Title": "Planning and acting in partially observable stochastic domains",
                "Authors": [
                    "L Kaelbling",
                    "M Littman",
                    "A Cassandra"
                ],
                "Journal": "Artificial Intelligence"
            },
            {
                "id": "b9",
                "Title": "Planning in the presence of cost functions controlled by an adversary",
                "Authors": [
                    "B Mcmahan",
                    "G Gordon",
                    "A Blum"
                ]
            },
            {
                "id": "b10",
                "Title": "Robust control of markov decision processes with uncertain transition matrices",
                "Authors": [
                    "A Nilim",
                    "L Ghaoui"
                ],
                "Journal": "Operations Research"
            },
            {
                "id": "b12",
                "Title": "Regret-based reward elicitation for Markov decision processes",
                "Authors": [
                    "K Regan",
                    "C Boutilier"
                ]
            },
            {
                "id": "b14",
                "Title": "Parametric regret in uncertain Markov decision processes",
                "Authors": [
                    "H Xu",
                    "S Mannor"
                ]
            }
        ]
    },
    "to max or not to max: online learning for speeding up optimal planning *": {
        "Title": "to max or not to max: online learning for speeding up optimal planning *",
        "Authors": [
            "Carmel Domshlak",
            "Erez Karpas",
            "Shaul Markovitch"
        ],
        "Abstract": "It is well known that there cannot be a single \"best\" heuristic for optimal planning in general. One way of overcoming this is by combining admissible heuristics (e.g. by using their maximum), which requires computing numerous heuristic estimates at each state. However, there is a tradeoff between the time spent on computing these heuristic estimates for each state, and the time saved by reducing the number of expanded states. We present a novel method that reduces the cost of combining admissible heuristics for optimal search, while maintaining its benefits. Based on an idealized search space model, we formulate a decision rule for choosing the best heuristic to compute at each state. We then present an active online learning approach for that decision rule, and employ the learned model to decide which heuristic to compute at each state. We evaluate this technique empirically, and show that it substantially outperforms each of the individual heuristics that were used, as well as their regular maximum.",
        "Keywords": [],
        "BookMarks": [
            "Introduction",
            "Notation",
            "A Model for Heuristic Selection",
            "Dealing with Model Assumptions",
            "Online Learning of the Selection Rule",
            "Experimental Evaluation",
            "Discussion"
        ],
        "Papertext": [
            [
                "One of the most prominent approaches to cost-optimal planning is using the A * search algorithm with an admissible heuristic. Many admissible heuristics have been proposed, varying from cheap to compute yet typically not very informative to expensive to compute but often very informative. Since the accuracy of heuristic functions varies for different problems, and even for different states of the same problem, we can produce a more robust optimal planner by combining several admissible heuristics. The simplest way of doing this is by using their point-wise maximum at each state. Presumably, each heuristic is more accurate, that is, provides a higher estimate, in different regions of the search space, and thus their maximum is at least as accurate as each of the individual heuristics. In some cases it is also possible to use additive  (Felner, Korf, and Hanan 2004; <ref id=#b6> Haslum, Bonet, and Geffner 2005; <ref id=#b10> Katz and Domshlak 2008) <ref id=#b17>  or mixed additive/maximizing  (Coles et al. 2008; <ref id=#b4> Haslum et al. 2007) <ref id=#b9>  combinations of admissible heuristics.",
                "An important issue with both max-based and sum-based approaches is that the benefit of adopting them over sticking to just a single heuristic is assured only if the planner is not constrained by time. Otherwise, the time spent on computing numerous heuristic estimates at each state may outweigh the time saved by reducing the number of expanded states. This is precisely the contribution of this paper: We propose a novel method for combining admissible heuristics that aims at providing the accuracy of their max-based combination while still computing just a single heuristic for each search state. This method, called selective max, is presented and evaluated in what follows in the context of heuristic-search planning, yet is applicable to any search problem.",
                "At a high level, selective max can be seen as a hyperheuristic  (Burke et al. 2003) <ref id=#b2>  -a heuristic for choosing between other heuristics. Specifically, selective max is based on a seemingly useless observation that, if we had an oracle indicating the most accurate heuristic for each state, then computing only the indicated heuristic would provide us with the heuristic estimate of the max-based combination. In practice, of course, such an oracle is not available. However, in the time-limited settings of our interest, this is not our only concern: It is possible that the extra time spent on computing the more accurate heuristic (indicated by the oracle) may not be worth the time saved by the reduction in expanded states.",
                "Addressing the latter concern, we first analyze an idealized model of a search space and deduce a decision rule for choosing a heuristic to compute at each state when the objective is to minimize the overall search time. Taking that decision rule as our target concept, we then describe an online active learning procedure for that concept that constitutes the essence of selective max. Our experimental evaluation with two state-of-the-art admissible heuristics for domainindependent planning, h LA  (Karpas and Domshlak 2009) <ref id=#b16>  and h LM-CUT  (Helmert and Domshlak 2009) <ref id=#b11> , shows that, under various time limits, using selective max consistently results in solving more problems than using each of these two heuristics individually, as well as using their max-based combination. Furthermore, the results show that using selective max results in solving problems faster on average."
            ],
            [
                "We consider planning in the SAS + formalism  (Bäckström and Nebel 1995) <ref id=#b0> ; a SAS + description of a planning task can be automatically generated from its PDDL description  (Helmert 2009) <ref id=#b15> . A SAS + task is given by a 4-tuple  Figure 1 <figure> : An illustration of the idealized search space model and the f -contours of two admissible heuristics.",
                "<formula_0>",
                "<formula_1>",
                "Each complete assignment s to V is called a state; s 0 is an initial state, and the goal G is a partial assignment to V . A is a finite set of actions, where each action a is a pair pre(a), eff(a) of partial assignments to V called preconditions and effects, respectively.",
                "An action a is applicable in a state s iff pre(a) ⊆ s. Applying a changes the value of each state variable",
                "<formula_2>",
                "is specified. The resulting state is denoted by s a ; by s a 1 , . . . , a k we denote the state obtained from sequential application of the (respectively applicable) actions a 1 , . . . , a k starting at state s. Such an action sequence is a plan if G ⊆ s 0 a 1 , . . . , a k ."
            ],
            [
                "Given a set of admissible heuristics and the objective of minimizing the overall search time, we are interested in a decision rule for choosing the right heuristic to compute at each search state. In what follows, we derive such a decision rule for a pair of admissible heuristics with respect to an idealized search space model corresponding to a tree-structured search space with a single goal state, constant branching factor b, and uniform cost actions  (Pearl 1984) <ref id=#b20> . Two additional assumptions we make are that the heuristics are consistent, and that the time t i required for computing heuristic h i is independent of the state being evaluated; w.l.o.g. we assume t 2 ≥ t 1 . Obviously, most of the above assumptions do not hold in typical search problems, and later we carefully examine their individual influences on our framework.",
                "Adopting the standard notation, let g(s) be the cost of the cheapest path from s 0 to s. Defining max h (s) = max(h 1 (s), h 2 (s)), we then use the notation f 1 (s) = g(s)+ h 1 (s), f 2 (s) = g(s) + h 2 (s), and max f (s) = g(s) + max h (s). The A * algorithm with a heuristic h expands states in increasing order of f = g + h. Assuming the goal state is at depth c * , let us consider the states satisfying f 1 (s) = c * (the dotted line in  Fig. 1 <figure> ) and those satisfying f 2 (s) = c * (the solid line in  Fig. 1 <figure> ). The states above the f 1 = c * and f 2 = c * contours are those that are surely expanded by A * with h 1 and h 2 , respectively. The states above both these contours (the grid-marked region in  Fig. 1 <figure> ), that is, the states SE = {s | max f (s) < c * }, are those that are surely expanded by A * using max h (see Theorem 4, p. 79,  Pearl 1984) <ref id=#b20> .",
                "Under the objective of minimizing the search time, observe that the optimal decision for any state s ∈ SE is not to compute any heuristic at all, since all these states are surely expanded anyway. The optimal decision for all other states is a bit more complicated. f 2 = c * contour that separates between the grid-marked and lines-marked areas. Since f 1 (s) and f 2 (s) account for the same g(s), we have h 2 (s) > h 1 (s), that is, h 2 is more accurate in state s than h 1 . If we were interested solely in reducing state expansions, then h 2 would obviously be the right heuristic to compute at s. However, for our objective of reducing the actual search time, h 2 may actually be the wrong choice because it might be much more expensive to compute than h 1 .",
                "Let us consider the effects of each of our two alternatives. If we compute h 2 (s), then s is no longer surely expanded since f 2 (s) = c * , and thus whether A * expands s or not depends on tie-breaking. In contrast, if we compute h 1 (s), then s is surely expanded because f 1 (s) < c * . Note that not computing h 2 for s and then computing h 2 for one of the descendants s of s is surely a sub-optimal strategy as we do pay the cost of computing h 2 , yet the pruning of A * is limited only to the search sub-tree rooted in s . Therefore, our choices are really either computing h 2 for s, or computing h 1 for all the states in the sub-tree rooted in s that lie on the f 1 = c * contour. Suppose we need to expand l complete levels of the state space from s to reach the f 1 = c * contour. This means we need to generate order of b l states, and then invest b l t 1 time in calculating h 1 for all these states that lie on the f 1 = c * contour. In contrast, suppose we choose to compute h 2 (s). Assuming favorable tie-breaking, the time required to \"explore\" the sub-tree rooted in s will be t 2 .",
                "Putting things together, the optimal decision in state s is thus to compute h 2 iff t 2 < b l t 1 , or if we rewrite this, if",
                "<formula_3>",
                "As a special case, if both heuristics take the same time to compute, this decision rule boils down to l > 0, that is, the optimal choice is simply the more accurate (for state s) heuristic.",
                "The next step is to somehow estimate the \"depth to go\" l. For that, we make another assumption about the rate at which f 1 grows in the sub-tree rooted at s. Although there are many possibilities here, we will look at two estimates that appear to be quite reasonable. The first estimate assumes that the h 1 value remains constant in the subtree rooted at s, that is, the additive error of h 1 increases by 1 for each level below s. In this case, f 1 increases by 1 for each expanded level of the sub-tree (because h 1 remains the same, and g increases by 1), and it will take expanding ∆ h (s) = h 2 (s) − h 1 (s) levels to reach the f 1 = c * contour. The second estimate we examine assumes that the absolute error of h 1 remains constant, that is, h 1 increases by 1 for each level expanded, and so f 1 increases by 2. In this case, we will need to expand ∆ h (s)/2 levels. This can be generalized to the case where the estimate h 1 increases by any constant additive factor c, which results in ∆ h (s)/(c + 1) levels being expanded. In either case, the dependence of l on ∆ h (s) is linear, and thus our decision rule can be refor-",
                "<formula_4>",
                "where α is a hyper-parameter for our algorithm. Note that, given b, t 1 , and t 2 , the quantity α log b (t 2 /t 1 ) becomes fixed and in what follows we denote simply by threshold τ ."
            ],
            [
                "The idealized model above makes several assumptions, some of which appear to be very problematic to meet in practice. Here we examine these assumptions more closely, and when needed, suggest pragmatic compromises. First, the model assumes that the search space forms a tree with a single goal state and uniform cost actions, and that the heuristics in question are consistent. Although the first assumption does not hold in most planning problems, and the second assumption is not satisfied by some state-of-the-art heuristics, they do not prevent us from using the decision rule suggested by the model. Furthermore, there is some empirical evidence to support our conclusion about exponential growth of the search effort as a function of heuristic error, even when the assumptions made by the model do not hold. In particular, the experiments of  Helmert and Röger (2008) <ref id=#b12>  with heuristics with small constant additive errors clearly show that the number of expanded nodes typically grows exponentially as the (still very small and additive) error increases.",
                "The model also assumes that both the branching factor and the heuristic computation times are constant across the search states. In our application of the decision rule to planning in practice, we deal with this assumption by adopting the average branching factor and heuristic computation times, estimated from a random sample of search states. Finally, the model assumes perfect knowledge about the surely expanded search states. In practice, this information is obviously not available. We approach this issue conservatively by treating all the examined search states as if they were on the decision border, and thus apply the decision rule at all the search states. Note that this does not hurt the correctness of our algorithm, but only costs us some heuristic computation time on the surely expanded states. Identifying the surely expanded region during search is the subject of ongoing work, and can hopefully be used to improve search efficiency even further."
            ],
            [
                "Our decision rule for choosing a heuristic to compute at a given search state s suggests to compute the more expensive heuristic h 2 when h 2 (s) − h 1 (s) > τ . However, computing h 2 (s) − h 1 (s) requires computing in s both heuristics, defeating the whole purpose of reducing search time by selectively evaluating only one heuristic at each state. To overcome this pitfall, we take our decision rule as a target concept, and suggest an active online learning procedure for that concept. Intuitively, our concept is the set of states where the more expensive heuristic h 2 is \"significantly\" more accurate than the cheaper heuristic h 1 . According to our model, this corresponds to the states where the reduction in expanded states by computing h 2 outweighs the extra time needed to compute it. In what follows, we present our learning-based methodology in detail, describing the way we select and label training examples, the features we use to represent the examples, the way we construct our classifier, and the way we employ it within A * search.",
                "To build a classifier, we first need to collect training examples, which should be representative of the entire search space. One option for collecting the training examples is to use the first k states of the search where k is the desired number of training examples. However, this method has a bias towards states that are closer to the initial state, and therefore is not likely to well represent the search space. Hence, we instead collect training examples by sending \"probes\" from the initial state. Each such \"probe\" simulates a stochastic hill-climbing search with a depth limit cutoff. All the states generated by such a probe are used as training examples, and we stop probing when k training examples have been collected. In our evaluation, the probing depth limit was set to twice the heuristic estimate of the initial state, that is 2 max h (s 0 ), and the next state s for an ongoing probe was chosen with a probability proportional to 1/ max h (s). This \"inverse heuristic\" selection biases the sample towards states with lower heuristic estimates, that is, to states that are more likely to be expanded during the search. It is worth noting here that more sophisticated procedures for search space sampling have been proposed in the literature (e.g., see  Haslum et al. 2007) <ref id=#b9> , but as we show later, our much simpler sampling method is already quite effective for our purpose.",
                "After the training examples T are collected, they are first used to estimate b, t 1 and t 2 by averaging the respective quantities over T . Once b, t 1 and t 2 are estimated, we can compute the threshold τ = α log b (t 2 /t 1 ) for our decision rule. We generate a label for each training example by calculating ∆ h (s) = h 2 (s) − h 1 (s), and comparing it to the decision threshold. If ∆ h (s) > τ , we label s with h 2 , otherwise with h 1 . If t 1 > t 2 we simply switch between the heuristics-our decision is always whether to compute the more expensive heuristic or not; the default is to compute the cheaper heuristic, unless the classifier says otherwise.",
                "Besides deciding on a training set of examples, we need to choose a set of features to represent each of these examples. The aim of these features is to characterize search states with respect to our decision rule. While numerous features for characterizing states of planning problems have been proposed in previous literature (see, e.g.,  Yoon, Fern, and Givan (2008) <ref id=#b21> ; de la Rosa, Jiménez, and Borrajo (2008)), they were all designed for inter-problem learning, and most of them are not suitable for intra-problem learning like ours. In our work we decided to use only elementary features corresponding simply to the actual state variables of the planning problem.",
                "Once we have our training set and features to represent the examples, we can build a binary classifier for our concept. This classifier can then play the role of our hypothetical or-evaluate(s) h, conf idence := CLASSIFY(s, model) if (conf idence > ρ) then return h(s) else acle indicating which heuristic to compute where. However, as our classifier is not likely to be a perfect such oracle, we further consult the confidence the classifier associates with its classification. The resulting state evaluation procedure of selective max is depicted in  Figure 2 <figure id=#fig_0> . If state s is to be evaluated by A * , we use our classifier to decide which heuristic to compute. If the classification confidence exceeds a parameter threshold ρ, then only the indicated heuristic is computed for s. Otherwise, we conclude that there is not enough information to make a selective decision for s, and compute the regular maximum over h 1 (s) and h 2 (s). However, we use this opportunity to improve the quality of our prediction for states similar to s, and update our classifier. This is done by generating a label based on h 2 (s) − h 1 (s) and learning from this new example. 1 This can be viewed as the active part of our learning procedure. The last decision to be made is the choice of classifier. Although many classifiers can be used here, there are several requirements that need to be met due to our particular setup. First, both training and classification must be very fast, as both are performed during time-constrained problem solving. Second, the classifier must be incremental to allow online update of the learned model. Finally, the classifier should provide us with a meaningful confidence for its predictions. While several classifiers meet these requirements, we found the classical Naive Bayes classifier to provide a good balance between speed and accuracy  (Mitchell 1997) <ref id=#b19> . One note on the Naive Bayes classifier is that it assumes a very strong conditional independence between the features. Although this is not a fully realistic assumption for planning problems, using a SAS + formulation of the problem instead of the classical STRIPS helps a lot: instead of many binary variables which are highly dependent upon each other, we have a much smaller set of variables which are less dependent upon each other.  2 <ref>  As a final note, extending selective max to use more than two heuristics is rather straightforward-simply compare the heuristics in a pair-wise manner, and choose the best heuristic by a vote, which can either be a regular vote (i.e., 1 for the winner, 0 for the loser), or weighted according to the classifier's confidence. Although this requires a quadratic number of classifiers, training and classification  time (at least with Naive Bayes) appear to be much lower than the overall time spent on heuristic computations, and thus the overhead induced by learning and classification is likely to remain relatively low.",
                "<formula_5>"
            ],
            [
                "To empirically evaluate the performance of selective max we have implemented it on top of the A * implementation of the Fast Downward planner  (Helmert 2006) <ref id=#b14> , and conducted an empirical study on a wide range of planning domains from the International Planning Competitions 1998-2006; the domains are listed in  Table 2 <table> . The search for each problem instance was limited to 30 minutes 3 and to 1.5 GB of memory. The search times do not include the PDDL to SAS + translation as it is common to all planners, and is tangential to the issues considered in our study. The search times do include learning and classification time for selective max. In the experiments we set the size of the initial training set to 100, the confidence threshold ρ to 0.6, and α to 1.",
                "Our evaluation of selective max was based on two stateof-the-art admissible heuristics h LA  (Karpas and Domshlak 2009) <ref id=#b16>  and h LM-CUT  (Helmert and Domshlak 2009) <ref id=#b11> . In contrast to abstraction heuristics that are typically based on expensive offline preprocessing, and then very fast online per-state computation  (Helmert, Haslum, and Hoffmann 2007; <ref id=#b13> Katz and Domshlak 2009) <ref id=#b18> , both h LA and h LM-CUT perform most of their computation online. Neither of our two base heuristics is better than the other across all planning domains, although A * with h LM-CUT solves more problems overall. On the other hand, the empirical time complexity of computing h LA is typically much lower than that of computing h LM-CUT .",
                "We compare our selective max approach (sel h ) to each of the two base heuristics individually, as well as to their standard, max-based combination (max h ). In addition, to avoid erroneous conclusions about the impact of our specific decision rule on the effectiveness of selective max, we also compare sel h to a trivial version of selective max that chooses between the two base heuristics uniformly at random (rnd h ).",
                "As the primary purpose of selective max is speeding up optimal planning, we first examine the average runtime complexity of A * with the above five alternatives for search node evaluation. While the results of this evaluation in detail are given in  Table 2 <table> , the table in  Figure 3a <figure>  provides the bottomline summary of these results. The first two rows in that table provide the average search times for all five methods. The times in the first row are plain averages over all (403) problem instances that were solved by all five methods. Based on the same problem instances, the times in the second row are averages over average search times within each planning domain. Supporting our original motivation, these results clearly show that sel h is on average substantially faster than any of the other four alternatives, including not only the max-based combination of the base heuristics, but also both our individual base heuristics. Focusing the comparison to only max h (averaging on 454 problem instances that were solved with both max h and sel h ), the average search time with sel h was 65.2 seconds, in contrast to 90.75 seconds with max h . Although it is hard to measure the exact overhead of the learning component of selective max, the average overhead for training and classification over all problems that took more than 1 second to solve was about 2%",
                "The third row in the table provides the total number of problems solved by each of the methods. Here as well, selective max is the winner, yet the picture gets even sharper when the results are considered in more detail. For all five methods,  Figure 3b <figure>  plots the total number of solved instances as a function of timeout. The plot is self-explanatory, and it clearly indicates that selective max has a consistently better anytime behavior than any of the alternatives. We also point out that A * with sel h solved all the problems that were solved by A * with max h , and more. Finally, note that the results with rnd h in terms of both average runtime and number of problems solved clearly indicate that the impact of the concrete decision rule suggested by our model on the performance of selective max is spectacular.",
                "One more issue that is probably worth discussing is the \"sophistication\" of the classifiers that were learned for selective max. Having read so far, the reader may wonder whether the classifiers we learn are not just trivial classifiers in a sense that, for any given problem, they either always suggest computing h LM-CUT or always suggest comput-  ing h LA . However,  Table 1 <table id=#tab_2>  shows that typically this is not the case. For each problem solved by sel h , we recorded the distribution of the high-confidence decisions made by our classifier, and  Table 1 <table id=#tab_2>  shows the averages of these numbers for each domain. We say that a domain has a significant preference for heuristic h if the classifier chose h for over 75% of the search states encountered while searching the problems from that domain. Only 8 domains out of 22 had a significant preference, and even those are divided almost evenly-3 domains had significant preference for h LM-CUT , while 5 had significant preference for h LA . Finally, we have also compared sel h to two of its minor variants: in one, the successor state in each \"probe\" used to generate the initial training set is chosen from the successors uniformly, and in the other, the decision rule's threshold τ was set simply to 0. The results of this comparison are omitted here for the sake of brevity, but they both performed worse than sel h ."
            ],
            [
                "Learning for planning has been a very active field starting in the early days of planning  (Fikes, Hart, and Nilsson 1972) <ref id=#b7> , and is recently receiving growing attention in the community. So far, however, relatively little work has dealt with learning for heuristic search planning, one of the most prominent approaches to planning these days. Most works in this direction have been devoted to learning macroactions (see, e.g.,  Finkelstein and Markovitch 1998 <ref id=#b8> , Botea et al. 2005 <ref id=#b1> , and Coles and Smith 2007 <ref id=#b3> . Among the other works, the one most closely related to ours is probably the work by  Yoon, Fern and Givan (2008) <ref id=#b21>  that suggest learning an (inadmissible) heuristic function based upon features extracted from relaxed plans. In contrast, our focus is on optimal planning. Overall, we are not aware of any previous work that deals with learning for optimal heuristic search.",
                "The experimental evaluation demonstrates that selective max is a more effective method for combining arbitrary admissible heuristics than their regular point-wise maximization. Another advantage of the selective max approach is   Table 2 <table> : Results of the evaluation in details. For each pair of domain D and state evaluation method E, we give the number of problems in D that were solved by A * using E (left column), and the average search time (right column). The search time is averaged only on problems that were solved using all five methods; the number of these problems for each domain is listed in parentheses next to the domain name.",
                "that it can successfully exploit pairs of heuristics where one dominates the other. For example, the h LA heuristic can be used with two action cost partitioning schemes: uniform and optimal  (Karpas and Domshlak 2009) <ref id=#b16> . The heuristic induced by the optimal action cost partitioning dominates the one induced by the uniform action cost partitioning, but takes much longer to compute. Selective max could be used to learn when it is worth spending the extra time to compute the optimal cost partitioning, and when it is not. In contrast, the max-based combination of these two heuristics would simply waste the time spent on computing the uniform action cost partitioning."
            ]
        ],
        "Tables": [
            {
                "id": "tab_2",
                "Head": "Table 1",
                "Label": "1",
                "Description": ""
            }
        ],
        "Figures": [
            {
                "id": "fig_0",
                "Head": "Figure 2 :",
                "Label": "2",
                "Description": "Figure 2: The selective max state evaluation procedure."
            },
            {
                "id": "fig_1",
                "Head": "",
                "Label": "",
                "Description": "Summary of the evaluation. Table (a) summarizes the average search times and number of problem instances solved with each of the five methods under comparison. Plot (b) depicts the number of solved instances under different timeouts; the xand y-axes capture the timeout in seconds and the number of problems solved, respectively."
            }
        ],
        "References": [
            {
                "id": "b0",
                "Title": "Complexity results for SAS + planning",
                "Authors": [
                    "C Bäckström",
                    "B Nebel"
                ],
                "Journal": "Comp. Intell"
            },
            {
                "id": "b1",
                "Title": "Macro-FF: Improving AI planning with automatically learned macro-operators",
                "Authors": [
                    "A Botea",
                    "M Enzenberger",
                    "M Müller",
                    "J Schaeffer"
                ],
                "Journal": "JAIR"
            },
            {
                "id": "b2",
                "Title": "Hyper-heuristics: an emerging direction in modern search technology",
                "Authors": [
                    "E Burke",
                    "G Kendall",
                    "J Newall",
                    "E Hart",
                    "P Ross",
                    "S Schulenburg"
                ]
            },
            {
                "id": "b3",
                "Title": "Marvin: A heuristic search planner with online macro-action learning",
                "Authors": [
                    "A Coles",
                    "A Smith"
                ],
                "Journal": "JAIR"
            },
            {
                "id": "b4",
                "Title": "Additive-disjunctive heuristics for optimal planning",
                "Authors": [
                    "A Coles",
                    "M Fox",
                    "D Long",
                    "A Smith"
                ]
            },
            {
                "id": "b5",
                "Title": "Learning relational decision trees for guiding heuristic planning",
                "Authors": [
                    "T De La Rosa",
                    "S Jiménez",
                    "D Borrajo"
                ]
            },
            {
                "id": "b6",
                "Title": "Additive pattern database heuristics",
                "Authors": [
                    "A Felner",
                    "R Korf",
                    "S Hanan"
                ],
                "Journal": "JAIR"
            },
            {
                "id": "b7",
                "Title": "Learning and executing generalized robot plans",
                "Authors": [
                    "R Fikes",
                    "P Hart",
                    "N Nilsson"
                ],
                "Journal": "AIJ"
            },
            {
                "id": "b8",
                "Title": "A selective macrolearning algorithm and its application to the NxN sliding-tile puzzle",
                "Authors": [
                    "L Finkelstein",
                    "S Markovitch"
                ],
                "Journal": "JAIR"
            },
            {
                "id": "b9",
                "Title": "Domain-independent construction of pattern database heuristics for cost-optimal planning",
                "Authors": [
                    "P Haslum",
                    "A Botea",
                    "M Helmert",
                    "B Bonet",
                    "S Koenig"
                ]
            },
            {
                "id": "b10",
                "Title": "New admissible heuristics for domain-independent planning",
                "Authors": [
                    "P Haslum",
                    "B Bonet",
                    "H Geffner"
                ]
            },
            {
                "id": "b11",
                "Title": "Landmarks, critical paths and abstractions: What's the difference anyway",
                "Authors": [
                    "M Helmert",
                    "C Domshlak"
                ]
            },
            {
                "id": "b12",
                "Title": "How good is almost perfect",
                "Authors": [
                    "M Helmert",
                    "G Röger"
                ]
            },
            {
                "id": "b13",
                "Title": "Flexible abstraction heuristics for optimal sequential planning",
                "Authors": [
                    "M Helmert",
                    "P Haslum",
                    "J Hoffmann"
                ]
            },
            {
                "id": "b14",
                "Title": "The Fast Downward planning system",
                "Authors": [
                    "M Helmert"
                ],
                "Journal": "JAIR"
            },
            {
                "id": "b15",
                "Title": "Concise finite-domain representations for PDDL planning tasks",
                "Authors": [
                    "M Helmert"
                ],
                "Journal": "AIJ"
            },
            {
                "id": "b16",
                "Title": "Cost-optimal planning with landmarks",
                "Authors": [
                    "E Karpas",
                    "C Domshlak"
                ]
            },
            {
                "id": "b17",
                "Title": "Optimal additive composition of abstraction-based admissible heuristics",
                "Authors": [
                    "M Katz",
                    "C Domshlak"
                ]
            },
            {
                "id": "b18",
                "Title": "Structural-pattern databases",
                "Authors": [
                    "M Katz",
                    "C Domshlak"
                ]
            },
            {
                "id": "b21",
                "Title": "Learning control knowledge for forward search planning",
                "Authors": [
                    "S Yoon",
                    "A Fern",
                    "R Givan"
                ],
                "Journal": "JMLR"
            }
        ]
    },
    "learning spatial-temporal varying graphs with applications to climate data analysis": {
        "Title": "learning spatial-temporal varying graphs with applications to climate data analysis",
        "Authors": [
            "Xi Chen",
            "Yan Liu",
            "Han Liu",
            "Jaime Carbonell"
        ],
        "Abstract": "An important challenge in understanding climate change is to uncover the dependency relationships between various climate observations and forcing factors. Graphical lasso, a recently proposed 1 penalty based structure learning algorithm, has been proven successful for learning underlying dependency structures for the data drawn from a multivariate Gaussian distribution. However, climatological data often turn out to be non-Gaussian, e.g. cloud cover, precipitation, etc. In this paper, we examine nonparametric learning methods to address this challenge. In particular, we develop a methodology to learn dynamic graph structures from spatial-temporal data so that the graph structures at adjacent time or locations are similar. Experimental results demonstrate that our method not only recovers the underlying graph well but also captures the smooth variation properties on both synthetic data and climate data.",
        "Keywords": [],
        "BookMarks": [
            "Introduction",
            "Preliminary",
            "Nonparanormal",
            "Kernel Weighted Covariance Matrix",
            "Experiments",
            "Synthetic Data",
            "Climate Data",
            "Conclusion"
        ],
        "Papertext": [
            [
                "Climate change poses many critical socio-technological issues in the new century  (IPCC 2007) <ref> . An important challenge in understanding climate change is to uncover the dependency relationships between the various climate observations and forcing factors, which can be of either natural or anthropogenic (human) origin, e.g. to assess which parameters are mostly responsible for climate change.",
                "Graph is one of the most natural representations of dependency relationships among multiple variables. There have been extensive studies on learning graph structures that are invariant over time. In particular, 1 penalty based learning algorithms, such as graphical lasso, establish themselves as one of the most promising techniques for structure learning, especially for data with inherent sparse graph structures  (Meinshausen and Bühlmann 2006; <ref id=#b11> Yuan and Lin 2007) <ref id=#b15>  and have been successfully applied in diverse areas, such as gene regulatory network discovery  (Friedman 2004) <ref id=#b3> , social network analysis  (Goldenberg and Moore 2005) <ref id=#b4>  and so on. Very recently, several methods have been proposed to model time-evolving graphs with applications from gene regulatory network analysis  (Song, Kolar, and Xing 2009) <ref id=#b13> , financial data analysis  (Xuan and Murphy 2007) <ref id=#b14>  to oil-production monitoring system  (Liu, Kalagnanam, and Johnsen 2009) <ref id=#b8> .",
                "Most of the existing methods assume that the data are drawn from a multivariate Gaussian distribution at each time stamp and then estimate the graphs on a chain of time.",
                "Compared with the existing approaches for graph structure learning, there are two major challenges associated with climate data: one is that meteorological or climatological data often turn out to be non-Gaussian, e.g. precipitation, cloud cover, and relative humidity, which belong to bounded or skewed distributions  (Boucharel et al. 2009) <ref> ; the other is the smooth variation property, i.e. the graph structures may vary over temporal and/or spatial scales, but the graphs at adjacent time or locations should be similar.",
                "In this paper, we present a nonparametric approach with kernel weighting techniques to address these two challenges for spatial-temporal data in climate applications. Specifically, for a fixed time t and location s, we propose to adopt a two-stage procedure: (1) instead of blindly assuming that the data follow Gaussian or any other parametric distributions, we learn a set of marginal functions which can transform the original data into a space where they are normally distributed; (2) we construct the covariance matrix for t and s via a kernel weighted combination of all the data at different time and locations. Then the state-of-the-art graph structure learning algorithm, \"graphical lasso\"  (Yuan and Lin 2007; <ref id=#b15> Friedman, Hastie, and Tibshirani 2008) <ref id=#b2> , can be applied to uncover the underlying graph structure. It is worthwhile noting that our kernel weighting techniques are very flexible, i.e. they can account for smooth variation in many types (e.g. altitude) besides time and space. To the best of our knowledge, it is the first practical method for learning nonstationary graph structures without assuming any parametric underlying distributions."
            ],
            [
                "We concern ourselves with the problem in learning graph structures which vary in both temporal and spatial domains. At each time t and location s, we take n i.i.d. observations on p random variables which are denoted as {X ts i } n i=1 , where each X ts i := (X ts i1 , . . . X ts ip ) T ∈ R p is a p dimensional vector. Taking the climate data for example, we may independently measure several factors (variables), such as temperature, precipitation, carbon-dioxide (CO 2 ), at each location spreading at different time in a year. Our goal is to explore the dependency relationships among these variables over time and locations. Markov Random Fields (MRFs) have been widely adopted for modeling dependency relationships  (Kinderman and Snell 1980) <ref id=#b6> . For a fixed time and location, denote each observation as a p-dimensional random vector X = (X 1 , . . . X p ). We encode the structure of X with an undirected graph G = (V, E), where each node u in the vertex set V = {v 1 , . . . v p } corresponds to a component of X. The edge set encodes conditional independencies among components of X. More precisely, the edge between (u, v) is excluded from E if and only if X u is conditionally independent of X v given the rest of variables",
                "<formula_0>",
                "A large body of literature assumes that X follows a multivariate Gaussian distribution, N (µ, Σ), with the mean vector µ and the covariance matrix Σ. Let Ω = Σ −1 be the inverse of the covariance matrix (a.k.a. the precision matrix). One good property of multivariate Gaussian distributions is that",
                "<formula_1>",
                "if and only if Ω uv = 0  (Lauritzen 1996) <ref id=#b7> . Under the Gaussian assumption, we may deduce conditional independencies by estimating the inverse covariance matrix. In real world applications, many variables are conditionally independent given others. Therefore, only a few essential edges should appear in the estimated graph. In other words, the estimated inverse covariance matrix Ω should be sparse with many zero elements.",
                "Inspired by the success of \"lasso\" for linear models, Yuan and Lin proposed \"graphical lasso\" to obtain a sparse Ω by minimizing the negative log-likelihood with 1 penalization on Ω  (Yuan and Lin 2007) <ref id=#b15> . More precisely, let {X 1 , X 2 , . . . X n } be n random samples from N (µ, Σ), where each X i ∈ R p and let Σ be the estimated covariance matrix using maximum likelihood:",
                "<formula_2>",
                "where X is the sample mean. The estimator Ω is obtained by minimizing:",
                "<formula_3>",
                "where",
                "<formula_4>",
                "is the log-likelihood and λ is the tuning parameter that controls the sparsity of Ω. The minimization can be done efficiently using the algorithm in  (Friedman, Hastie, and Tibshirani 2008) <ref id=#b2> , which is a block coordinate descent algorithm that updates a single row and column of Ω at each iteration. It has been proven that, under certain conditions, Ω can recover the edge set of the underlying true graph with high probability  (Ravikumar et al. 2008 <ref id=#b12> )."
            ],
            [
                "As discussed in the previous section, graphical lasso can estimate an inverse covariance matrix with good statistical properties as long as the data are drawn from a multivariate Gaussian distribution. However, this is not the case in many applications. Take our climate data for example, we have 39 measurements of CO 2 at a location on California coast in the first quarter over 13 years  (1990 ∼ 2002) <ref> . The density and Q-Q plot are presented in  Figure 1 <figure id=#fig_0> . The p-value of Anderson-Darling normality test is 0.0133 < 0.05, which rejects the null hypothesis and hence indicates that samples are not normally distributed. However, in many cases, it is possible to find a set of marginal functions to transform the original data into another space so that they are normally distributed. More precisely, if there exists a set of univariate functions",
                "<formula_5>",
                ", we say that X follows a nonparanormal (NPN) distribution  (Liu, Lafferty, and Wasserman 2009) <ref id=#b9>  and denote it as:",
                "<formula_6>",
                ", we adopt the method in  (Liu, Lafferty, and Wasserman 2009) <ref id=#b9>  to find a set of good estimators of {f j } p j=1 . We constrain each f j to be monotone and differentiable. Moreover, we demand that f j preserves the mean and variance for the identifiability consideration:",
                "<formula_7>",
                "To find a good estimator of f j , we start by writing down the cumulative distribution function (CDF) of f j under our basic assumption f j (X j ) ∼ N (µ, Σ):",
                "<formula_8>",
                "where Φ(•) is the CDF of a standard Gaussian distribution. Let F j (x) = P(X j ≤ x) denote the CDF of X j , the monotone property of f j implies that",
                "<formula_9>",
                "Connecting (6) and (5), we obtain",
                "<formula_10>",
                "Figure 2: Inverse of the CDF of a standard Gaussian distribution (left). The truncated empirical CDF with red lines representing the truncations (right).",
                "which implies that",
                "<formula_11>",
                "By substituting µ j , σ j and F j (x) in (8) with the sample mean µ j , the sample standard deviation σ j and the empirical CDF F j (x) defined as below, we obtain an estimator of f j .",
                "<formula_12>",
                "However, the transformation function (8) is not well defined since Φ −1 (x) approaches negative or positive infinity when x approaches to 0 or 1 as shown in the left panel of  Figure 2 <figure> . To tackle this problem, we use the approach in  (Liu, Lafferty, and Wasserman 2009) <ref id=#b9>  to truncate the empirical CDF F j (x) in the following manner so that our new CDF estimator F j (x) is bounded away from 0 and 1:",
                "<formula_13>",
                "where δ is a truncation parameter. The truncated empirical CDF for our motivating example is presented in the right panel of  Figure 2 <figure> . We set the truncation parameter δ to be 1/(4n 1/4 √ π log n) as in  (Liu, Lafferty, and Wasserman 2009) <ref id=#b9> , which leads to O P (log(n)/n 1/4 ) rate of convergence of Ω.",
                "By plugging F j (x), µ j and σ j back into (8), we obtain our estimator of f j :",
                "<formula_14>",
                "After taking the transformations in (10), the data are",
                "<formula_15>",
                "The maximum likelihood estimator of the mean and the covariance matrix",
                "<formula_16>",
                "Back to our motivating example at the beginning of the section, we present the density and Q-Q plot of the transformed data in  Figure 3 <figure id=#fig_1> . The p-value of Anderson-Darling normality test is 0.9995 which strongly indicates that transformed data are normally distributed."
            ],
            [
                "For a fixed time t and location s, we have n measurements",
                "<formula_17>",
                ", we estimate f ts by f ts in (10) and obtain the covariance matrix Σ ts in (11). By substituting Σ in (3) with Σ ts and minimizing (2), we obtain the estimated inverse covariance matrix for a single time and location.",
                "However, this simple approach does not take into account the rich information on temporal and spatial constraints, i.e. the graph structures of two adjacent locations (e.g. New York and New Jersey) should be more similar than those of two faraway locations (e.g. New York and San Francisco). Similarly, the difference between graphs in winter and spring should be smaller than that between graphs in winter and summer. To capture this smooth variation property, we use all the data at different time and locations to construct a weighted covariance matrix S ts as an estimator for the covariance matrix at t and s:",
                "<formula_18>",
                "where w tt ss is the weighting of the difference between time location pairs (t, s) and (t , s ). The idea behind the kernel weighting technique is that all the data should contribute to the estimated covariance matrix at t and s. The smooth variation property requires that when t is close to t and/or s is adjacent to s, w tt ss should be large since the data from t and s are more important for constructing S ts . A natural way to define w tt ss is to utilize the product kernel: where",
                "<formula_19>",
                "<formula_20>",
                "h is a symmetric nonnegative kernel function and h t , h s are the kernel bandwidths for time and space. For example, one of the most widely adopted kernel functions is Gaussian RBF kernel where",
                "<formula_21>",
                "2h 2 ). In this paper, each time stamp is represented by a single discrete number. The absolute value of the difference between two time stamps is adopted to measure their distance. Each location is represented as a two dimensional vector composed of its longitude and latitude. And the distance between two locations is defined by their Euclidean distance, i.e. their vector 2 norm.",
                "Note that this kernel weighting technique is very flexible. For example, if we have more continuity constraints besides time and space, we can easily extend the product kernel to include all these conditions to enforce the smooth variation effect.",
                "With the weighting technique above, the estimated covariance matrices, S ts , are \"smooth\" in time and space, i.e. estimated covariance matrices of two adjacent time stamps or places should not differ too much. Then we plug S ts into (3) to replace Σ and obtain the estimated sparse inverse covariance matrix Ω ts ."
            ],
            [
                "In our experiment, we compare four different methods on both synthetic data and our motivating climate dataset: 1. Kernel Weighted Nonparanormal: taking the transformation in (10) and using the kernel weighted estimator of the covariance matrix in (12). 2. Kernel Weighted Normal: using the kernel weighted estimator of the covariance matrix based on the raw data. 3. Nonparanormal: taking the transformation in (10) but without the kernel weighting step. 4. Normal: directly computing the sample covariance at each time and location."
            ],
            [
                "For the synthetic data experiment, we only consider timevarying graphs for ease of illustration. In fact, we can adapt our method to time-varying graphs simply by replacing the product kernel in (13) with a kernel only involving time:",
                "<formula_22>",
                ". We set the number of nodes p = 20, the number of edges e = 15, the number of time stamps T = 20, the sample size for each time stamp n = 50 and the maximum node degree to be 4. The observation sequence for synthetic Markov Random Fields are generated as follows:",
                "1. Generate an Erdös-Rényi random graph G 1 = (V 1 , E 1 ). Then from t = 2 to T , we construct the graph G t = (V t , E t ) by randomly adding one edge and removing one edge from G t−1 and taking care that the maximum node degree is still 4. 2. For each graph G t , generate the inverse covariance matrix Ω t as in  (Meinshausen and Bühlmann 2006) <ref id=#b11> :",
                "<formula_23>",
                "where 0.245 guarantees the positive definiteness of Ω t when the maximum node degree is 4. 3. For each t, we sample n data points from a multivariate Gaussian distribution with mean µ = (1.5, . . . 1.5) and covariance matrix Σ t = (Ω t ) −1 :",
                "<formula_24>",
                "i , we take the Gaussian CDF transformation {g j (•)} p j=1 on each dimension and generate the corresponding X t i :",
                "<formula_25>",
                ". The Gaussian CDF transformation function g(x) takes the basic form of Φ(",
                "x−µg σg ) and is scaled to preserve mean and variance. In general, it transforms a standard Gaussian data into a bi-modal distribution as shown in  Figure 4 <figure id=#fig_2> . Here we omit the rigorous definition due to space limitations; interested readers may refer to  (Liu, Lafferty, and Wasserman 2009) <ref id=#b9> .",
                "We run four methods with the bandwidth h t = T • 5.848 N 1/3 = 11.7, where N = n • T = 1000 is the total number of data points and  5.848 <ref>  N 1/3 is a widely adopted plug-in bandwidth for nonparametric learning. We independently simulate the above procedure for 50 times and evaluate different methods based on F1-Score which is the harmonic mean of precision and recall in retrieving the true graph edges. The result is presented in the left panel of  Figure 5 <figure> . As we can see, at all 20 time stamps, Kernel Weighted Nonparanormal achieves significantly higher F1-Score as compared to other methods. Moreover, we plot the ROC curve at t = 1 for randomly selected simulation on the right of  Figure 5 <figure> . Kernel Weighted Nonparanormal is still superior to other methods. For other time, the ROC curves exhibit similar patterns."
            ],
            [
                "We run our proposed method on a climate dataset  (Lozano et al. 2009) <ref id=#b10> , which contains monthly data of 18 different climatological factors from 1999 to 2002. The observations span 125 locations in the U.S. on an equally spaced grid with the range of latitude from 30.475 to 47.975 and the range of longitude from -119.75 to -82.25. Each location s is denoted by (s 1 , s 2 ) where s 1 is the latitude and s 2 is  Figure 5 <figure> : F1-Score (left) and ROC Curve (right) the longitude. The 18 climatological factors measured for each month include CO2, CH4, H2, CO, average temperature (TMP), diurnal temperature range (DTR), minimum temperate (TMN), maximum temperature (TMX), precipitation (PRE), vapor (VAP), cloud cover (CLD), wet days (WET), frost days (FRS), global solar radiation (GLO), direct solar radiation (DIR), extraterrestrial radiation (ETR), extraterrestrial normal radiation (ETRN) and UV aerosol index (UV). (for more details, see  (Lozano et al. 2009) <ref id=#b10> ).",
                "At a specific location, we divide a year into 4 quarters and treat the data in the same quarter of all years as separate i.i.d. observations from a nonparanormal distribution. We set the tuning parameter λ = 0.15 to enforce a moderate sparsity of graphs. We use Gaussian RBF kernel with the bandwidth h t = 4 •  5.848 <ref>  n 1/3 = 0.87 where 4 is the number of quarters in a year and n = 19500 is the total number of observations. Similarly, the bandwidth h s is set to be max",
                "<formula_26>",
                "n 1/3 = 9. As a typical example, we show in  Figure 6 <figure>  the estimated graphs for 4 quarters at the location (30.475 , -114.75) which is a place in CA south of San Diego. We see that the graph structures are quite \"smooth\" between every two adjacent quarters except for quarter 3 and 4. It indicates that the climate may change more significantly between 3rd and 4th quarter. Another interesting observation is that some factors are clustered on the graph, such as {CO2, CH4, H2}, {DIR, UV, GLO}, etc. It indicates that these factors are highly correlated and should be studied together by meteorologists. In fact, it is quite possible that, due to the greenhouse effect, {CO2, CH4, H2} are highly correlated.",
                "We show some examples to illustrate the spatial smoothness. For an adjacent location (32.975, -117.250) (in CA between San Diego and Los Angeles) and a faraway location (42.975, -84.75) (in Michigan) from the one in  Figure  6 <figure> , the estimated graphs for 4 quarters are shown in  Figure 7 <figure>  and  Figure 8 <figure>  respectively. As we can see, there are at most 2 different edges for each quarter between  Figure 6 <figure>  and 7. But  Figure 6 <figure>  and 8 are very different.",
                "Moreover, by varying the tuning parameter λ in (2) from a large value to a small one, we obtain the full regularization path which could be useful to identify the influence of other factors on a specific factor of interest, e.g. CO2. More precisely, the order in which the edges appear on the regularization path indicates their degree of influence on a particular factor. As an illustration,  Figure 9 <figure>  shows the changing of the edges connecting CO2 in the first quarter at location   (30.475 , -114.75) <ref> . From the plots, we see that the edge between CO2 and CH4 appears first, followed by H2 and then DIR. It indicates that the amount of CH4 is the most crucial factor to estimate CO2, and the second is H2, the third is DIR, etc. The result is quite interpretable in meteorology. In fact, CO2 is mainly produced by burning fossil fuels which primarily consists of CH4. In addition, the generating capacity of fossil fuel, formed by organic matters mixed with mud, is directly determined by solar radiation (Chapter 7 in IPCC 2007). These domain facts seem to suggest that the graph structures we learned are quite reasonable. Furthermore, they might be able to provide additional insights to help meteorologists better understand the dependency relationships among these factors. Finally, we run the full regularization path of Kernel Weighted Normal method and find the graph having the smallest symmetric difference compared to the first quarter of  Figure 6 <figure> . The symmetric difference is plotted in the left of  Figure 10 <figure id=#fig_0> , where we see that several factors, such as UV, involve several edges in the symmetric difference graph. It indicates that our marginal transformations on these factors change them substantially. To see this, we select two representative factors, UV and CO and plot their marginal   30.475 , -114.75 <ref> ). The red edges are those appear in the graph with transformations and the black edges are those appear in the graph without transformations. Estimated transformation for UV (middle) and CO (right). The black dashed lines plot identity map and the red lines indicate the transformations on the data.",
                "transformations in the middle and right of  Figure 10 <figure id=#fig_0> . As we can see, for UV, the transformation does change the original data. In contrast, for CO, there is no associated edge in the symmetric difference graph which suggests that the transformation might have no effect. This could be verified by the right panel of  Figure 10 <figure id=#fig_0>  where the transformation and identity map on the data nearly coincide."
            ],
            [
                "Motivated by the task of analyzing climate data, we develop a two-stage procedure to learn dynamic graph structures when the underlying distributions are non-Gaussian. In the first stage, we learn a set of marginal functions that transform the data to be normally distributed. In the second stage we use the kernel weighting technique to construct an estimated covariance matrix and then adopt graphical lasso to uncover the underlying graph structure. Empirical results show that our method not only better recovers each single graph structure when the distributions are highly skewed, but also captures the smooth variation property in both spatial and temporal domains. This paper is a preliminary work aiming at modeling the dependency relationships among different climate factors with powerful structure learning tools. The next step is to collaborate with meteorologists and incorporate domain knowledge constraints to find more interesting structures. Our hope is that the structures we learned could help meteorologists better understand the climatological phenomena and lead to new discoveries in the field of climatological analysis."
            ]
        ],
        "Tables": [],
        "Figures": [
            {
                "id": "fig_0",
                "Head": "Figure 1 :",
                "Label": "1",
                "Description": "Figure 1: Density plot (left) and Q-Q plot (right) of raw CO 2 data"
            },
            {
                "id": "fig_1",
                "Head": "Figure 3 :",
                "Label": "3",
                "Description": "Figure 3: Density plot (left) and Q-Q plot (right) of the transformed CO 2 data"
            },
            {
                "id": "fig_2",
                "Head": "Figure 4 :",
                "Label": "4",
                "Description": "Figure 4: Density plot for the normally distributed data (left) and transformed data (right) by the Gaussian CDF transformation"
            },
            {
                "id": "fig_3",
                "Head": "Figure 6 :Figure 7 :Figure 8 :",
                "Label": "678",
                "Description": "Figure 6: Estimated graphs at location(30.475 , -114.75). The common ones between time (t mod 4) and (t + 1 mod 4) are colored as green.Quarter 1Quarter 2 Quarter 3 Quarter 4"
            },
            {
                "id": "fig_4",
                "Head": "Figure 9 :Figure 10 :",
                "Label": "910",
                "Description": "Figure 9: Estimated graphs using different λs in the first quarter at(30.475 , -114.75)"
            }
        ],
        "References": [
            {
                "id": "b1",
                "Title": "Enso's non-stationary and non-gaussian character: the role of climate shifts",
                "Authors": [],
                "Journal": "Nonlinear Processes in Geophysics"
            },
            {
                "id": "b2",
                "Title": "Sparse inverse covariance estimation with the graphical lasso",
                "Authors": [
                    "J Friedman",
                    "T Hastie",
                    "R Tibshirani"
                ],
                "Journal": "Biostatistics"
            },
            {
                "id": "b3",
                "Title": "Inferring cellular networks using probabilistic graphical models",
                "Authors": [
                    "N Friedman"
                ],
                "Journal": "Science"
            },
            {
                "id": "b8",
                "Title": "Learning dynamic temporal graphs for oil-production equipment monitoring system",
                "Authors": [
                    "Y Liu",
                    "J Kalagnanam",
                    "O Johnsen"
                ]
            },
            {
                "id": "b9",
                "Title": "The nonparanormal: Semiparametric estimation of high dimensional undirected graphs",
                "Authors": [
                    "H Liu",
                    "J Lafferty",
                    "L Wasserman"
                ],
                "Journal": "J. Mach. Learn. Res"
            },
            {
                "id": "b10",
                "Title": "Spatial-temporal causal modeling for climate change attribution",
                "Authors": [
                    "A Lozano",
                    "H Li",
                    "A Niculescu-Mizil",
                    "Y Liu",
                    "C Perlich",
                    "J Hosking",
                    "Abe",
                    "N"
                ]
            },
            {
                "id": "b11",
                "Title": "High dimensional graphs and variable selection with the lasso",
                "Authors": [
                    "N Meinshausen",
                    "P Bühlmann"
                ],
                "Journal": "The Annals of Stat"
            },
            {
                "id": "b12",
                "Title": "Model selection in gaussian graphical models: Highdimensional consistency of 1 -regularized mle",
                "Authors": [
                    "P Ravikumar",
                    "G Raskutti",
                    "M Wainwright",
                    "B Yu"
                ]
            },
            {
                "id": "b13",
                "Title": "Time-varying dynamic bayesian networks",
                "Authors": [
                    "L Song",
                    "M Kolar",
                    "E Xing"
                ]
            },
            {
                "id": "b14",
                "Title": "Modeling changing dependency structure in multivariate time series",
                "Authors": [
                    "X Xuan",
                    "K Murphy"
                ]
            },
            {
                "id": "b15",
                "Title": "Model selection and estimation in the gaussian graphical model",
                "Authors": [
                    "M Yuan",
                    "Lin",
                    "Y"
                ],
                "Journal": "Biometrika"
            }
        ]
    },
    "good rationalizations of voting rules": {
        "Title": "good rationalizations of voting rules",
        "Authors": [
            "Edith Elkind",
            "Piotr Faliszewski",
            "Arkadii Slinko"
        ],
        "Abstract": "We explore the relationship between two approaches to rationalizing voting rules: the maximum likelihood estimation (MLE) framework originally suggested by Condorcet and recently studied in (Conitzer and Sandholm 2005;Conitzer, Rognlie, and Xia 2009) and the distance rationalizability (DR) framework (Meskanen and Nurmi 2008;Elkind, Faliszewski, and Slinko 2009). The former views voting as an attempt to reconstruct the correct ordering of the candidates given noisy estimates (i.e., votes), while the latter explains voting as search for the nearest consensus outcome. We provide conditions under which an MLE interpretation of a voting rule coincides with its DR interpretation, and classify a number of classic voting rules, such as Kemeny, Plurality, Borda and Single Transferable Vote (STV), according to how well they fit each of these frameworks. The classification we obtain is more precise than the ones that result from using MLE or DR alone: indeed, we show that the MLE approach can be used to guide our search for a more refined notion of distance rationalizability and vice versa.",
        "Keywords": [],
        "BookMarks": [
            "Introduction",
            "Preliminaries",
            "MLEs, SRSFs and DR",
            "Main Results",
            "Conclusions"
        ],
        "Papertext": [
            [
                "Various aspects of voting, and, more generally, preference aggregation, are an active research topic in the artificial intelligence community. Indeed, voting can be used in a variety of applications that range from decision-making in multiagent planning  (Ephrati and Rosenschein 1997) <ref id=#b6>  to ranking movies  (Ghosh et al. 1999) <ref id=#b7>  to aggregating the outputs of web search engines  (Dwork et al. 2001) <ref id=#b3> .",
                "Voting has a rich history going back to ancient times, and, unsurprisingly, the human societies explored many different approaches to joint decision making, resulting in a number of voting rules, or algorithms for determining the best alternative or the optimal ordering of the alternatives. A natural question, then, is which voting rule is most appropriate for a given scenario. One can try to answer this question by choosing a rule that satisfies the voting axioms that are most pertinent to the problem in hand, such as monotonicity or Pareto-optimality. However, if the voters are seen as cooperative entities that aim to aggregate information about the alternatives in the presence of errors-a viewpoint advocated by Condorcet and appropriate for many of the AI applications of voting-an attractive approach to choose a voting rule that has a natural interpretation within the Maximum Likelihood Estimation (MLE) framework, or its younger cousin, the distance rationalizability framework (DR).",
                "The MLE framework is based on the idea that there is some objective ordering of the candidates from best to worst, and each vote is a noisy estimate of this ordering. Thus, the role of the voting rule is to find a ranking that is most likely to be the objectively correct one. Formally, in the MLE framework we assume that there exists a probability distribution of votes that is conditioned on the correct ranking, and that each voter draws her vote randomly and independently from this distribution. If a voting rule outputs a ranking that is most likely to be the correct one given the distribution, then we say that this rule is MLE.",
                "The first voting rule that has been shown to fit the MLE framework is the Kemeny rule  (Young and Levenglick 1978; <ref id=#b9> Young 1988; <ref id=#b10> 1995) <ref id=#b11> . Subsequently,  Conitzer and Sandholm (2005) <ref id=#b1>  have proved that all scoring rules (a large class of voting rules that includes Plurality, Borda and Veto, among others) are MLE, while many other voting rules, such as Bucklin, Copeland or Maximin are not (the case of Single Transferable Vote (STV) is more complicated and has been fully analyzed in  (Conitzer, Rognlie, and Xia 2009) <ref id=#b2> ).",
                "A related, but different approach is to interpret voting as search for a consensus. This is the main idea behind the distance rationalizability framework introduced in (Meskanen and Nurmi 2008;  Elkind, Faliszewski, and Slinko 2009) <ref id=#b4> . In this framework, we define a distance between elections, and seek the closest consensus election (an election with a single, clear winner) to the given election. A voting rule is said to be distance-rationalizable if it elects the winner of the nearest consensus, for some distance and some class of consensus elections (such as, e.g., elections where all voters agree on the ranking of candidates, or elections where all voters rank the same candidate first).  Meskanen and Nurmi (2008) <ref id=#b8> ,  and Elkind, Faliszewski and Slinko (2009) <ref id=#b4>  show that almost all voting rules are distance-rationalizable, including some rules that are provably not MLE.",
                "However, not all existing MLE and distancerationalizability results are equally appealing. For example, the noise model in the MLE interpretation of the Kemeny rule is extremely natural (a voter provides a correct ranking of each pair of candidates with a fixed probability p > 0.5), while for the scoring rules the noise model of  Conitzer and Sandholm (2005) <ref id=#b1>  does not seem to have a simple interpretation. Nevertheless, the MLE framework alone does not provide us with a principled way of formalizing this intuition. For distance rationalizability, the need to refine the original framework of  (Meskanen and Nurmi 2008; <ref id=#b8> Elkind, Faliszewski, and Slinko 2009) <ref id=#b4>  is even more striking. Indeed,  Elkind, Faliszewski, and Slinko (2010) <ref id=#b5>  show that unless we place additional restrictions on the type of distances used, essentially any voting rule can be distance-rationalized, rendering distance-rationalizability results meaningless. To remedy this, they propose to focus on rationalizing voting rules via so-called votewise distances, which first measure by how much each voter has to modify its vote to reach a consensus, and then aggregate these measurements in a uniform manner. Some, but not all distance rationalizability results can be shown to hold with respect to such distances.",
                "An interesting property of the votewise distances is that their definition is syntactically very similar to that of simple ranking scoring functions (SRSFs) introduced by  Conitzer, Rognlie and Xia (2009) <ref id=#b2>  in the context of the MLE framework. Specifically,  Conitzer, Rognlie and Xia (2009) <ref id=#b2>  show that SRSFs are, in fact, equivalent to MLE rules. Thus, our first goal in this paper is to better understand the connection between SRSFs and distance rationalizability. It turns out that, while in general these notions are incomparable, we can identify additional constraints under which they become almost identical. Interestingly, the SRSF for the Kemeny rule satisfies these constraints, which means that Kemeny can be shown to be both MLE and DR via the same underlying function. On the other hand, for scoring rules, this is not the case: while they have an interpretation within both frameworks, this interpretation is substantially different. In other words, if we simply ask which rules are both MLE and DR, scoring rules are indistinguishable from Kemeny, but if we ask which rules can be represented as MLE and DR in a consistent manner, the Kemeny rule emerges as a better choice. Thus, the DR framework can be used to refine the MLE framework, i.e., to explain why some rules are better maximum likelihood estimators than others. The converse is also true: the very connection between SRSFs and DR is an additional argument in favor of votewise distances, as only such distances can be interpreted as SRSFs.",
                "To illustrate this idea, in the second part of the paper, we consider four rules-Kemeny, Plurality, Borda, and STVand rank them according to how well they can be represented as DR, MLE, or both. Our approach places all these rules in different categories, with Kemeny being the best, Plurality a close second, and STV failing the test completely. This allows us to conclude that combining MLE and DR leads to a better understanding of voting rules than either of these approaches on its own."
            ],
            [
                "Elections. An election is modelled as a pair E = (C, V ), where C = {c 1 , . . . , c m } is a set of candidates and V = (v 1 , . . . , v n ) is a list of voters. Each voter v i is described by a linear order i over C, called her preference order. A collection of preference orders is called a preference profile. When the set of candidates is fixed, we will sometimes identify E with V . We interpret i as the ranking of candidates according to the i-th voter. Thus, a 1 b 1 c means that the first voter prefers a to b to c. For brevity, we will often write abc in place of a i b i c. For a set of candidates C, we denote by L(C) the set of all possible preference orders over C. Given a linear order v over a candidate set C and a permutation π : C → C, let π(v) denote the order obtained from v by replacing each candidate c ∈ C with π(c). We say that a function φ defined on L(C) k for k ≥ 1 is neutral if for any π :",
                "<formula_0>",
                "Neutrality is a very natural requirement in the context of voting, so from now on we assume that all functions on L(C) k that we consider (voting rules, distances, noise models, etc.) are neutral.",
                "Preference functions and voting rules. In this paper, we distinguish voting rules, i.e., mappings from elections to subsets of candidates, and preference functions, i.e., mappings from elections to subsets or candidate rankings. Formally, a voting rule is a mapping F that given an election E = (C, V ) outputs a set W ⊆ C of election winners, and a preference function is a mapping f that given an elec-",
                "<formula_1>",
                "The interpretation here is that f views each of r 1 , . . . , r t as an equally good ranking of the candidates from C given votes V . Given a preference function f , we can construct a voting rule",
                "<formula_2>",
                "A number of prominent preference functions are defined via families of so-called scoring protocols. A scoring protocol for m candidates can be identified with a vector",
                "<formula_3>",
                "Under this protocol, a candidate c receives α j points from each voter that puts c in the j-th position in her ranking. Given an election (C, V ) with |C| = m, the corresponding preference function f α outputs a set of linear orders that rank members of C in the order of decreasing number of points (there may be many rankings satisfying this condition if some candidates have the same number of points). We will focus on two most prominent families of scoring protocols: Plurality and Borda. Plurality is defined via scoring protocols of the form (1, 0, . . . , 0), i.e., under Plurality candidates get points for being ranked first only. For elections with m candidates, Borda is defined via vector (m − 1, m − 2, . . . , 0).",
                "In Single Transferable Vote (STV) preference function the rankings are created as follows: We find a candidate with the lowest Plurality score, remove him from the votes, place him on the last available position in the output ranking, and repeat the process with the modified votes until all candidates are processed. For STV the issue of handling ties-that is, the issue of the order in which candidates with equal Plurality scores are handled-is quite important, and is discussed in detail by  Conitzer, Rognlie and Xia (2009) <ref id=#b2> ; however, our results are independent of the choice of a tie-breaking rule.",
                "Finally, given an election E = (C, V ) with |V | = n, Kemeny's preference function outputs the rankings that minimize the expression",
                "<formula_4>",
                "is the number of swaps of adjacent candidates needed to transform into i . (Equivalently, d s ( i , ) is the number of inversions between and i .)"
            ],
            [
                "We will now formally define the two approaches to thinking about voting rules that are discussed in this paper, i.e., the maximum likelihood estimation (MLE) framework and the distance rationalizability (DR) framework, as well as the notion of simple ranking scoring functions (SRSFs) that provides a bridge between them. Let us fix a candidate set C throughout this section. MLEs and SRSFs. The following overview is based on  (Conitzer, Rognlie, and Xia 2009) <ref id=#b2> . For each v, r ∈ L(C), a noise model ν specifies a conditional probability P ν (v|r), that is, the probability that a voter submits a ranking v given that the \"correct\" ranking is r. We say that a preference function f can be interpreted as a maximum likelihood estimator (MLE) if there exists a noise model ν such that for each preference profile",
                "<formula_5>",
                "Intuitively, this definition assumes that the votes are distributed according to ν in an i.i.d. fashion.",
                "A preference function f is a simple ranking scoring function if there exists a function s f :",
                "<formula_6>",
                "Via a slight abuse of notation, we will often refer to the function s f itself as the simple ranking scoring function. Each preference function f that can be interpreted as an MLE is an SRSF: if f is MLE via a noise model ν, we let s f (v, u) = − ln(P ν (v|u)) for any u, v ∈ L(C); the converse is also true.",
                "Preference functions that can be interpreted as MLE include the Kemeny rule and all scoring rules. Distance rationalizability. The definition of distance rationalizability has two main ingredients: a notion of distance and a notion of consensus.",
                "<formula_7>",
                "The last condition is called the triangle inequality. If d satisfies all conditions except (b), then d is called a pseudodistance.",
                "A consensus is a set of elections with a clear winner. The three most standard consensus classes are the strong unanimity consensus S, which consists of all elections in which all voters rank the candidates in the same way, the weak unanimity consensus U, which consists of all elections in which all voters rank the same candidate first, and the Condorcet consensus C, which consists of all elections that have a Condorcet winner, i.e., a candidate that would beat any other candidate in a pairwise election. Additionally, the majority consensus M consists of all elections in which a majority of voters ranks the same candidate first.",
                "The following definition is adapted from  (Elkind, Faliszewski, and Slinko 2009) <ref id=#b4> . A voting rule F is said to be distance-rationalizable (DR) with respect to a consensus class X ∈ {S, U, M, C} if for any n ≥ 1 there is a distance d over L n (C) such that for each collection V = (v 1 , . . . , v n ) of voters, a candidate is a winner in (C, V ) under F if and only if he is a winner in a nearest (with respect to d) election in X . The definition above differs from the one given in  (Elkind, Faliszewski, and Slinko 2009) <ref id=#b4>  in that it requires the distance to be defined on profiles of the same length; for our purposes, this distinction is irrelevant.",
                "Recall that a norm on a vector space S over R is a mapping N :",
                "<formula_8>",
                "for any u, v ∈ S, and (c) N (u) = 0 if and only if u is the zero vector. The class of votewise distances introduced in  (Elkind, Faliszewski, and Slinko 2010) <ref id=#b5>  consists of all product metrics obtained by composing a distance",
                "<formula_9>",
                "is said to be votewise if there exist a distance d :",
                "<formula_10>",
                ". Note that we can define d in the same manner for an arbitrary function d : L(C) × L(C) → R + ∪ {0}, i.e., d need not to be a metric.  Elkind, Faliszewski and Slinko (2010) <ref id=#b5>  show that essentially any voting rule is DR with respect to S. However, the distance used in their construction is not votewise. The rules that are known to be DR via votewise distances include a variant of the Bucklin rule, the Dodgson rule, the Kemeny rule, Plurality, and all \"good\" scoring rules, i.e., those with α i = α j for any i = j; for all of these rules except for Bucklin, the corresponding distance is additively votewise. These DR results make use of all four consensus classes listed above: Bucklin is DR with respect to M, Dodgson is DR with respect to C, Kemeny is DR with respect to S, and Plurality and the \"good\" scoring rules are DR with respect to U. SRSF vs DR. There is a remarkable similarity between the definition of a simple ranking scoring function and that of a distance-rationalizable voting rule. However, in general the two notions are incomparable.",
                "First, in the definition of SRSF, the score of a profile is obtained as a sum of individual scores, while the definition of distance-rationalizability allows arbitrary distances. Thus, for the purposes of the comparison, we need to focus on rules that are DR via additively votewise distances. Note that Elkind, Faliszewski and Slinko (2010) argue that we should restrict ourselves to votewise distances when proving DR results; the comparison with SRSFs provides another argument in favor of that position. Second, in the definition of an SRSF, we try to minimize the sum of scores with respect to a single ranking, while in the definition of a DR rule we compute the distance to a consensus, i.e., a profile of rankings. This leads us to another refinement of the notion of distancerationalizability: namely, rationalizability with respect to the consensus class S. Indeed, a strong unanimity consensus can be represented by a single vote, so finding a vote u that minimizes the expression",
                "<formula_11>",
                "where s f is the additively votewise function that corresponds to s f . With these constraints in place, given a voting rule that is rationalized with respect to S via some distance d on votes, we can form a noise model so that for each u, v ∈ L(C), P d (u|v) = ce −d  (u,v) <ref>  , where c is a normalization constant. This noise model almost proves that f is an MLE: It leads to rankings with correct candidate ranked first, but makes no guarantees as to how further candidates are ranked (recall that the definition of DR applies to voting rules, not to preference functions).",
                "Conversely, consider a preference function F f that corresponds to an SRSF function s f . For the voting rule F f to be distance-rationalizable via s f , the function s f needs to be a metric. However, the definition of an SRSF imposes no restrictions on s f : in particular, it can be asymmetric, or fail the triangle inequality.",
                "Nevertheless, there exists a voting rule for which we can show that it is both SRSF and additively votewise DR by using the same function, namely, the Kemeny rule! Indeed, it is not hard to see that the function d s in the definition of Kemeny rule is a metric. Thus, the Kemeny rule can be consistently explained in both frameworks. It is interesting to ask if other voting rules also have this property; arguably, such rules provide the most principled approach to preference aggregation. Thus, in the next section, we subject three classic voting rules-Plurality, Borda, and STV-to this test."
            ],
            [
                "In this section we implement the program outlined in the introduction and in the preceding section. Specifically, we show that (a) Plurality is additively votewise DR with respect to S, but not via any of its SRSFs, (b) Borda is not additively votewise DR with respect to S, and (c) STV is not votewise DR with respect to S, U or C, even if we allow a very general class of norms instead of 1 .",
                "Our proofs proceed by constructing counterexamples for the special case of three candidates. Unless stated otherwise, we assume that the candidate set is C = {a, b, c}.",
                "Consider a distance d over C. By symmetry and neutrality, d is completely described by its values on the pairs (abc, abc), (abc, acb), (abc, bac), (abc, bca), (abc, cab), and (abc, cba).   Table 1 <table id=#tab_0>  gives the values abc acb bac bca cab cba abc 0",
                "<formula_12>",
                "<formula_13>",
                "of voters and a ranking r, by d(V, r) we mean  r) <figure> . Plurality.  Meskanen and Nurmi (2008) <ref id=#b8>  show that Plurality is DR with respect to U. The distance employed in their construction is additively votewise. Further, Plurality is rationalizable with respect to S via an additively votewise pseudodistance: for any two votes u, v ∈ L(C), u = v, we can set d(u, v) = 0 if u and v rank the same candidate first and d(u, v) = 1 otherwise. We can strengthen the latter result to additively votewise distance-rationalizability.",
                "<formula_14>",
                "Theorem 1. Plurality is additively votewise DR with respect to S.",
                "Proof. We start with the construction for three candidates. Let d be the distance given by S = T = 1, B = C = 2. We claim that d rationalizes Plurality with respect to S.",
                "To see this, consider an election E = (C, V ) with C = {a, b, c} that has a 1 voters with preferences abc, a 2 voters with preferences acb, b 1 voters with preferences bca, b 2 voters with preferences bac, c 1 voters with preferences cab, and c 2 voters with preferences cba.",
                "We have d(V, abc) = a 2 +b 1 +c 1 +2b 2 +2c 2 , d(V, acb) = a 1 + b 2 + c 2 + 2b 1 + 2c 1 . Thus, the distance from V to the nearest profile in S with winner a is min{a 2 + b 2 + c 2 , a 1 + b 1 + c 1 } + (b 1 + c 1 + b 2 + c 2 ). Symmetrically, the distance from V to the nearest profile in S with winner b is min{a 2 + b 2 + c 2 , a 1 + b 1 + c 1 } + (a 1 + c 1 + a 2 + c 2 ), and the distance from V to the nearest profile in S with winner c is min{a",
                "<formula_15>",
                "We observe that the first component of these expressions is identical, and the second component counts the number of voters that do not rank a (respectively, b, c) first. Thus, the set of Plurality winners coincides with the set of winners in the nearest strong consensus profiles with respect to d.",
                "We will now extend this construction to any number of candidates. Fix C = {c 1 , . . . , c m }. For two votes u, v ∈ L(C), we say that v can be obtained from u by a cyclic shift if there exists an i ∈ [m] and a permuta- tion π :",
                "<formula_16>",
                "Partition L(C) into m groups L 1 , . . . , L m , where the voters in L i rank c i on top. Set s = (m − 1)! and, for each i ∈ [m], number the votes in L i as v 1 i , . . . , v s i so that for any i, j ∈ [m] the vote v t j can be obtained from the vote v t i by a cyclic shift. This is possible, since for each u",
                "<formula_17>",
                ", there is exactly one vote in L j that can be obtained from v t i by a cyclic shift. Now, set   [m] <ref> , i.e., c i has the largest number of first-place votes. Thus, Plurality is distance-rationalizable with respect to S via d.",
                "<formula_18>",
                "Yet, it is impossible to set the values B, C, S, T so that d rationalizes Plurality with respect to S in such a way that the nearest S-consensus orders the candidates by their Plurality scores: If d is a distance that additively rationalizes Plurality with respect to S, then d is not an SRSF for Plurality.",
                "Indeed, let k ≥ 2 be an integer, and consider a collection V of 2k − 1 voters where k voters have preference order acb and k − 1 voters have preference order bca. The Plurality scores of a, b, and c are, respectively, k, k − 1, and 0. Thus, the nearest ranking should be abc. However, it is impossible to set B, C, S, and T to ensure this, while keeping d a metric. To see this, note that d(V, abc) = kT + (k − 1)S and d(V, acb) = (k − 1)C. We would have to have T + (k − 1)T + (k − 1)S < (k − 1)C. Yet, this is impossible, because by triangle inequality we have d(acb, abc)+d(abc, bca) ≥ d(acb, bca), that is, T +S ≥ C. Borda. Like all scoring rules, the Borda rule is an SRSF. Further, it is known to be DR with respect to unanimity consensus U via an additively votewise distance  (Meskanen and Nurmi 2008) <ref id=#b8> . In fact, the distance used in this construction is just the distance d s that rationalizes the Kemeny rule with respect to S, i.e., Borda and Kemeny are rationalized via the same distance, but with respect to different consensus classes. However, the SRSF for Borda is not a distance. Our next result explains why this is the case.",
                "Theorem 2. For three candidates, Borda is not DR with respect to S via a neutral additively votewise distance.",
                "Proof. Suppose that Borda is additively votewise DR with respect to S via a distance d on votes given by T , C, B, and S, and consider two families of preference profiles, V 1 (k) and V 2 (k), where k > 0. V 1 (k) and V 2 (k) both contain k voters with preference order acb, k voters with preference order bca, and one extra voter. In the case of V 1 (k) this extra voter has preference order cab, and in the case of V 2 (k), the extra voter has preference acb. We have",
                "<formula_19>",
                "Further, for each k > 0, the unique winner of V 1 (k) is c, and the unique winner of V 2 (k) is a. Therefore, for any k > 0 it holds that d 1 < d 3 , and",
                "<formula_20>",
                "that is, in particular, for each k we have kB +kS −B < kC. On the other hand, for V 2 (k) we have:",
                "<formula_21>",
                "By triangle inequality, T +S ≥ C, so",
                "<formula_22>",
                "In particular, for each k > 0 it holds that kC < kB + kS + B. By combining this inequality with the previous one, we get that for each k > 0 it holds that B + S − B k < C < B + S + B k . Since k can be arbitrarily large k, we have C = B + S. Now, consider a preference profile V 3 = (abc, acb). Clearly, a is the unique Borda winner for V 3 . We have",
                "<formula_23>",
                "acb) = T , and",
                "<formula_24>",
                "Since a is the unique winner, it holds that T < S + B. In particular, T < B. However, by triangle inequality we know that T + S ≥ C. We also know that C = B + S, so we have T + S ≥ B + S, that is, T ≥ B. This is a contradiction.",
                "Single Transferable Vote.  Conitzer, Rognlie, and Xia (2009) <ref id=#b2>  have shown that STV is not MLE. By our observations regarding the relationship between MLE and DR, this implies that STV cannot be rationalized with respect to S via an additively votewise distance. We will now strengthen this result to (almost) arbitrary votewise distances. We need the following definition. Definition 1  (Bauer, Stoer, and Witzgall 1961) <ref id=#b0> . A norm N in R n is monotonic in the positive orthant, or R n + -monotonic, if for any two vectors (x 1 , . . . , x n ), (y 1 , . . . , y n ) ∈ R n + such that x i ≤ y i for all i = 1, . . . , n we have N (x 1 , . . . , x n ) ≤ N (y 1 , . . . , y n ).  Bauer, Stoer, and Witzgall (1961) <ref id=#b0>  provide a discussion of norms that are monotonic in the positive orthant. We remark that this is a fairly weak notion of monotonicity: the class of R n + -monotonic norms strictly contains the class of all monotonic norms (as defined in  (Bauer, Stoer, and Witzgall 1961) <ref id=#b0> ). The requirement of R n + -monotonicity is very natural when the norm in question is to be used to construct a product metric, as in our case. We say that a votewise distance is monotonic if the respective norm is monotonic in the positive orthant. Theorem 3. For three candidates, STV (together with any intermediate tie-breaking rule) is not distancerationalizable with respect to the strong unanimity and any neutral anonymous monotonic votewise distance.",
                "Proof. For the sake of contradiction, suppose that STV can be rationalized with respect to S via a neutral anonymous monotonic votewise distance d, and let N denote the corresponding norm. Consider a profile V with 2k + 1 voters, k ≥ 2, where the voters' preferences are as follows: k voters have preferences given by abc, k voters have preferences given by bca, one voter has preferences given by cab. We have",
                "<formula_25>",
                ". . , B, T, . . . , T, C). Clearly, under STV candidate a is the unique winner in V . Thus, it must be the case that min{d 1 , d 2 } < min{d 3 , d 4 }. By symmetry we have d 1 = d 3 , and hence d 2 < d 4 . Also, by symmetry we get d 4 = N (T, . . . , T, C, B, . . . , B). Hence, by monotonicity C < B.",
                "Now, consider the profile W obtained by replacing the last voter in V by a voter whose preferences are cba. We have In W , the STV-winner is b, so we have min{d 1 , d 2 } > min{d 3 , d 4 }. Furthermore, by symmetry, we have d 3 = N (0, . . . , 0, S, . . . , S, B). As C < B, by monotonicity we conclude that d 1 ≤ d 3 . This implies that d 2 > d 4 . However, by symmetry we have d 4 = N (T, . . . , T, B, . . . , B, S), so by monotonicity d 2 ≤ d 4 , a contradiction.",
                "We can use similar ideas to show that STV is not distancerationalizable with respect to U and a neutral anonymous monotonic votewise distance. Theorem 4. For three candidates, STV (together with any intermediate tie-breaking rule) is not distancerationalizable with respect to weak unanimity and any neutral anonymous monotonic votewise distance.",
                "Finally, we remark that STV is not DR with respect to C since it is not Condorcet-consistent. Note that  Meskanen and Nurmi (2008) <ref id=#b8>  show that STV can be distance-rationalized with respect to U. Their distance is neutral, but not votewise (note that the notions of anonymity and monotonicity are only defined for votewise distances, so they are not applicable here). Further, it is not immediately clear if this distance is polynomial-time computable. Thus, of all rules we have considered, STV is DR in the weakest possible sense."
            ],
            [
                "Maximum likelihood estimation and distance rationalizability provide two related, but distinct approaches to understanding voting rules. In this paper, we have classified several prominent voting rules according to how well they can be explained in each of these frameworks. According to this criterion, the best voting rule is Kemeny rule, which can be shown to fit both the MLE framework and the DR framework via the same underlying function. Plurality is a close second: it is both an SRSF and DR via a \"good\" distance with respect to our strongest consensus class S (as well as a weaker consensus U); however, we show that one cannot use the same function to prove both of these results. Borda rule, too, is both an SRSF and additively votewise DR, but only with respect to a weaker consensus class U. Finally, STV fails this test completely: we show that it cannot be rationalized via a votewise distance with respect to any of the standard consensus classes. Thus, our approach provides a more refined classification of voting rules than either MLE or DR alone."
            ]
        ],
        "Tables": [
            {
                "id": "tab_0",
                "Head": "Table 1 :",
                "Label": "1",
                "Description": "The values of d for each pair of votes over C. of d for each pair of preference orders. 2 Since d is a distance, we have T, C, B, S > 0. For a collection"
            },
            {
                "id": "tab_1",
                "Head": "",
                "Label": "",
                "Description": "The second component of this expression does not depend on i, while its first component counts the number of voters that do not rank c i first. Thus, the nearest strong unanimity consensus to V has c i as a winner if and only if i minimizes the sum j∈[m]\\{i} r∈[s] a r"
            }
        ],
        "Figures": [
            {
                "id": "fig_0",
                "Head": "",
                "Label": "",
                "Description": "Further, we have d(abc, abc) = 0, and by neutrality and symmetry we have d(abc, bca) = d(abc, cab) (to see this, note that the permutation π given by π(a) = c, π(b) = a, π(c) = b transforms abc into cab and bca into abc). Set d(abc, acb) = T , d(abc, bac) = B, d(abc, cba) = C, d(abc, bca) = d(abc, cab) = S."
            },
            {
                "id": "fig_1",
                "Head": "2",
                "Label": "",
                "Description": "If a vote u is obtained from a vote v by permuting the second and the third candidate (and leaving the top candidate in place), we have d(u, v) = T , if u is obtained from v by permuting the first and the second candidate (and leaving the bottom candidate in place), we have d(u, v) = B, if u is obtained from v by permuting the first and the third candidate (and leaving the center in place), we have d(u, v) = C, and if u is obtained from v by a cyclic shift, we have d(u, v) = S."
            },
            {
                "id": "fig_2",
                "Head": "d 1",
                "Label": "1",
                "Description": "= d(W, abc) = N (0, . . . , 0, S, . . . , S, C), d 2 = d(W, acb) = N (T, . . . , T, C, . . . , C, S), d 3 = d(W, bca) = N (S, . . . , S, 0, . . . , 0, B), d 4 = d(W, bac) = N (B, . . . , B, T, . . . , T, S)."
            }
        ],
        "References": [
            {
                "id": "b0",
                "Title": "Absolute and monotonic norms",
                "Authors": [
                    "F Bauer",
                    "J Stoer",
                    "C Witzgall"
                ],
                "Journal": "Numerische Matematik"
            },
            {
                "id": "b1",
                "Title": "Common voting rules as maximum likelihood estimators",
                "Authors": [
                    "V Conitzer",
                    "T Sandholm"
                ]
            },
            {
                "id": "b2",
                "Title": "Preference functions that score rankings and maximum likelihood estimation",
                "Authors": [
                    "V Conitzer",
                    "M Rognlie",
                    "L Xia"
                ]
            },
            {
                "id": "b3",
                "Title": "Rank aggregation methods for the web",
                "Authors": [
                    "C Dwork",
                    "R Kumar",
                    "M Naor",
                    "D Sivakumar"
                ]
            },
            {
                "id": "b4",
                "Title": "On distance rationalizability of some voting rules",
                "Authors": [
                    "E Elkind",
                    "P Faliszewski",
                    "A Slinko"
                ]
            },
            {
                "id": "b5",
                "Title": "On the role of distances in defining voting rules",
                "Authors": [
                    "E Elkind",
                    "P Faliszewski",
                    "A Slinko"
                ]
            },
            {
                "id": "b6",
                "Title": "A heuristic technique for multi-agent planning",
                "Authors": [
                    "E Ephrati",
                    "J Rosenschein"
                ],
                "Journal": "Annals of Mathematics and Artificial Intelligence"
            },
            {
                "id": "b7",
                "Title": "Voting for movies: The anatomy of recommender systems",
                "Authors": [
                    "S Ghosh",
                    "M Mundhe",
                    "K Hernandez",
                    "S Sen"
                ]
            },
            {
                "id": "b9",
                "Title": "A consistent extension of Condorcet's election principle",
                "Authors": [
                    "H Young",
                    "A Levenglick"
                ],
                "Journal": "SIAM Journal on Applied Mathematics"
            },
            {
                "id": "b10",
                "Title": "Condorcet's theory of voting",
                "Authors": [
                    "H Young"
                ],
                "Journal": "American Political Science Review"
            },
            {
                "id": "b11",
                "Title": "Optimal voting rules",
                "Authors": [
                    "H Young"
                ],
                "Journal": "Journal of Economic Perspectives"
            }
        ]
    },
    "temporal planning for interacting durative actions with continuous effects": {
        "Title": "temporal planning for interacting durative actions with continuous effects",
        "Authors": [
            "Serdar Kecici",
            "Sanem Talay"
        ],
        "Abstract": "",
        "Keywords": [],
        "BookMarks": [
            "Introduction",
            "Problem Statement",
            "Lifted Actions",
            "Schedule (apply) an action If",
            "Conclusion"
        ],
        "Papertext": [
            [
                "One of the most important properties of temporal planners is their capability of integrating planning into scheduling so that time can be considered as an optimization objective. Forward chaining temporal planners provides tight coupling of planning and scheduling. In these planners, a search node is represented by a world state including the applied action directly attached with a world clock which defines starting time of that action. The search proceeds by applying a new action or advancing the world clock which moves the time forward. To avoid infinite branching factor, forward chaining temporal planners can set a world clock only at certain points in time called decision epochs for which an at-end effect of a previously applied action is scheduled. Since all necessary timestamps in continuous time line are not covered, the resulting plan may be suboptimal for planning problems that include both discrete effects and time-dependent continuous linear effects. Continuous linear effects occur especially when agents share time-dependent critical resources. In these cases, besides discrete and continuous changes, their interactions should also be taken into consideration.",
                "Some of the earlier studies have investigated planning domains including actions with continuous linear effects. TM-LPSAT  (Shin & Davis 2005) <ref id=#b3>  is an extension to SATbased planning framework to handle concurrent actions with continuous change. COLIN  (Coles et al. 2009) <ref id=#b1>  is another effective planner which plans with mixed discrete continuous numeric changes. Both of these planners support PDDL+  (Fox & Long 2006) <ref>  language which provides representation of continuous process effects in planning domains. However, the focus of both systems is to handle increase and decrease in the value of singular variables. Neither of the studies considers interactions among continuous effects of different actions in planning domain.",
                "This paper presents an extension to forward chaining temporal planning to handle continuous linear effects of actions and interactions among them. The main motivation is providing completeness considering these effects.",
                "We propose an action lifting approach in order to ensure completeness in domains with interacting linear continuous 10, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. effects. VHPOP system  (Younes & Simmons 2003) <ref id=#b4>  as a partial order causal link planner also applies lifted actions to reduce branching factor of their algorithm.  (Cushing et al. 2007) <ref id=#b2>  proposes lifting states over time in order to provide completeness for domains including actions with atend conditions. However both of them cannot handle continuous effects of actions.",
                "We implemented our algorithm as an extension to the TLPLAN planning system  (Bacchus & Ady 2001) <ref id=#b0> , a progression temporal planner which handles durative concurrent actions and metric quantities."
            ],
            [
                "We analyze the path sharing problem to illustrate interaction of continuous linear effects in the planning domain. In addition to discrete changes, this problem presents further relations such as interactions of continuous changes. In a path sharing problem, such interaction appears when agents move on the path simultaneously.",
                "Multiple agents, defined in the planning domain, carry out several tasks in order to reach the goal state of the planning problem. There are several rooms connected with a shared narrow corridor which is a one-lane shared path.  Figure 1 <figure>  shows the problem domain. Agents could move on the path at predefined speed unless they collide with any other agent concurrently moving on the shared path.",
                "Critical resource that agents must share in our domain is the shared path of which availability cannot be represented by a singular predicate, but by several allocations. An allocation is a linear continuous effect of a move action which is formulated by 4 values: speed of the agent, entrance position and exit positions on the path and start-time of the movement. As simultaneous movement of agents on the path is possible, there may be multiple concurrent allocations to represent the availability of the critical resource."
            ],
            [
                "Temporal planners do not consider continuous time, but plans with limited set of timestamps called decision epochs. This set of decision epochs includes the timestamps where world state is modified by at-end effect of an action. Although this usage is adequate for domains in-volving actions with ordinary preconditions, it is not sufficient for complex actions which are related with continuous changes on shared critical resources (e.g., shared path) especially when the metric to minimize is the total duration of the plan.",
                "We illustrate this incompleteness with a simple problem instance. Initially agent 1 is in room 3 , and agent 2 is in room 1 . The goal is defined as agent 1 in room 0 and agent 2 in room 3 .  Figure 2 <figure id=#fig_0>  illustrates the solution scenario for this simple problem.",
                "In this problem, after applying the move action of agent 1 , the search process will fail to match preconditions of move action for agent 2 at t 0 because of the collision of requested allocation with the allocation already set by the already scheduled movement of agent 1 . So advance-worldclock action is applied. As classical forward chaining mechanism considers only the set of decision epochs, this action will advance the world clock to t 2 . Although an allocation for the movement of agent 2 starting at t 1 does not collide with any other allocation, it is missed by the planner since t 1 is not a decision epoch. Therefore, the planner cannot find the makespan optimal solution. It finds a sub-optimal plan in which the movement of agent 2 is applied at t 2 instead of t 1 . This is the strategy performed by TLPLAN. To overcome this problem, we extend the planner to broaden its search by applying lifted actions in addition to standard actions. Lifting is decision of an action application without scheduling it to a certain time point. The actual start time of a lifted action is left open until the next decision epoch where the lifted action may be grounded (scheduled) to an intermediate time point.",
                "There are 4 successor generators for the planner while expanding a state s."
            ],
            [
                "Lift (apply without scheduling) an action if : if L and This extended branching scheme reveals complete and optimal solutions for the given scenario. As it is illustrated in figure 2, it is possible for the planner to lift the move action of agent 2 at t 0 , so that it might be grounded (scheduled) to t 1 after advancing the world clock to t 2 ."
            ],
            [
                "In this paper, we introduce an extension to forward chaining planning in order to solve planning domains with continuous linear change (e.g., domains with a shared path). Such domains may involve mixed discrete continuous changes and interactions among them. We show that lifting can be adapted to handle continuous change in planning domains so that optimal solutions in terms of makespan minimization are provided. However the drawback of this approach is the increased computational complexity due to the extended branching factor. Special pruning techniques for temporally identical states and use of heuristics for makespan estimation will be investigated in future studies to reduce complexity."
            ]
        ],
        "Tables": [],
        "Figures": [
            {
                "id": "fig_0",
                "Head": "Figure 2 :",
                "Label": "2",
                "Description": "Figure 2: Solution for a sample problem with two agents"
            }
        ],
        "References": [
            {
                "id": "b0",
                "Title": "Planning with resources and concurrency: A forward chaining approach",
                "Authors": [
                    "F Bacchus",
                    "M Ady"
                ]
            },
            {
                "id": "b1",
                "Title": "Temporal planning in domains with linear processes",
                "Authors": [
                    "A Coles",
                    "A Coles",
                    "M Fox",
                    "D Long"
                ]
            },
            {
                "id": "b2",
                "Title": "Modelling mixed discretecontinuous domains for planning",
                "Authors": [
                    "W Cushing",
                    "S Kambhampati",
                    "M Mausam",
                    "D Weld",
                    "D Long"
                ]
            },
            {
                "id": "b3",
                "Title": "Processes and continuous change in a SAT-based planner",
                "Authors": [
                    "J.-A Shin",
                    "Davis",
                    "E"
                ],
                "Journal": "Artificial Intelligence"
            },
            {
                "id": "b4",
                "Title": "Vhpop: Versatile heuristic partial order planner",
                "Authors": [
                    "H Younes",
                    "R Simmons"
                ],
                "Journal": "JAIR"
            }
        ]
    },
    "bayes-adaptive interactive pomdps": {
        "Title": "bayes-adaptive interactive pomdps",
        "Authors": [
            "Brenda Ng",
            "Kofi Boakye",
            "Carol Meyers",
            "Andrew Wang"
        ],
        "Abstract": "We introduce the Bayes-Adaptive Interactive Partially Observable Markov Decision Process (BA-IPOMDP), the first multiagent decision model that explicitly incorporates model learning. As in I-POMDPs, the BA-IPOMDP agent maintains beliefs over interactive states, which include the physical states as well as the other agents' models. The BA-IPOMDP assumes that the state transition and observation probabilities are unknown, and augments the interactive states to include these parameters. Beliefs are maintained over this augmented interactive state space. This (necessary) state expansion exacerbates the curse of dimensionality, especially since each I-POMDP belief update is already a recursive procedure (because an agent invokes belief updates from other agents' perspectives as part of its own belief update, in order to anticipate other agents' actions). We extend the interactive particle filter to perform approximate belief update on BA-IPOMDPs. We present our findings on the multiagent Tiger problem.",
        "Keywords": [],
        "BookMarks": [
            "1 Introduction",
            "2 Preliminaries",
            "2.1 Bayes-Adaptive POMDPs (BA-POMDPs)",
            "2.2 Interactive POMDPs (I-POMDPs)",
            "3 Bayes-Adaptive I-POMDPs",
            "4 Solving BA-IPOMDPs",
            "4.1 Bayes Adaptive Interactive Particle Filter",
            "4.2 Reachability Tree Sampling",
            "5 Empirical Results",
            "5.1 Analysis of BA-IPF",
            "5.2 Analysis of Observation Parameter Learning",
            "5.3 Analysis of Joint Parameter Learning",
            "6 Discussion"
        ],
        "Papertext": [
            [
                "Within the last decade, the body of work in multiagent sequential decision-making methods has grown substantially, both in terms of theory and practical feasibility. For these methods to be applicable in real-world settings, we must account for the fact that agents usually lack perfect knowledge about their environments, with regard to (1) the state of the world, and (2) the consequences of their interactions, in terms of model parameters such as transition and observation probabilities. Thus, agents must infer their state from the history of actions and observations, and concurrently learn their model parameters via trial and error. The development of such a framework is the objective of this paper.",
                "Our particular focus is on adversarial agents that are intelligent, and actively seek to \"game\" against each other during the course of repeated interactions. In such a setting, each agent must anticipate its adversary's responses to its actions, which entails also anticipating the adversary's observations and beliefs about the state of the world. We feel that this type of study is highly relevant to realistic security problems, since these intelligent agents abound in the form of cyber intruders, money launderers, material smugglers, etc. While each \"attack\" might be launched by one individual, it is reasonable to treat an entire class of attackers as a single adversary, as similar tactics are adopted by multiple individuals to exploit the vulnerabilities of the target agent.",
                "Many sequential multiagent decision-making frameworks are extensions of the single-agent Partially Observable Markov Decision Process (POMDP) model. In a POMDP, a single agent with limited knowledge of its environment attempts to optimize a discrete sequence of actions to maximize its expected rewards. Because the agent does not fully know the state of the environment, it infers a state distribution through a series of noisy observations. Solution algorithms for POMDPs have been studied extensively  (Kaelbling, Littman, and Cassandra 1998) <ref id=#b12> , and POMDPs have been applied to real-world problems including the assistance of patients with dementia  (Hoey et al. 2007) <ref id=#b11> .",
                "Among the multiagent frameworks that have been studied, the largest body of literature has been on decentralized POMDPs (DEC-POMDPs)  (Bernstein et al. 2002) <ref id=#b0> , which generalize POMDPs to multiple decentralized agents and are used to model multiagent teams  (Seuken and Zilberstein 2008) <ref id=#b18> . While algorithms have been developed to solve such problems  (Seuken and Zilberstein 2008) <ref id=#b18> , DEC-POMDPs are not suitable for modeling adversarial agents because the framework assumes common rewards among agents. The related framework of Markov Team Decision Problems (MTDPs) (Pynadath and Tambe 2002) has the same issue. Partially Observable Stochastic Games (POSGs)  (Hansen, Bernstein, and Zilberstein 2004) <ref id=#b10>  avoid this problem by allowing for different agent rewards, but exact POSG algorithms have been so far limited to small problems  (Guo and Lesser 2006) <ref id=#b9> , and approximate POSG algorithms have only been developed for the common rewards case  (Emery-Montemerlo et al. 2004) <ref id=#b8> .",
                "A suitable framework for modeling multiagent adversarial interactions is that of interactive POMDPs (I-POMDPs)  (Doshi and Gmytrasiewicz 2005) <ref id=#b2> . The I-POMDP is a multiagent extension of the POMDP, in which each agent maintains beliefs about both the physical states of the world and the decision process models of the other agents. An I-POMDP incorporates nested intent into agent beliefs, which potentially allows for modeling of \"gaming\" agents. There are approximate algorithms for solving I-POMDPs that do not impose common agent rewards.  (Ng et al. 2010) <ref id=#b15>  demonstrates an attempt to apply I-POMDPs to money laundering.",
                "Although I-POMDPs can be used to model adversarial agents, they are not amenable to real-world applications because transition and observation probabilities need to be specified as part of the model. In most cases, these parameters are not known exactly, and must be approximated a priori or learned during the interaction. Reinforcement learning  (Kaelbling, Littman, and Moore 1996) <ref id=#b13>  provides a methodology by which these parameters may be estimated sequentially, thus avoiding potentially non-optimal solutions associated with poor a priori approximations.",
                "Bayes-Adaptive POMDPs (BA-POMDPs)  (Ross, Chaibdraa, and Pineau 2007) <ref>  enable parameter learning in POMDPs. In a BA-POMDP, the agent's state is augmented to include the agent's counts of state transitions and observations, and these counts are used to estimate parameters in the transition and observation functions. Parameter estimates are improved through interactions with the environment, and optimal actions converge over time to the true optimal solution.",
                "To date, work in multiagent learning has been focused mainly on fully observable domains  (Busoniu, Babuska, and Schutter 2008; <ref id=#b1> Melo and Ribeiro 2010) <ref id=#b14>  or cooperative, partially observable domains  (Peshkin et al. 2000; <ref id=#b16> Zhang and Lesser 2011; <ref id=#b19> Oliehoek 2012) <ref id=#b16> , where policy learning is emphasized over model learning.",
                "The contribution of this work is the Bayes-Adaptive I-POMDP (BA-IPOMDP), that incorporates elements of I-POMDPs and BA-POMDPs, to achieve the first multiagent adversarial learning framework that explicitly learns model parameters. The BA-IPOMDP allows for imperfect knowledge of both the world state and the agents' transition and observation probabilities, thus bringing theory closer to human agent modeling. Our preliminary results show that the BA-IPOMDP learning agent achieves better rewards than the I-POMDP static agent, when the two start from the same prior model. We cover technical background in Section 2, explain our BA-IPOMDP model in Section 3 and our belief update algorithm in Section 4. We present results in Section 5 and conclude in Section 6."
            ],
            [],
            [
                "In a BA-POMDP  (Ross, Chaib-draa, and Pineau 2007) <ref id=#b17> , the state, action, and observation spaces are assumed to be finite and known, but the state transition and observation probabilities are unknown and must be inferred. It extends the POMDP S, A, T, R, Z, O , by allowing uncertainty to be associated with T (s, a, s ) and O(s , a, z).",
                "The uncertainties are parametrized by Dirichlet distributions defined over experience counts. The count φ a ss denotes the number of times that transition (s, a, s ) has occurred, and the count ψ a s z denotes the number of times observation z was made in state s after performing action a. Given these counts, the expected transition probabilities T φ (s, a, s ) and the expected observation probabilities O ψ (s ,a, z) are:",
                "<formula_0>",
                "The BA-POMDP incorporates the count vectors φ and ψ as part of the state, so functions need to be augmented accordingly to capture the evolution of these vectors. Let δ a ss be a vector of zeros with a 1 for the count φ a ss , and δ a s z be a vector of zeros with a 1 for the count ψ a s z . Formally, a BA-POMDP is parametrized by S , A, T , R , Z, O , where the differences from a POMDP are:",
                "• S = S × T × O is the augmented state space, where",
                "<formula_1>",
                "if φ = φ+δ a ss and ψ = ψ+δ a s z , and 0 otherwise;",
                "<formula_2>",
                "δ a ss and ψ = ψ + δ a s z , and 0 otherwise. The belief update in BA-POMDPs is analogous to that of POMDPs, but inference is performed over a larger state space, as beliefs are maintained over the  (unobserved) <ref>  counts, φ and ψ, in addition to the physical states. Monte Carlo sampling along with online look-ahead search have been applied to solve BA-POMDPs."
            ],
            [
                "I-POMDPs  (Doshi and Gmytrasiewicz 2005) <ref id=#b2>  generalize POMDPs to multiple agents with different (and possibly conflicting) objectives. In an I-POMDP, beliefs are maintained over interactive states, which include the physical states and the models of other agents' behaviors.",
                "In the case of two intentional agents, i and j, agent i's I-POMDP with l levels of nesting is specified by:",
                "<formula_3>",
                "• Z i is the finite set of i's observations; and",
                "<formula_4>",
                "At each time step, agent i maintains a belief state:",
                "<formula_5>",
                "(3) where β is a normalizing factor and P (a t−1 j |θ t−1 j,l−1 ) is the probability that a t−1 j is Bayes rational for an agent modeled by θ t−1 j,l−1 . Let OP T (θ j ) denote the set of j's optimal actions computed from a planning algorithm that maximizes rewards accrued over an infinite horizon (i.e., E(",
                "<formula_6>",
                "where 0 < γ < 1 is the discount factor and r t is the reward achieved at time t. P (a t−1 j |θ t−1 j,l−1 ) is set to",
                "<formula_7>",
                "j,l−1 ) and 0 otherwise. The belief update procedure for I-POMDPs is more complicated than that of POMDPs, because physical state transitions depend on both agents' actions. To predict the next physical state, i must update its beliefs about j's behavior based on its anticipation of how j updates its belief (note",
                "<formula_8>",
                "in the belief update equation). This can lead to infinite nesting of beliefs, for which a finite level of nesting l is imposed in practice.  (Doshi et al. 2010) <ref id=#b5>  has shown that most humans act using up to two levels of reasoning in general-sum strategic games, suggesting the nesting level of l = 2 as an upper bound for modeling human agents.",
                "A popular method for solving I-POMDPs is the interactive particle filter (I-PF) in conjunction with reachability tree sampling (RTS). Other methods include dynamic influence diagrams  (Doshi, Zeng, and Chen 2009) <ref id=#b7>  and generalized point-based value iteration  (Doshi and Perez 2008) <ref id=#b4> ."
            ],
            [
                "While other cooperative multiagent frameworks can exploit common rewards to simplify the learning problem into multiple parallel instances of single-agent learning, the learning process in an adversarial multiagent framework is intrinsically more coupled. This is because each agent can no longer rely on its own model as a baseline for modeling others; each agent is less informed about the other (adversarial) agents because agents have different rewards and potentially different dynamics stemming from their own actions and observations.",
                "Since state transitions depend on joint actions from all agents, it is imperative that the adversarial agent consider other agents' perspectives before taking action. The I-POMDP offers a vehicle for recursive modeling to address this, which the BA-IPOMDP augments with the additional capability for learning. Thus, the BA-IPOMDP can model learning about self, learning about other agents, and learning about other agents learning about self, etc.",
                "Like the BA-POMDP, the BA-IPOMDP assumes that the state, action, and observation spaces are finite and known a priori. Each agent is trying to learn a |S| × |A| × |S| matrix T of state transition probabilities, where T (s t−1 , a t−1 , s t ) = P (s t |s t−1 , a t−1 ), and a |S| × |A| × |Z i | matrix O of observation probabilities, where O(s t , a t , z t i ) = P (z t i |s t , a t ). Each agent's physical state is augmented to include the transition counts and all agents' observation counts. We denote this state as s = (s, φ, ψ i , ψ j ) ∈ S = S×T ×O i ×O j , where s is the physical state, φ is the transition counts (over S), ψ i is agent i's observation counts (over Z i ), and ψ j is agent j's observation counts (over Z j ). Note that this does not require access to other agents' observations. We treat the other agent's observations like the physical states, as partially observable and maintain beliefs over them. (If each agent only maintained its individual observation counts, there would be insufficient information to infer the joint transition function.) Given φ, ψ i , and ψ j , the expected prob-",
                "<formula_9>",
                "are defined similarly as in BA-POMDPs (cf. Equations (1) and (2)).",
                "We construct the BA-",
                "<formula_10>",
                ", and its knowledge of agent j's model, θ t j,l−1 = {b t j,l−1 ,θ j }, which consists of j's belief b t j,l−1 and frameθ",
                "<formula_11>",
                "are each a vector of zeros with a 1 for the counts that correspond to the \"s t−1 → s t \" transition and the \"s t → z t k \" observation respectively. The conditions [φ] and [ψ k ] ensure that φ and ψ k are properly incremented by the state transition or observation that manifests after each action. Subsequently:",
                "<formula_12>",
                "hold, and 0 otherwise. As in I-POMDPs, the joint transition function T can differ between agents, reflecting different degrees of knowledge about the environment. In contrast, the individual observation function O is deterministic; it is parametrized by the previous and current augmented states. Lastly, the individual reward function is",
                "<formula_13>",
                ". At each time step, agent i maintains beliefs over the augmented interactive states, which include agent j's beliefs (b t j,l−1 ) and frame (θ j ). As part of the l-th level belief update, i invokes j's (l − 1)-th level belief update:",
                "<formula_14>",
                "inducing recursion. The recursion ends at l = 0 when the BA-IPOMDP reduces to a BA-POMDP. Unlike the usual I-POMDP assumption of static frames, the BA-IPOMDP can estimate components of these frames, so they need not be completely fixed. Theorem 1. The belief update for the BA-IPOMDP",
                "<formula_15>",
                "where * * represents the set of interactive states is",
                "<formula_16>",
                "Proof. (Sketch) Start with Bayes' theorem and expand:",
                "<formula_17>",
                "From is t i,l and is t−1 i,l , the count vectors,",
                "<formula_18>",
                ", determine the agents' observations (thus eliminating the need to marginalize over z t j as is traditionally done in the I-POMDP belief update procedure).",
                "Under",
                "<formula_19>",
                ", the following terms simplify:",
                "<formula_20>",
                "Furthermore, T i (s t−1 , a t−1 , s t ) can be simplified by Equation  4 <formula> , where the constituent expected probabilities T φ and O ψ k can be computed from the counts in the same fashion as Equations (1) and (2)."
            ],
            [
                "For a problem with |S| physical states and |Z| observations, an I-POMDP formulation for two symmetric agents will contain up to |S| 2 (non-interactive) states, and a singleagent BA-POMDP formulation will contain |S| |z|+|Z|−1 |Z|−1 possible augmented states, where |z| is the total number of observations received during the episode. The corresponding (one-level) BA-IPOMDP formulation will contain up to |S| 2 |zi|+|Zi|−1 |Zi|−1 2 augmented (non-interactive) states, which is exponentially larger than either of the previous quantities. Thus, the extension from either BA-POMDPs or I-POMDPs to BA-IPOMDPs is nontrivial."
            ],
            [
                "To perform inference on BA-IPOMDPs, we approximate the belief as a set of samples over the augmented interactive states, and update the agent's beliefs via an extension of the interactive particle filter (I-PF)  (Doshi and Gmytrasiewicz 2005) <ref id=#b2> . Our algorithm, the Bayes-Adaptive interactive particle filter (BA-IPF), is presented in  Figure 1 <figure> . Each sample is a possible interactive state. For each sample, an approximate value iteration algorithm, using a sparse reachability tree (to be explained in the next subsection), is applied to compute the set of approximately optimal actions for the opposing agent. Each action in this set is then uniformly weighted so each of these actions is equally likely to be sampled. Then, enumerating over physical states from the next time step, we sample the opposing agent's action and, for all possible opposing agent's observations, we apply this action to update the model of the opposing agent and the counts in the sample. As part of updating the opposing agent's model, which includes its belief, the procedure recurses until level one is reached, when the standard BA-POMDP update is invoked instead. After the samples are propagated forward in time as prescribed, we weigh the samples and normalize them so their weights sum to one. Lastly, the samples are resampled with replacement to avoid sample degeneracy.",
                "<formula_21>",
                "for all s t ∈ S do 5:",
                "<formula_22>",
                "11:",
                "<formula_23>",
                "end for 19: end for 20: end for 21: Normalize all w",
                "<formula_24>",
                "Selection 22: Resample with replacement N particles from the set b tmp k,l according to the importance weights; store these (unweighted) samples as is  Figure 1 <figure> : The BA-IPF algorithm for approximate BA-IPOMDP belief update. n is the particle index and k is the agent index. If k denotes i, then −k denotes j, and vice versa.",
                "<formula_25>",
                "Compared to I-PF, the BA-IPF consists of additional steps to update the counts  (Lines 10 and 14) <figure> . We have also chosen to enumerate the physical states (Line 4) instead of sampling, to achieve higher accuracy for our test problem where the small number of physical states allowed this to be feasible. As a result, our samples of interactive states are weighted by the BA-IPOMDP transition function (Line 16) instead of the uniform weighting suggested in  (Ross, Chaibdraa, and Pineau 2007) <ref>  in which physical states were sampled rather than enumerated."
            ],
            [
                "While I-PF addresses the curse of dimensionality due to the complexity of the belief state, the curse of history can also be problematic, because the search space for policies increases with the horizon length. To perform this policy search, a reachability tree is constructed, and with increasing horizons, this tree grows exponentially to account for every possible sequence of actions and observations. To address this issue,  (Doshi and Gmytrasiewicz 2005) <ref id=#b2>  proposed reachability tree sampling (RTS) as a way to reduce the tree branching factor. In RTS, observations are sampled according to z t k ∼ P (Z k |a t−1 k ,b t−1 k,l ) and a partial reachability tree is built based on the sampled observations and the complete set of actions.",
                "In solving BA-IPOMDPs, the curse of history requires approximations beyond the standard RTS to address an additional computational bottleneck: the construction of the opposing agent's reachability tree. In order for agent k to behave optimally, it must anticipate what action −k might take; thus, in solving for k's optimal policy, it must also construct −k's reachability tree and use it to find −k's optimal action. As the tree size grows as O((|A −k ||Z −k |) l ), it becomes large quickly. Consequently, we follow  (Ng et al. 2010) <ref id=#b15>  to prune the opposing agent's reachability tree in addition to the agent's reachability tree."
            ],
            [
                "In our evaluation, we are interested in (1) the effect of the approximate belief update from BA-IPF, and (2) the effect of learning. We applied the multiagent Tiger problem  (Doshi and Gmytrasiewicz 2009) <ref id=#b3>  to study these effects. In our experiments, we limit the nesting to one level and the planning horizon to two. For each scenario, we solved both agents as level-1 BA-IPOMDPs (since each models the other agent) independently and present results obtained from simulating their behaviors against each other. Our experiments were performed on a 2.53GHz dual quad core Intel Xeon processor with 24GB of RAM.",
                "In what follows, Sections 5.1 and 5.2 present results for learning the observation probabilities, which entails estimating the 12 observation probabilities associated with the joint action of Listen, Listen (six for TigerLeft and six for Tiger-Right). Section 5.3 briefly discusses the results for learning the transition probabilities concurrently. This involves estimating the 32 probabilities associated with either agent opening either door (16 for TigerLeft and 16 for TigerRight).    Table 2 <table> : Parameter learning scenarios. The uniform distribution is used as (1) the prior for when the agent is learning, and (2) the static incorrect parameter for when the agent is not learning."
            ],
            [
                "The quality of approximation in BA-IPF is parametrized by the number of particles.  Table 1 <table id=#tab_1>  shows, as a function of particle number, Agent 0's (1) average reward per episode;",
                "(2) KL divergence between the actual and estimated observation distributions at the end of each episode; and (3) average time for planning and execution per episode. The results are averaged over 200 simulations of 100 episodes each. In this scenario, Agent 1 is using the correct observation probabilities and both agents are using the correct observation probabilities for their respective opponent models. Hence, Agent 0 is only learning its own observation probabilities. The prior for Agent 0's observation probabilities is set to uniform. In general, as the number of particles increases, reward and time increase while KL divergence decreases. For subsequent experiments, the number of particles is set to 16."
            ],
            [
                "In a given simulation of the two-agent Tiger game, parameter learning can occur for each agent and/or the agent's model of its opponent. Furthermore, when an agent is not learning, parameter values can be set correctly to the actual values or incorrectly to the uniform distribution. We explored a variety of simulation scenarios and report on the select ones that show interesting trends (cf.  Table 2 <table> ). In each scenario, we have two agents, each of which could either be (1) not learning and using correct model parameters;",
                "(2) learning its own parameters while assuming correct or incorrect values for its opponent's parameters; or (3) learning both its own and the opponent's parameters. In these scenarios, learning takes place only over the observation probabilities, which uses the uniform distribution as the prior. One notable trend is that agents accrue less rewards when they are both learning compared to when only when one is learning. This is shown by comparing Scenario 1, in which only Agent 0 is learning, with Scenario 3, where both agents are learning.  Figure 2 <figure id=#fig_0>  shows the results for these two scenarios, averaged over 500 simulations of 100 episodes each. It also shows the \"baseline\" static performance, for when the   Figure 3 <figure> : Plots of agent rewards and observation/transition KL divergences for a case of biased doors. agents are not learning, but are using either the correct or incorrect observation probabilities. Each baseline is averaged over 50,000 simulations (here episodes have no significance as no learning occurs).",
                "For Scenario 1,  Figure 2(a) <figure id=#fig_0>  shows the learning agent-Agent 0-to be initially at a disadvantage, accruing smaller returns than its opponent. This gap in performance is narrowed over time, reflecting the positive effects of successful learning. This is further supported by  Figure 2(c) <figure id=#fig_0> , which reveals a dramatic reduction in KL divergence for the learning agent.",
                "For Scenario 3, when Agent 1 is also learning, it too yields improved rewards over time, as evidenced in  Figure 2(b) <figure id=#fig_0> . The average reward obtained for the two agents is less than the correct parameter baseline, but is still higher than the incorrect parameter baseline, showing benefits provided by the BA-IPOMDP over the I-POMDP with static incorrect parameters. When both agents are learning, each agent's learning success is reduced, as reflected in the KL divergence in  Figure 2(d) <figure id=#fig_0> . Only one agent's KL divergence is shown since both exhibited similar convergence. Nonetheless, as shown in the percentile plots, most of the simulations are still in close convergence to the correct parameters.  Figure 4 <figure>  summarizes the results for all five scenarios from  Table 2 <table> . First, we see that the boxplots confirm the trend in reward reductions evident in Scenarios 1 and 3. Interestingly, in the case of Scenario 2, this phenomenon manifests itself in a different way. Here, Agent 0 both learns and models its opponent as learning; Agent 1, in contrast, is using the correct parameters for both itself and its opponent model. The result is that Agent 0 experiences a substantial relative reward reduction. This highlights another interesting trend we have observed in our various scenario analyses: a \"stronger\" opponent model leads to higher rewards.",
                "This trend may be explained as follows. In Scenario 1, Agent 0's opponent model possesses the correct parameter values and thus represents a strong opponent. A strong op-ponent will typically pursue exploitation over exploration, adopting a more aggressive strategy. Thus, Agent 0 will utilize a exploitation-dominant strategy, and will consequently reap higher rewards on average. Contrast this with the \"weaker\" learning opponent model in Scenario 2, in which Agent 0 opts more for exploration, performing the listening action more frequently to improve parameter estimates (assuming its opponent is likely to do the same). The ultimate result is that Agent 0's average accrued reward is reduced in Scenario 2 compared to Scenario 1.",
                "The boxplot results for Scenarios 4 and 5 illustrate the effect of fixed incorrect parameters versus learned parameters for the opponent model. At least for Agent 0, the comparison between the two scenarios suggests that, for the specific choice of prior (i.e., uniform), an opponent model whose parameters are fixed incorrectly to the prior yields comparable rewards to an opponent model that starts with the prior and evolves with learning."
            ],
            [
                "Unfortunately, the multiagent Tiger problem is not very well-suited for analyzing the learning of transition probabilities, because the only state transition is the resetting of the tiger's location to the left or right door with equal probability, triggered by the door opening action. To investigate the impact of learning both transition and observation probabilities, we considered an alternative version of the Tiger problem, one with biased doors, in which the tiger resets to the left door with a probability of 0.75 and the right with a probability of 0.25.  Figure 3 <figure>  presents our results for Scenario 3. (Like before, only one agent's KL divergence is shown since both exhibited similar convergence.) Under this biased-door version of the problem, concurrent learning (of observation and transition probabilities) leads to rewards comparable to the correct parameter baseline. Compared to the learning agent in the unbiased multiagent Tiger problem, this learning agent  Figure 4 <figure> : Boxplots of rewards in an episode for each agent and across all 100 simulation episodes. Each boxplot pair represents one of the five scenarios in  Table 2. <table>  accrues higher rewards. However, the reduction in observation KL divergence is somewhat less than in the unbiased problem. The transition KL divergence also reflects the fact that this problem does not offer sufficient information to adequately learn the transition probabilities, since the true state is revealed only after a door is opened."
            ],
            [
                "Our results demonstrate that the BA-IPOMDP framework can successfully be used to model multiple agents with uncertainties about (1) the current state of the world, and (2) their associated transition and observation probabilities. In particular, we observe that the reward for agents employing learning strategies improves over time, suggesting that learning is beneficial (  Figure 2) <figure id=#fig_0> ; moreover, the speed of this convergence is heavily affected by whether one or more agents are learning. Generally, scenarios in which more learning takes place (see  Table 2 <table> ) take longer to converge than scenarios with less learning, and consequently the average rewards over the same timespan are lower  (Figure 4 <figure> ). This effect is also impacted by whether non-learning agents have a fixed and incorrect model of their opponent.",
                "Finally, we note that recent work  (Doshi-Velez 2009) <ref id=#b6>  has addressed learning in POMDP models where the state space itself is not fully known. This approach uses infinite hidden Markov models in learning the size of the state space. Incorporating such a framework within the BA-IPOMDP would produce more computational challenges, but it might have the benefit of broadening BA-IPOMDP's applicability."
            ]
        ],
        "Tables": [
            {
                "id": "tab_1",
                "Head": "Table 1 :",
                "Label": "1",
                "Description": "Comparison of average rewards, final KL divergence, and average overall time, for varying numbers of particles."
            }
        ],
        "Figures": [
            {
                "id": "fig_0",
                "Head": "Figure 2 :",
                "Label": "2",
                "Description": "Figure 2: Plots comparing agent rewards (Figures 2(a) and 2(b)) and KL divergences (Figures 2(c) and 2(d)) in Scenarios 1 and 3."
            }
        ],
        "References": [
            {
                "id": "b0",
                "Title": "The complexity of decentralized control of Markov decision processes",
                "Authors": [
                    "D Bernstein",
                    "R Givan",
                    "N Immerman",
                    "S Zilberstein"
                ],
                "Journal": "Mathematics of Operations Research"
            },
            {
                "id": "b1",
                "Title": "A comprehensive survey of multiagent reinforcement learning. Systems, Man, and Cybernetics, Part C: Applications and Reviews",
                "Authors": [
                    "L Busoniu",
                    "R Babuska",
                    "B Schutter"
                ],
                "Journal": "IEEE Transactions on"
            },
            {
                "id": "b2",
                "Title": "Approximating state estimation in multiagent settings using particle filters",
                "Authors": [
                    "P Doshi",
                    "P Gmytrasiewicz"
                ]
            },
            {
                "id": "b3",
                "Title": "Monte Carlo sampling methods for approximating Interactive POMDPs",
                "Authors": [
                    "P Doshi",
                    "P Gmytrasiewicz"
                ],
                "Journal": "Journal of AI Research"
            },
            {
                "id": "b4",
                "Title": "Generalized point-based value iteration for interactive POMDPs",
                "Authors": [
                    "P Doshi",
                    "D Perez"
                ]
            },
            {
                "id": "b5",
                "Title": "Modeling recursive reasoning by humans using empirically informed interactive POMDPs",
                "Authors": [
                    "P Doshi",
                    "X Qu",
                    "A Goodie",
                    "D Young"
                ]
            },
            {
                "id": "b6",
                "Title": "The infinite partially observable Markov decision process",
                "Authors": [
                    "F Doshi-Velez"
                ],
                "Journal": "Neural Information Processing Systems"
            },
            {
                "id": "b7",
                "Title": "Graphical models for interactive POMDPs: representations and solutions",
                "Authors": [
                    "P Doshi",
                    "Y Zeng",
                    "Q Chen"
                ],
                "Journal": "Autonomous Agents and Multi-Agent Systems"
            },
            {
                "id": "b8",
                "Title": "Approximate solutions for partially observable stochastic games with common payoffs",
                "Authors": [
                    "R Emery-Montemerlo",
                    "G Gordon",
                    "J Schneider",
                    "S Thrun"
                ]
            },
            {
                "id": "b9",
                "Title": "Stochastic planning for weakly coupled distributed agents",
                "Authors": [
                    "A Guo",
                    "V Lesser"
                ]
            },
            {
                "id": "b10",
                "Title": "Dynamic programming for partially observable stochastic games",
                "Authors": [
                    "E Hansen",
                    "D Bernstein",
                    "S Zilberstein"
                ]
            },
            {
                "id": "b11",
                "Title": "Assisting persons with dementia during handwashing using a partially observable Markov decision process",
                "Authors": [
                    "J Hoey",
                    "A Von Bertoldi",
                    "P Poupart",
                    "A Mihailidis"
                ]
            },
            {
                "id": "b12",
                "Title": "Planning and acting in partially observable stochastic domains",
                "Authors": [
                    "L Kaelbling",
                    "M Littman",
                    "Cassandra",
                    "A"
                ],
                "Journal": "Artificial Intelligence"
            },
            {
                "id": "b13",
                "Title": "Reinforcement learning: a survey",
                "Authors": [
                    "L Kaelbling",
                    "M Littman",
                    "A Moore"
                ],
                "Journal": "Journal of AI Research"
            },
            {
                "id": "b14",
                "Title": "Coordinated learning in multiagent MDPs with infinite state spaces",
                "Authors": [
                    "F Melo",
                    "I Ribeiro"
                ],
                "Journal": "Journal of Autonomous Agents and Multiagent Systems"
            },
            {
                "id": "b15",
                "Title": "Towards applying interactive POMDPs to real-world adversary modeling",
                "Authors": [
                    "B Ng",
                    "C Meyers",
                    "K Boakye",
                    "J Nitao"
                ]
            },
            {
                "id": "b16",
                "Title": "The communicative multiagent team decision problem: analyzing teamwork theories and models",
                "Authors": [
                    "F Oliehoek",
                    "L Peshkin",
                    "K.-E Kim",
                    "N Meuleau",
                    "L Kaelbling",
                    "M Tambe"
                ]
            },
            {
                "id": "b17",
                "Title": "Bayes-adaptive POMDPs",
                "Authors": [
                    "S Ross",
                    "B Chaib-Draa",
                    "J Pineau"
                ]
            },
            {
                "id": "b18",
                "Title": "Formal models and algorithms for decentralized decision making under uncertainty",
                "Authors": [
                    "S Seuken",
                    "S Zilberstein"
                ],
                "Journal": "Autonomous Agents and Multi-Agent Systems"
            },
            {
                "id": "b19",
                "Title": "Coordinated multi-agent reinforcement learning in networked distributed POMDPs",
                "Authors": [
                    "C Zhang",
                    "V Lesser"
                ]
            }
        ]
    }
}